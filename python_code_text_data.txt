import cv2
import matplotlib.pyplot as plt
img = cv2.imread("./opencv-logo-white.png")
plt.imshow(img, interpolation='bicubic')
plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis
plt.show()

import cv2
img = cv2.imread("./opencv-logo-white.png")
cv2.namedWindow("img",cv2.WINDOW_NORMAL)#WINDOW_NORMALにするとウィンドウサイズが変わる
cv2.imshow("img", img)
cv2.waitKey(0)
cv2.destroyAllWindows()

import cv2
import numpy as np
import os
from lobe import ImageModel<N><N>

def has_green_pixels(img):
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hsv_l = np.array([40, 40, 40])
    hsv_h = np.array([70, 255, 255])
    return 255 in cv2.inRange(hsv, hsv_l, hsv_h)<N><N>

def createCroppedDigits(num):
    threshold = 4
    top = 96  # android notification bar in xxxhdpi
    left = 0
    path = os.path.join(os.getcwd(), str(num))
    if os.path.exists(path):
        return<N><N>
    os.mkdir(path)
    img = cv2.imread("nonogram" + str(num) + ".png")
    outImg = img<N><N>
    # create contour
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 300, apertureSize=3)
    # cv2.imshow("Display window", gray)
    # cv2.waitKey(0)
    # cv2.destroyAllWindows()<N><N>
# prioritize lines with new information and those that are almost finished
# recursively solve segments
# determine the position of the parts<N><N># 15: 3 2 2 3: [0,0,0,0,1,1,0,0,0,1,1,0,0,0,0]

"""
Code from rosettacode.org/wiki/Nonogram_solver
Available under GNU Free Documentation License 1.2
top: C BA CB BB F AE F A B
left: AB CA AE GA E C D C
with A=1, B=2, etc.
"""
import os
from functools import reduce<N><N>

def gen_row(w, s):
    """Create all patterns of a row or col that match given runs."""<N><N>
    def gen_seg(o, sp):
        if not o:
            return [[2] * sp]
        return [[2] * x + o[0] + tail
                for x in range(1, sp - len(o) + 2)
                for tail in gen_seg(o[1:], sp - x)]<N><N>
    return [x[1:] for x in gen_seg([[1] * i for i in s], w + 1 - sum(s))]<N><N>

def deduce(hr, vr):
    """Fix inevitable value of cells, and propagate."""<N><N>
    def allowable(row):
        return reduce(lambda a, b: [x | y for x, y in zip(a, b)], row) if row else []<N><N>
    def fits(a, b):
        return all(x & y for x, y in zip(a, b))<N><N>
    def fix_col(n):
        """See if any value in a given column is fixed;
        if so, mark its corresponding row for future fixup."""
        c = [x[n] for x in can_do]
        cols[n] = [x for x in cols[n] if fits(x, c)]
        for i, x in enumerate(allowable(cols[n])):
            if x != can_do[i][n]:
                mod_rows.add(i)
                can_do[i][n] &= x<N><N>
    def fix_row(n):
        """Ditto, for rows."""
        c = can_do[n]
        rows[n] = [x for x in rows[n] if fits(x, c)]
        for i, x in enumerate(allowable(rows[n])):
            if x != can_do[n][i]:
                mod_cols.add(i)
                can_do[n][i] &= x<N><N>
    def show_gram(m):
        # If there's 'x', something is wrong.
        # If there's '?', needs more work.
        for x in m:
            print(" ".join("x#.?"[i] for i in x))
        print()<N><N>
    w, h = len(vr), len(hr)
    rows = [gen_row(w, x) for x in hr]
    cols = [gen_row(h, x) for x in vr]
    can_do = list(map(allowable, rows))<N><N>
    # Initially mark all columns for update.
    mod_rows, mod_cols = set(), set(range(w))<N><N>
    while mod_cols:
        for i in mod_cols:
            fix_col(i)
        mod_cols = set()
        for i in mod_rows:
            fix_row(i)
        mod_rows = set()<N><N>
    if all(can_do[i][j] in (1, 2) for j in range(w) for i in range(h)):
        print("Solution would be unique")  # but could be incorrect!
    else:
        print("Solution may not be unique, doing exhaustive search:")<N><N>
    # We actually do exhaustive search anyway. Unique solution takes
    # no time in this phase anyway, but just in case there's no
    # solution (could happen?).
    out = [0] * h<N><N>
    def try_all(n=0):
        if n >= h:
            for j in range(w):
                if [x[j] for x in out] not in cols[j]:
                    return 0
            show_gram(out)
            return 1
        sol = 0
        for x in rows[n]:
            out[n] = x
            sol += try_all(n + 1)
        return sol<N><N>
    n = try_all()
    if not n:
        print("No solution.")
    elif n == 1:
        print("Unique solution.")
    else:
        print(n, "solutions.")
    print()<N><N>

def solve(p, show_runs=True):
    s = [[[ord(c) - ord('A') + 1 for c in w] for w in l.split()]
         for l in p]
    if show_runs:
        print("Horizontal runs:", s[0])
        print("Vertical runs:", s[1])
    deduce(s[0], s[1])<N><N>

def solveFile(num):
    from readPuzzle import createBoardString
    puzzleFile = createBoardString(num)
    solve(open(puzzleFile).read().split('\n'))<N><N>

if __name__ == "__main__":
    num = input("Please provide the nonogram number and check that nonogram png file exists (i.e. nonogram12.png): ")
    solveFile(num)<N><N>
#  -------------------------------------------------------------
#   Copyright (c) Microsoft Corporation.  All rights reserved.
#  -------------------------------------------------------------
"""
Skeleton code showing how to load and run the TensorFlow SavedModel export package from Lobe.
"""
import argparse
import os
import json
import tensorflow as tf
from PIL import Image
import numpy as np<N><N>
EXPORT_MODEL_VERSION = 1<N><N>
import numpy as np
import cv2<N><N>
print(cv2.__version__)<N><N>
url = 0
cam = cv2.VideoCapture(url)<N><N>
_, background = cam.read()<N><N>
kernel1 = np.ones((3, 3), np.uint8)
kernel2 = np.ones((11, 11), np.uint8)
kernel3 = np.ones((13, 13), np.uint8)<N><N>
width, height = 640, 360
cv2.namedWindow("original", cv2.WINDOW_NORMAL)
cv2.resizeWindow("original", width, height)
cv2.namedWindow("mask", cv2.WINDOW_NORMAL)
cv2.resizeWindow("mask", width, height)
cv2.namedWindow("dst", cv2.WINDOW_NORMAL)
cv2.resizeWindow("dst", width, height)
cv2.namedWindow('controls', cv2.WINDOW_NORMAL)
cv2.resizeWindow("controls", width, height)<N><N>

def callback(x):
    pass<N><N>

# global variable
H_low = 0
H_high = 179
S_low = 0
S_high = 255
V_low = 0
V_high = 255<N><N>
# create trackbars for high,low H,S,V
cv2.createTrackbar('low H', 'controls', H_low, 179, callback)
cv2.createTrackbar('high H', 'controls', H_high, 179, callback)<N><N>
cv2.createTrackbar('low S', 'controls', S_low, 255, callback)
cv2.createTrackbar('high S', 'controls', S_high, 255, callback)<N><N>
cv2.createTrackbar('low V', 'controls', V_low, 255, callback)
cv2.createTrackbar('high V', 'controls', V_high, 255, callback)<N><N>
while cam.isOpened():
    _, frame = cam.read()<N><N>
    # get trackbar positions
    H_low = cv2.getTrackbarPos('low H', 'controls')
    H_high = cv2.getTrackbarPos('high H', 'controls')
    S_low = cv2.getTrackbarPos('low S', 'controls')
    S_high = cv2.getTrackbarPos('high S', 'controls')
    V_low = cv2.getTrackbarPos('low V', 'controls')
    V_high = cv2.getTrackbarPos('high V', 'controls')<N><N>
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)<N><N>
    lower = np.array([H_low, S_low, V_low])
    upper = np.array([H_high, S_high, V_high])<N><N>
    mask = cv2.inRange(hsv, lower, upper)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel2)
    mask = cv2.dilate(mask, kernel3, iterations=2)<N><N>
    mask_not = cv2.bitwise_not(mask)<N><N>
    bg = cv2.bitwise_and(background, background, mask=mask)
    fg = cv2.bitwise_and(frame, frame, mask=mask_not)<N><N>
    dst = cv2.addWeighted(bg, 1, fg, 1, 0)<N><N>
    cv2.imshow("original", frame)
    cv2.imshow("mask", mask)
    cv2.imshow("dst", dst)<N><N>
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break<N><N>
cam.release()
cv2.destroyAllWindows()
<N><N>
import cv2 as cv
 
select_image = ''<N><N>
haar_cascade = cv.CascadeClassifier('./data/haer_file.xml')<N><N>

# Display the resulting frame<N><N>
def face_detector_image():<N><N>
    image = cv.imread(select_image)
    
    faces_rect = haar_cascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=9)
    for (x,y,w,h) in faces_rect:
        cv.rectangle(image, (x,y), (x+w,y+h), (0,255,0), thickness=2)<N><N>
    print('======================= image ready for showing =======================')<N><N>
    cv.imshow('Image', image)<N><N>
    cv.waitKey(0)<N><N>
    # destroying window on click q
    while(True):
        if cv.waitKey(1) & 0xFF == ord('q'):
            break
        
    cv.destroyAllWindows()<N><N>
import cv2 as cv<N><N>
select_video = ''<N><N>
def face_detector_video():
    
    video = cv.VideoCapture(select_video)<N><N>
    haar_cascade = cv.CascadeClassifier('./data/haer_file.xml')
    
    while(True):
        
        # Capture the video frame by frame
        ret, frame = video.read()
        
        # Display the resulting frame
                
        faces_rect = haar_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5)
        for (x,y,w,h) in faces_rect:
            cv.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), thickness=2)<N><N>

        cv.imshow('frame', frame)
        
        # destroying window on click q
        if cv.waitKey(1) & 0xFF == ord('q'):
            break
        
    print('======================= video ready to play =======================')
    video.release()
    cv.destroyAllWindows()
<N><N>
import cv2 as cv<N><N>
def face_detector_video_live():  
    video = cv.VideoCapture(0)<N><N>
    haar_cascade = cv.CascadeClassifier('./data/haer_file.xml')
  
    while(True):
    
        # Capture the video frame by frame
        ret, frame = video.read()
    
        # Display the resulting frame
            
        faces_rect = haar_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=9)
        for (x,y,w,h) in faces_rect:
            cv.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), thickness=2)<N><N>
        cv.imshow('frame', frame)<N><N>
        # destroying window on click q
        if cv.waitKey(1) & 0xFF == ord('q'):
            break
    
    video.release()
    cv.destroyAllWindows()<N><N>
import numpy as np
import cv2 as cv<N><N>
predict_image = '' <N><N>
people = [ 'Elon Musk', 'Robert Downey' ]<N><N>
haar_cascade = cv.CascadeClassifier('./data/haer_file.xml')<N><N>

def face_recognizer():<N><N>
    face_recognizer = cv.face.LBPHFaceRecognizer_create()
    
    # reading from trained model
    face_recognizer.read('./data/model_trained.yml')<N><N>
    # add for testing
    img = cv.imread(predict_image)<N><N>
    # converting to gray
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)<N><N>
    faces_rect = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=9)<N><N>
    for (x,y,w,h) in faces_rect:
        faces_marked = gray[y:y+h,x:x+w]<N><N>
        # making prediction
        label, confidence = face_recognizer.predict(faces_marked)
        print('Label = ' + people[label] )
        
        # rectangle
        cv.rectangle(img, (x,y), (x+w,y+h), (0,255,0), thickness=2)<N><N>
        # putting text below image
        cv.putText(img, str(people[label]), ( x, y+h+30 ), cv.FONT_HERSHEY_COMPLEX, 1.0, (0,0,255), thickness=2)<N><N>
    cv.imshow('Detected Face', img)<N><N>
    cv.waitKey(0)<N><N>
    # destroying window on click q
    while(True):
        if cv.waitKey(1) & 0xFF == ord('q'):
            break
        
    cv.destroyAllWindows()<N><N>
import cv2 as cv
import os
import numpy as np<N><N>

# arrays defined for x, y of training set
features = []
labels = []<N><N>
# direcory of images
people = [ 'Elon Musk', 'Robert Downey' ]
DIR = './images'<N><N>
# face detection file
haar_cascade = cv.CascadeClassifier('./data/haer_file.xml')<N><N>
def create_train():
    for x in people:
        path = os.path.join(DIR, x)
        index = people.index(x)
     <N><N>
        for img in os.listdir(path):
            # making the location of image
            img_path = os.path.join(path, img)<N><N>
            # reading image from img_path
            img_array = cv.imread(img_path)<N><N>
            if img_array is None:
                continue
            
            # converting to gray scale
            gray = cv.cvtColor(img_array, cv.COLOR_BGR2GRAY)<N><N>
            face_rect = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=9)<N><N>
            for (x, y, w, h) in face_rect:
                faces_marked = gray[y:y+h, x:x+w]
                features.append(faces_marked)
                labels.append(index)<N><N>

def train_model():<N><N>
    create_train()<N><N>
    X = np.array(features, dtype='object')
    Y = np.array(labels)<N><N>
    face_recognizer = cv.face.LBPHFaceRecognizer_create()<N><N>
    # trainiAfflekng model
    face_recognizer.train(X, Y)
    print('======================= Training done =======================')<N><N>
    # saving model to use further
    face_recognizer.save('./data/model_trained.yml')
    np.save('./data/model_features.npy', X)
    np.save('./data/model_labels.npy', Y)
    print('======================= Files saved =======================')
<N><N>
# ======================================= import dependences =======================================
from tkinter import * 
from tkinter import filedialog
import cv2 as cv
from PIL import Image, ImageTk
import webbrowser<N><N>
# ======================================= importing face detector =======================================
import face_detector_image as fdi
import face_detector_video as fdv
import face_detector_video_live as fdvl<N><N>
# ======================================= importing face recognizer =======================================
import face_recognizer_training as frt
import face_recognizer_prediction as frp<N><N>
# ======================================= css =======================================
color = '#0CAFFF'
color_heading = '#0076CE'<N><N>
font_heading = ('Times', 22, 'bold', 'underline')
font_label= ("Open Sans", 12)<N><N>
padx = 8
pady = 8
ipadx = 8
ipady = 8
thickness = 1<N><N>

# ======================================= root =======================================
root = Tk()
root.geometry('900x600')
root.title('Face Check ✅')<N><N>
# ======================================= images =======================================
arrow1 = Image.open('./icons/arrow.png')
arrow2 = arrow1.resize((20, 20))
arrow = ImageTk.PhotoImage(arrow2)<N><N>
label1_image_old = Image.open('./icons/face_detector.png')
label1_image = label1_image_old.resize((40, 40))<N><N>
label2_image_old = Image.open('./icons/face_recognizer.png')
label2_image = label2_image_old.resize((40, 40))<N><N>

# ================================== frame1 : face detector frame ===================================
frame1 = Frame(root)
frame1.pack()<N><N>
# ======================================= frame2 : face recognizer frame =======================================
frame2 = Frame(root)
frame2.pack()<N><N>

people = [ 'Elon Musk', 'Robert Downey' ]<N><N>

# face detector functions 
def select_image():
    image_loc = filedialog.askopenfilename(initialdir="./images/", title="Image",  filetypes=[('Image Files', ['.jpeg', '.jpg', '.png', '.gif', '.tiff', '.tif', '.bmp'])]) 
    fdi.select_image = str(image_loc)<N><N>
def select_video():
    video_loc = filedialog.askopenfilename(initialdir="./videos", title="Video",  filetypes=[('Video Files', ['.mp4', '.mov', '.wmv', '.avi', '.mkv' ])]) 
    fdv.select_video = str(video_loc)<N><N>
#  face recognizer functions
def add_person():
    people.append( placeholder.get() )
    frp.people.append( placeholder.get() )
    frt.people.append( placeholder.get() )
    placeholder.delete(0,  END)<N><N>
def remove_person():
    people.remove(placeholder.get())
    placeholder.delete(0,  END)<N><N>
def see_people():
    label = Label(root, text=people )
    label.pack()<N><N>
def predict_image():
    image_loc = filedialog.askopenfilename(initialdir="./images/", title="image",  filetypes=[('Image Files', ['.jpeg', '.jpg', '.png', '.gif', '.tiff', '.tif', '.bmp'])]) 
    frp.predict_image = str(image_loc)<N><N>
<N><N># ====================================== frame1 : face detector =========================================== <N><N>
    # ------------ 1 -----------------
label_heading1 = Label( frame1, text='Face Detector', justify='center', fg=color_heading, font = font_heading )<N><N>
label1_img = ImageTk.PhotoImage(label1_image)
label1_image = Label( frame1, image=label1_img)<N><N>
    # ----------------- 2 -----------------
label1 = Label(frame1, text=f'Detect faces \n in image', bg=color, font=font_label )<N><N>
arrow1 = Label(frame1, image = arrow)<N><N>
select_image_button = Button( frame1, text="See image", command=select_image, highlightthickness = thickness, font = font_label )
select_image_button.config( highlightbackground = color, highlightcolor= color )<N><N>
arrow2 = Label(frame1, image = arrow, justify='center')<N><N>
face_detector_image = Button( frame1, text="Detect Faces in image", command=lambda : fdi.face_detector_image(), highlightthickness = thickness, font = font_label )
face_detector_image.config( highlightbackground = color, highlightcolor= color )<N><N>
    # ----------------- 3 -----------------
label2 = Label(frame1, text=f'Detect faces \n in video', bg=color, font= font_label )<N><N>
arrow3 = Label(frame1, image = arrow)<N><N>
select_video_button = Button(frame1, text='Select Video', command=select_video, highlightthickness= thickness, font = font_label )
select_video_button.config( highlightbackground = color, highlightcolor= color )<N><N>
arrow4 = Label(frame1, image = arrow )<N><N>
face_detector_video = Button( frame1, text="Detect Faces in video", command=lambda : fdv.face_detector_video(), highlightthickness=thickness, font = font_label )
face_detector_video.config( highlightbackground = color, highlightcolor= color )<N><N>
    # ----------------- 4 -----------------
label3 = Label(frame1, text=f'Detect faces \n in image', bg=color, font = font_label )<N><N>
arrow5 = Label(frame1, image = arrow)<N><N>
face_detector_video_live = Button( frame1, text="Detect Faces in live video", command=lambda : fdvl.face_detector_video_live(),  justify='center', highlightthickness=thickness, font = font_label )
face_detector_video_live.config( highlightbackground = color, highlightcolor= color )<N><N>

# ========================================== grid frame1 ============================================<N><N>
    # ----------------- 1 -----------------
label_heading1.grid         ( row=0, columnspan=5, pady=(20,20) )
label1_image.grid           ( row=0, column=4, pady=(10,20) )<N><N>
    # ----------------- 2 -----------------
label1.grid                 ( row=1, column=0, padx=padx, pady=pady, ipadx=ipadx, ipady=ipady )
arrow1.grid                 ( row=1, column=1, padx=padx, pady=pady )
select_image_button.grid    ( row=1, column=2, padx=padx, pady=pady )
arrow2.grid                 ( row=1, column=3, padx=padx, pady=pady )
face_detector_image.grid    ( row=1, column=4, padx=padx, pady=pady )<N><N>
    # ----------------- 3 -----------------
label2.grid                 ( row=2, column=0, padx=padx, pady=pady, ipadx=ipadx, ipady=ipady )
arrow3.grid                 ( row=2, column=1, padx=padx, pady=pady )
select_video_button.grid    ( row=2, column=2, padx=padx, pady=pady )
arrow4.grid                 ( row=2, column=3, padx=padx, pady=pady )
face_detector_video.grid    ( row=2, column=4, padx=padx, pady=pady )<N><N>
    # ----------------- 4 -----------------
label3.grid                 ( row=3, column=0, padx=5, pady=5, ipadx=ipadx, ipady=ipady )
arrow5.grid                 ( row=3, column=1, columnspan=3, padx=padx, pady=pady )
face_detector_video_live.grid( row=3, column=2, columnspan=3, padx=padx, pady=pady, sticky=E )<N><N>
<N><N><N><N>
# =================================== frame2 : face recognizer ========================================= <N><N>
    # ----------------- 1 -----------------
label_heading2 = Label( frame2, text='Face Recognizer', justify='center', fg=color_heading, font=font_heading )<N><N>
label2_img = ImageTk.PhotoImage(label2_image)
label2_image = Label( frame2, image=label2_img)<N><N>
    # ----------------- 2  -----------------
placeholder = Entry(frame2, width=20, highlightthickness=thickness, font=font_label )
placeholder.config( highlightbackground = color, highlightcolor= color )<N><N>
arrow6 = Label(frame2, image = arrow, justify='center')<N><N>
button_add_person = Button(frame2, text="Add Person" ,command=add_person, highlightthickness=thickness, font=font_label )
button_add_person.config( highlightbackground = color, highlightcolor= color )<N><N>
button_remove = Button( frame2, text="Remove Person", command=remove_person, highlightthickness=thickness, font=font_label )
button_remove.config( highlightbackground = color, highlightcolor= color )<N><N>
button_see_people = Button( frame2, text="See Persons", command=see_people, highlightthickness=thickness, font=font_label )
button_see_people.config( highlightbackground = color, highlightcolor= color )<N><N>
    # ----------------- 3 -----------------
train_model = Button( frame2, text= "train_model", command=lambda: frt.train_model(), highlightthickness=thickness, font=font_label )
train_model.config( highlightbackground = color, highlightcolor= color )<N><N>
arrow7 = Label(frame2, image = arrow, justify='center')<N><N>
predict_image = Button(frame2, text="choose image...", command=predict_image, highlightthickness=thickness, font=font_label )
predict_image.config( highlightbackground = color, highlightcolor= color )<N><N>
arrow8 = Label(frame2, image = arrow, justify='center')<N><N>
face_recognizer = Button( frame2, text="Recognize face", command=lambda: frp.face_recognizer(), highlightthickness=thickness, font=font_label )
face_recognizer.config( highlightbackground = color, highlightcolor= color )<N><N>
<N><N># ======================================== grid frame2 =============================================<N><N>
import cv2<N><N>
#웹캠에서 영상을 읽어온다
cap = cv2.VideoCapture(0)
cap.set(3, 640) #WIDTH
cap.set(4, 480) #HEIGHT<N><N>
#얼굴 인식 캐스케이드 파일 읽는다
face_cascade = cv2.CascadeClassifier('/haarcascade_frontface.xml')<N><N>
while(True):
    # frame 별로 capture 한다
    ret, frame = cap.read()<N><N>
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)<N><N>
    #인식된 얼굴 갯수를 출력
    print(len(faces))<N><N>
    # 인식된 얼굴에 사각형을 출력한다
    for (x,y,w,h) in faces:
         cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)<N><N>
    #화면에 출력한다
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import cv2
import numpy as np
import argparse
import pandas as pd<N><N>
# Using argparse library to create an argument parser. So that we can directly give an image path from the command prompt.
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True, help="Image Path")
args = vars(ap.parse_args())<N><N>
image_path = args["image"]<N><N>
# Read image using OpenCV
img = cv2.imread(image_path)<N><N>
clicked = False
r = g = b = xpos = ypos = 0<N><N>
# Read colors.csv using Pandas
index = ["color", "color_name", "hex", "R", "G", "B"]
df = pd.read_csv("colors.csv", names=index, header=None)<N><N>

# The getColorName function will get the color name, we calculate a distance(d) which tells us how close we are to color and choose the one having minimum distance.
def getColorName(R, G, B):
    minimum = 10000
    for i in range(len(df)):
        dist = abs(R - int(df.loc[i, "R"])) + abs(G - int(df.loc[i, "G"])) + abs(B - int(df.loc[i, "B"]))
        if dist <= minimum:
            minimum = dist
            color_name = df.loc[i, "color_name"]
    return color_name<N><N>

# The draw function will calculate the rgb values of the pixel which we double click
def draw(event, x, y, flags, param):
    if event == cv2.EVENT_LBUTTONDBLCLK:
        global b,g,r,xpos,ypos, clicked
        clicked = True
        xpos = x
        ypos = y
        b, g, r = img[y, x]
        b = int(b)
        g = int(g)
        r = int(r)<N><N>

# Name the window as "Image"
cv2.namedWindow("Image")<N><N>
# Set a mouse callback event -- Calls the draw function whenever a mouse event occurs
cv2.setMouseCallback("Image", draw)<N><N>

# Display the image
while True:
    cv2.imshow("Image", img)<N><N>
    if clicked:
        cv2.rectangle(img, (20,20), (750, 60), (b,g,r), -1)<N><N>
        # Create the string to be displayed (Color Name + RGB values)
        name = getColorName(r, g, b) + " | R=" + str(r) + " G=" + str(g) + " B=" + str(b)<N><N>
        cv2.putText(img, name, (50, 50), 2, 0.8, (255, 255, 255), 2, cv2.LINE_AA)<N><N>
        # For light colors we want text to be displayed in black coloe
        if(r+g+b >= 600):
            cv2.putText(img, name, (50, 50), 2, 0.8, (0, 0, 0), 2, cv2.LINE_AA)<N><N>
        clicked = False<N><N>
    # Break the loop when user hits 'esc' key  
    if cv2.waitKey(20) & 0xFF ==27:
        break<N><N>
cv2.destroyAllWindows()<N><N>
from aiohttp import web
import aiohttp_jinja2
import jinja2
from typing import Dict, Any
import cv2 as cv
import logging<N><N>
routes = web.RouteTableDef()<N><N>

class CameraHandle:
    @staticmethod
    def open_camera_and_get_image(cap: cv.VideoCapture) -> bytes:
        """
        Demonstrates how to stream video with the help of openCV.<N><N>
from aiohttp import web
import aiohttp_jinja2
import jinja2
from typing import Dict, Any
import logging
import cv2 as cv<N><N>

routes = web.RouteTableDef()<N><N>

class CameraHandle:
    @staticmethod
    def open_camera_and_get_image(cap: cv.VideoCapture) -> bytes:
        """
        Demonstrates how to stream video with the help of openCV.<N><N>
import cv2
import time<N><N>
CONFIDENCE_THRESHOLD = 0.2
NMS_THRESHOLD = 0.4
COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]<N><N>
class_names = []
with open("classes.txt", "r") as f:
    class_names = [cname.strip() for cname in f.readlines()]<N><N>
vc = cv2.VideoCapture(0)<N><N>
net = cv2.dnn.readNet("yolov4-tiny.weights", "yolov4-tiny.cfg")
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)<N><N>
model = cv2.dnn_DetectionModel(net)
model.setInputParams(size=(416, 416), scale=1/255, swapRB=True)<N><N>
while cv2.waitKey(1) < 1:
    (grabbed, frame) = vc.read()
    if not grabbed:
        exit()<N><N>
    start = time.time()
    classes, scores, boxes = model.detect(
        frame, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)
    end = time.time()<N><N>
    start_drawing = time.time()
    for (classid, score, box) in zip(classes, scores, boxes):
        color = COLORS[int(classid) % len(COLORS)]
        label = "%s : %f" % (class_names[classid[0]], score)
        cv2.rectangle(frame, box, color, 2)
        cv2.putText(frame, label, (box[0], box[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    end_drawing = time.time()<N><N>
    fps_label = "FPS: %.2f (excluding drawing time of %.2fms)" % (
        1 / (end - start), (end_drawing - start_drawing) * 1000)
    cv2.putText(frame, fps_label, (0, 25),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
    cv2.imshow("detections", frame)
<N><N>
import os
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import classification_report
import numpy as np
import cv2 as cv
import cv2<N><N>
BS = 8
data = []
new_model = tf.keras.models.load_model('tumor.model')<N><N>
im = cv.imread('subjects/1.png') #change to the desired image to test
image = cv.cvtColor(im, cv.COLOR_BGR2RGB)
image = cv.resize(image, (224, 224))
data.append(image)
data = np.array(data) / 255.0<N><N>
predIdxs = new_model.predict(data)
prob_normal = predIdxs[0][1] * 100;
prob_cancer  = predIdxs[0][0] * 100;<N><N>
print("Probabilite d'etre sain: %.2f" % prob_normal)
print("Probabilite du Cancer: %.2f" % prob_cancer)<N><N>
if prob_normal > 40 : 
    image = cv.resize(image, (600, 600))
    text = "{:.2f}%".format(prob_normal)
    cv2.putText(image, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
    cv2.imshow("Sain", image)
    cv.waitKey()
else: 
    image = cv.resize(image, (600, 600))
    text = "{:.2f}%".format(prob_cancer)
    cv2.putText(image, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
    cv2.imshow("Malade",image)
    cv.waitKey()<N><N>
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtGui import QMovie
from PyQt5.QtWidgets import QApplication, QWidget, QInputDialog, QLineEdit, QFileDialog
from PyQt5.QtGui import QIcon
import sys
import os
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import classification_report
import numpy as np
import cv2 as cv
import cv2<N><N>
#trainer created by Adrian Rosebrock , modified by Amine Bébba 30/03/2020 , modified again 15/03/2021
# python trainer.py --dataset dataset<N><N>
# -*- coding: utf-8 -*-
"""
Created on Sat May 29 12:50:12 2021<N><N>
@author: Admin
"""<N><N>
import cv2
import mediapipe as mp
import time<N><N>
cap = cv2.VideoCapture(0)<N><N>
mpHands = mp.solutions.hands
hands = mpHands.Hands()
mpDraw =mp.solutions.drawing_utils<N><N>
pTime = 0
cTime = 0<N><N>
import cv2 as cv
import numpy as np<N><N>

haystack_img = cv.imread('gae1.jpg', cv.IMREAD_UNCHANGED)
needle_img = cv.imread('alert.jpg', cv.IMREAD_UNCHANGED)<N><N>
result = cv.matchTemplate(haystack_img, needle_img, cv.TM_CCOEFF_NORMED)<N><N>
min_val, max_val, min_loc, max_loc = cv.minMaxLoc(result)<N><N>
print('Best Match top left positopn: %s' % str(max_loc))
print('Best Match cinfidence: %s' % max_val)<N><N>
threshold = 0.8
if max_val >= threshold:
    # cv.imshow('Original', haystack_img)
    print('Found needle.')<N><N>
    needle_w = needle_img.shape[1]
    needle_h = needle_img.shape[0]<N><N>
    top_left = max_loc
    bottom_right = (top_left[0] + needle_w, top_left[1] + needle_h)<N><N>
    cv.rectangle(haystack_img, top_left, bottom_right, color=(0, 255, 0), thickness=2, lineType=cv.LINE_4)<N><N>
    cv.imshow('Result', haystack_img)
    cv.waitKey()
    # cv.imwrite('result1.jpg', haystack_img)
else:
    print('Needle not found.')<N><N>
# cv.imshow('Originnal', result)
# cv.waitKey()<N><N>
import cv2
import numpy as np
from sklearn.metrics import pairwise<N><N>
# Define globals
background = None
accumulated_weight = 0.5
roi_top = 20
roi_bottom = 200
roi_right = 200
roi_left = 400<N><N>

def calc_accum_avg(frame, accumulated_weight):
    global background<N><N>
    if background is None:
        background = frame.copy().astype("float")
        return None<N><N>
    cv2.accumulateWeighted(frame, background, accumulated_weight)<N><N>

def segment(frame, min_threshold=25):
    diff = cv2.absdiff(background.astype("uint8"), frame)<N><N>
    ret, thresholded = cv2.threshold(diff, min_threshold, 255, cv2.THRESH_BINARY)<N><N>
    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)<N><N>
    if len(contours) == 0:
        return None<N><N>
    else:
        # assuming the largest external contour in roi is the hand
        hand_segment = max(contours, key=cv2.contourArea)
        return (thresholded, hand_segment)<N><N>

def count_fingers(thresholded, hand_segment):<N><N>
    conv_hull = cv2.convexHull(hand_segment)<N><N>
    top = tuple(conv_hull[conv_hull[:, :, 1].argmin()][0])
    bottom = tuple(conv_hull[conv_hull[:, :, 1].argmax()][0])
    left = tuple(conv_hull[conv_hull[:, :, 0].argmin()][0])
    right = tuple(conv_hull[conv_hull[:, :, 0].argmax()][0])<N><N>
    cX = (left[0] + right[0]) // 2
    cY = (top[1] + bottom[1]) // 2<N><N>
    distance = pairwise.euclidean_distances([(cX, cY)], Y=[left, right, top, bottom])[0]<N><N>
    max_distance = distance.max()<N><N>
    radius = int(0.9 * max_distance)
    circumfrence = 2 * np.pi * radius<N><N>
    circular_roi = np.zeros(thresholded.shape[:2], dtype="uint8")<N><N>
    cv2.circle(circular_roi, (cX, cY), radius, 255, 10)<N><N>
    circular_roi = cv2.bitwise_and(thresholded, thresholded, mask=circular_roi)<N><N>
    contours, hierarchy = cv2.findContours(circular_roi.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)<N><N>
    count = 0
    for cnt in contours:
        (x, y, w, h) = cv2.boundingRect(cnt)<N><N>
        out_of_wrist = (cY + (cY * 0.25)) > (y + h)<N><N>
        limit_points = (circumfrence * 0.25) > cnt.shape[0]<N><N>
        if out_of_wrist and limit_points:
            count = count + 1<N><N>
    return count<N><N>

def main():<N><N>
    cam = cv2.VideoCapture(0)<N><N>
    num_frames = 0<N><N>
    while True:
        ret, frame = cam.read()<N><N>
        frame_copy = frame.copy()<N><N>
        roi = frame[roi_top:roi_bottom, roi_right:roi_left]<N><N>
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)<N><N>
        gray = cv2.GaussianBlur(gray, (7, 7), 0)<N><N>
        if num_frames < 60:
            calc_accum_avg(gray, accumulated_weight)<N><N>
            if num_frames <= 59:
                cv2.putText(frame_copy, "WAIT. GETTING BACKGROUND", (200, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.imshow("Finger Count", frame_copy)
        else:
            hand = segment(gray)<N><N>
            if hand is not None:
                thresholded, hand_segment = hand<N><N>
                cv2.drawContours(frame_copy, [hand_segment + (roi_right, roi_top)], -1, (255, 0, 0), 5)<N><N>
                fingers = count_fingers(thresholded, hand_segment)<N><N>
                cv2.putText(frame_copy, str(fingers), (70, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)<N><N>
                cv2.imshow("Thresholded", thresholded)<N><N>
        cv2.rectangle(frame_copy, (roi_left, roi_top), (roi_right, roi_bottom), (0, 0, 255), 5)<N><N>
        num_frames += 1<N><N>
        cv2.imshow("Finger Count", frame_copy)<N><N>
        k = cv2.waitKey(1) & 0xFF<N><N>
        if k == 27:
            break<N><N>
    cam.release()
    cv2.destroyAllWindows()<N><N>

if __name__ == "__main__":
    main()
<N><N>
'''
    camera.py<N><N>
    source: https://medium.datadriveninvestor.com/video-streaming-using-flask-and-opencv-c464bf8473d6
'''<N><N>
import cv2<N><N>
cv2.samples.addSamplesDataSearchPath(cv2.__path__[0] + '/data')
face_cascade = cv2.CascadeClassifier(cv2.samples.findFile('haarcascade_frontalface_alt2.xml'))
eyes_cascade = cv2.CascadeClassifier(cv2.samples.findFile('haarcascade_eye_tree_eyeglasses.xml'))<N><N>
ds_factor = 0.6<N><N>
class VideoCamera(object):<N><N>
'''
    main.py<N><N>
    source: https://medium.datadriveninvestor.com/video-streaming-using-flask-and-opencv-c464bf8473d6
'''<N><N>
from flask import Flask, render_template, Response
from camera import VideoCamera<N><N>
app = Flask(__name__)<N><N>
@app.route('/')
def index():
    return render_template('index.html')<N><N>
def gen(camera):
    while True:
        frame = camera.get_frame()
        yield (b'--frame\r\n'
                b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n\r\n')<N><N>
@app.route('/video_feed')
def video_feed():
    return Response(gen(VideoCamera()),
            mimetype='multipart/x-mixed-replace; boundary=frame')<N><N>
@app.route("/about")
def about():
    return render_template("about.html")
    
if __name__ == '__main__':
    app.run(host = '0.0.0.0', debug = True)
<N><N>
'''
    main.py<N><N>
    source: https://medium.datadriveninvestor.com/video-streaming-using-flask-and-opencv-c464bf8473d6
'''<N><N>
from flask import Flask, render_template, Response
from camera import VideoCamera<N><N>
app = Flask(__name__)<N><N>
@app.route('/')
def index():
    return render_template('index.html')<N><N>
def gen(camera):
    while True:
        frame = camera.get_frame()
        yield (b'--frame\r\n'
                b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n\r\n')<N><N>
@app.route('/video_feed')
def video_feed():
    return Response(gen(VideoCamera()),
            mimetype='multipart/x-mixed-replace; boundary=frame')<N><N>
@app.route("/about")
def about():
    return render_template("about.html")
    
if __name__ == '__main__':
    app.run(host = '0.0.0.0', debug = True)
<N><N>
import cv2
 
img = cv2.imread('./akira.jpg')
 
cv2.imshow('image',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
z=dict()
for test in range(int(input())):
     t=list(map(int,input().split()))
     z[test]=t
for x in range(len(z)):
    n=z[x][0]
    m=z[x][1]
    d=z[x][2]
    D=z[x][3]
    if m<(d*n) or m>(n*D):
        print(-1)
        continue
    f = m // n
    rem = m % n
    for v in range(0, n):
        for i in range(0, f):
            print("%d %d" % (v + 1, 1 + (v + i) % n))
        if v < rem:
            print("%d %d" % (v + 1, 1 + (v + f) % n))

try:<N><N>
  def main():
    vertices, edges, lower_degree, upper_degree = map(int, input().split())
    edge_count = 0<N><N>
    if(edges < vertices*lower_degree or edges > vertices*upper_degree):
      print(-1)
    else:
      for i in range(0,vertices):
        for j in range(1,vertices+1):
          if(j+i>vertices):
            print(j, " ",((j+i)%vertices))
          else:
            print(j, " ",j+i)
          edge_count += 1
          if(edge_count==edges):
            break
        if(edge_count==edges):
           break<N><N>
  if __name__ == "__main__":
    for _ in range(int(input())):
      main()<N><N>
except:
  pass<N><N>
for test in range(int(input())):
    n, m, d, D = map(int, input().split())
    if n * d > m or n * D < m: 
        print(-1)
        continue
    ans = []
    e = [list(range(v, n)) + list(range(v)) for v in range(n)]
    for i in range(n):
        for iter in range(d):
            j = e[i].pop(-1)
            ans.append((i, j))
    i = 0
    while len(ans) < m:
        j = e[i].pop(-1)
        ans.append((i, j))
        i += 1
        if i == n: i = 0
    for c in ans:
        print(c[0] + 1, c[1] + 1)
t=int(input())
for i in range(t):
    n,m,d,D=map(int,input().split())
    
    if n*d > m or n*D < m :
        print("-1")
        continue
      
    c=0  
    f=0
    for i in range(0,n+1):
        for j in range(1,n+1):
            if j+i>n:
                print(j,(j+i)%n)
            else:
                print(j,j+i)
            c+=1
            if c==m:
                f=1
                break
        if f==1:
            break
    
    
    
    
t=int(input())
for i in range(t):
    n,m,d,D=map(int,input().split())
    
    if n*d > m or n*D < m :
        print("-1")
        continue
      
    c=0  
    f=0
    for i in range(0,n):
        for j in range(1,n+1):
            if j+i>n:
                print(j,(j+i)%n)
            else:
                print(j,j+i)
            c+=1
            if c==m:
                f=1
                break
        if f==1:
            break
    
    
    
    
# cook your dish here
t=int(input())
for i in range(t):
    n,m,d,D=map(int,input().split())
    
    if n*d > m or n*D < m :
        print("-1")
        continue
      
    c=0  
    f=0
    for i in range(0,n):
        for j in range(1,n+1):
            if j+i>n:
                print(j,(j+i)%n)
            else:
                print(j,j+i)
            c+=1
            if c==m:
                f=1
                break
        if f==1:
            break
    
# cook your dish here
t=int(input())
for i in range(t):
    n,m,d,D=map(int,input().split())
    
    if n*d > m or n*D < m :
        print("-1")
        continue
      
    c=0  
    f=0
    for i in range(0,n):
        for j in range(1,n+1):
            if j+i>n:
                print(j,(j+i)%n)
            else:
                print(j,j+i)
            c+=1
            if c==m:
                f=1
                break
        if f==1:
            break
    
# cook your dish here
t=int(input())
for i in range(t):
    n,m,d,D=map(int,input().split())
    
    if n*d > m or n*D < m :
        print("-1")
        continue
      
    c=0  
    f=0
    for i in range(0,n):
        for j in range(1,n+1):
            if j+i>n:
                print(j,(j+i)%n)
            else:
                print(j,j+i)
            c+=1
            if c==m:
                f=1
                break
        if f==1:
            break
    
# cook your dish here
import sys
t = int(input())
s = [int(j) for j in sys.stdin.read().split()]
for i in range(t):
   n, m, d, D = s[4*i:4*(i+1)]
   if m < n * d or m > n * D:
      print(-1)
   else:
      for j in range(m):
         print(str(j % n + 1) + " " + str((j + j//n) % n + 1))
def diff(a,b):
	if a>b:
		return(a-b)
	else:
		return(b-a)<N><N>
n = int(input())
a = [[], []]
a[0] = list(map(int,input().strip().split(' ')))
a[1] = list(map(int,input().strip().split(' ')))
#print (a)<N><N>
#dp[i] = max score till ith coloumn
#dp[i] = max( dp[i-1]+diff(a[0][i],a[1][i]), dp[i-2]+diff(a[0][i-1],a[1][i-1])+diff(a[0][i],a[1][i]), dp[i-2]+diff(a[0][i-1],a[0][i])+diff(a[1][i-1]+a[1][i]) )<N><N>
dp = []
minval = -1000000
for i in range(n):
	dp.append(minval)<N><N>
dp[0] = diff(a[0][0],a[1][0])
dp[1] = max( diff(a[0][0],a[1][0])+diff(a[0][1],a[1][1]), diff(a[0][0],a[0][1])+diff(a[1][0],a[1][1]) )<N><N>
for i in range(2,n):
	temp = max( dp[i-2]+diff(a[0][i-1],a[1][i-1])+diff(a[0][i],a[1][i]), dp[i-2]+diff(a[0][i-1],a[0][i])+diff(a[1][i-1],a[1][i]) )
	temp = max( temp, dp[i-1]+diff(a[0][i],a[1][i]) )
	dp[i] = temp<N><N>
print (dp[n-1])
<N><N>
test_cases = int(input())<N><N>for _ in range(test_cases):
    inputs = input().strip().split()
    n = int(inputs[0])
    m = int(inputs[1])
    d = int(inputs[2])
    D = int(inputs[3])
    
    if m>n*D or m<n*d or D>n:
        print(-1)
    else:
        count = 0
        i, j = 0, 0
        while count<m:
            print(i+1, j +1)
            count+=1
            i = (i+1)%n
            j = (j+1)%n
            if count%n==0:
                j = count//n
test_cases = int(input())<N><N>for _ in range(test_cases):
    inputs = input().strip().split()
    n = int(inputs[0])
    m = int(inputs[1])
    d = int(inputs[2])
    D = int(inputs[3])
    
    if m>n*D or m<n*d or D>n:
        print(-1)
    else:
        count = 0
        i, j = 0, 0
        while count<m:
            print(i+1, j +1)
            count+=1
            i = (i+1)%n
            j = (j+1)%n
            if count%n==0:
                j = count//n
t = int(input())<N><N>for q in range(t):<N><N>    n,m,d,D = map(int,input().split())
    
    if not (d*n <= m and m <= min(n,D)*n):
        print(-1)
    else:
        q = m//n 
        r = m % n
        for i in range(q):
            for j in range(n):
                print(j+1,((j+i) % n) +1)
        
        for i in range(r):
            print(i+1, ((i + q) % n)+1)
# cook your dish here
import numpy as np<N><N>c = int(input())
m = list(map(int,input().split()))
n = list(map(int,input().split()))<N><N>x = 0
y = abs(m[0] - n[0])<N><N>for i in range(1,c):
    
    v = y + abs(m[i] - n[i])
    h = x + abs(m[i] - m[i-1]) + abs(n[i] - n[i-1])
    
    x = y
    
    y = max(h,v)
    
print(y)

# cook your dish here<N><N>c = int(input())
m = list(map(int,input().split()))
n = list(map(int,input().split()))<N><N>x = 0
y = abs(m[0] - n[0])<N><N>for i in range(1,c):
    
    v = y + abs(m[i] - n[i])
    h = x + abs(m[i] - m[i-1]) + abs(n[i] - n[i-1])
    
    x = y
    
    y = max(h,v)
    
print(y)

t = int(input())<N><N>for q in range(t):<N><N>    n,m,d,D = map(int,input().split())
    
    if not (d*n <= m and m <= min(n,D)*n):
        print(-1)
    else:
        q = m//n 
        r = m % n
        for i in range(q):
            for j in range(n):
                print(j+1,((j+i) % n) +1)
        
        for i in range(r):
            print(i+1, ((i + q) % n)+1)
n=int(input())<N><N>
o = list(map(int,input().split()))
p = list(map(int,input().split()))<N><N>
a = [o,p]<N><N>
maxscores=[]<N><N>
maxscores.append(abs(a[0][0]-a[1][0]))<N><N>
m1 = abs(a[0][0]-a[1][0]) + abs(a[0][1]-a[1][1])
m2 = abs(a[0][0]-a[0][1]) + abs(a[1][0]-a[1][1])
maxscores.append(max(m1,m2))<N><N>
for i in range(2,n):
    iv = abs(a[0][i]-a[1][i])
    k1 = maxscores[i-1] + iv
    
    r1 = abs(a[0][i]-a[1][i]) + abs(a[0][i-1]-a[1][i-1])
    r2 = abs(a[0][i-1]-a[0][i]) + abs(a[1][i-1]-a[1][i])
    k2 = maxscores[i-2] + max(r1,r2)
    
    maxscores.append(max(k1,k2))
    
print(maxscores[n-1])<N><N>
# cook your dish here
import sys
t = int(input())
s = [int(j) for j in sys.stdin.read().split()]
for i in range(t):
   n, m, d, D = s[4*i:4*(i+1)]
   if m < n * d or m > n * D:
      print(-1)
   else:
      for j in range(m):
         print(str(j % n + 1) + " " + str((j + j//n) % n + 1))
# cook your dish here
n=int(input())<N><N>
o = list(map(int,input().split()))
p = list(map(int,input().split()))<N><N>
a = [o,p]<N><N>
maxscores=[]<N><N>
maxscores.append(abs(a[0][0]-a[1][0]))<N><N>
m1 = abs(a[0][0]-a[1][0]) + abs(a[0][1]-a[1][1])
m2 = abs(a[0][0]-a[0][1]) + abs(a[1][0]-a[1][1])
maxscores.append(max(m1,m2))<N><N>
for i in range(2,n):
    iv = abs(a[0][i]-a[1][i])
    k1 = maxscores[i-1] + iv
    
    r1 = abs(a[0][i]-a[1][i]) + abs(a[0][i-1]-a[1][i-1])
    r2 = abs(a[0][i-1]-a[0][i]) + abs(a[1][i-1]-a[1][i])
    k2 = maxscores[i-2] + max(r1,r2)
    
    maxscores.append(max(k1,k2))
    
print(maxscores[n-1])<N><N>
# cook your dish here
import sys
t = int(input())
s = [int(j) for j in sys.stdin.read().split()]
for i in range(t):
   n, m, d, D = s[4*i:4*(i+1)]
   if m < n * d or m > n * D:
      print(-1)
   else:
      for j in range(m):
         print(str(j % n + 1) + " " + str((j + j//n) % n + 1))
n=int(input())<N><N>
o = list(map(int,input().split()))
p = list(map(int,input().split()))<N><N>
a = [o,p]<N><N>
maxscores=[]<N><N>
maxscores.append(abs(a[0][0]-a[1][0]))<N><N>
m1 = abs(a[0][0]-a[1][0]) + abs(a[0][1]-a[1][1])
m2 = abs(a[0][0]-a[0][1]) + abs(a[1][0]-a[1][1])
maxscores.append(max(m1,m2))<N><N>
for i in range(2,n):
    iv = abs(a[0][i]-a[1][i])
    k1 = maxscores[i-1] + iv
    
    r1 = abs(a[0][i]-a[1][i]) + abs(a[0][i-1]-a[1][i-1])
    r2 = abs(a[0][i-1]-a[0][i]) + abs(a[1][i-1]-a[1][i])
    k2 = maxscores[i-2] + max(r1,r2)
    
    maxscores.append(max(k1,k2))
    
print(maxscores[n-1])
<N><N>
X,Y=map(float,input().split())
if(X%5==0):
    if(Y>=X+0.50):
        A=Y-(X+0.50)
        print("{0:.2f}".format(A))
    else:
        print("{0:.2f}".format(Y))
else:
    print("{0:.2f}".format(Y))
    
<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
w,b=map(float,input().split())
ba=w+0.5
if(w%5==0 and ba<=b):
    b=b-ba
    print(b)
else:
        print(b)

# cook your dish here
x,y=map(float,input().split())<N><N>if(x%5==0):
    if(y>=x+0.5):
        print( "{:.2f}".format(y-x-0.5))
    else:
        print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y))

n,k=map(int,input().split())
ch=0
while(n>0):
    ti=int(input())
    if(ti%k==0):
        ch=ch+1
    n=n-1
print(ch)
# cook your dish here
x,y=map(float,input().split())
charge = 0.5
if (((x + charge) > y ) or (x % 5 != 0)):
    print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y - x - charge))
# cook your dish here
x,y=map(float,input().split())
charge = 0.5
if (((x + charge) > y ) or (x % 5 != 0)):
    print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y - x - charge))
# cook your dish here
x,y=map(float,input().split())
charge = 0.5
if (((x + charge) > y ) or (x % 5 != 0)):
    print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y - x - charge))
n,k=map(int,input().split())
res=0
for i in range(n):
	t1=int(input())
	if(t1%k==0):
		res=res+1
	i=i+1
print(res)
# cook your dish here
x,y=map(float,input().split())
charge = 0.5
if (((x + charge) > y ) or (x % 5 != 0)):
    print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y - x - charge))
# cook your dish here
x, y = map(float,input().split())
charge = 0.50
if x%5 != 0 or ((x + charge)>y):
    print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y - x - charge))
    
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
n,k= map(int, input().split())
div = 0
for i in range(n):
	t = int(input())
	if t % k == 0:
		div += 1<N><N>print(div)	
n, k = map(int, input().split())
m = 0
output = 0
while m < n:
    t = int(input())
    m += 1
    if t % k == 0:
        output += 1
    else:
        pass
print(output)

x, y = map(float,input().split())
charge = 0.50
if x%5 != 0 or ((x + charge)>y):
    print("{:.2f}".format(y))
else:
    print("{:.2f}".format(y - x - charge))


n, m = map(float,input().split())
n,m=float(n),float(m)
if n%5==0 and (m-n-0.5)>=0 :
    m=m-n-0.5
print("{:.2f}".format(m))
n, m = map(float,input().split())
if n%5==0 and (m-n-0.5)>=0 :
    m=m-n-0.5
print("{:.2f}".format(m))
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1
		
# count_list = [1 for i in range(n) if int(input())%k ==0]
# ans = sum(count_list)<N><N>print(ans)	
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N># ans = 0<N><N># for i in range(n):
# 	x = int(input())
# 	if x % k == 0:
# 		ans += 1
		
count_list = [1 for i in range(n) if int(input())%k ==0]
ans = sum(count_list)<N><N>print(ans)	
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
x,y =map(float,input().split())<N><N>if (x + 0.5) <= y and x%5==0:
    y = y - (x+0.5)
print("{0:.2f}".format(y))
x,y =map(float,input().split())<N><N>if (x + 0.5) <= y and x%5==0:
    y = y - (x+0.5)
print("{0:.2f}".format(y))
# cook your dish here
(x, y) = map(float, input().split())
if x + 0.5 <= y and x%5 == 0:
    y =  y-(x+0.5)
print("{0:.2f}".format(y))
        
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.<N><N>(n, k) = map(int, input().split(' '))<N><N>ans = 0<N><N>for i in range(n):
	x = int(input())
	if x % k == 0:
		ans += 1<N><N>print(ans)	
x,y = map(float,input().split())
if (x+0.5) <= y and x%5 == 0:
    y = y - (x + 0.50)
print("%.2f"%y)

x,y = map(float,input().split())
if (x+0.5) <= y and x%5 == 0:
    y = y - (x + 0.50)
print ("{:.2f}".format(y))
  

# cook your dish here
x,y=map(float,input().split())
if x%5==0 and (x+0.50)<=y:
    z=y-(x+0.50)
    print(z)
else:
    print(y)
#Note that it's python3 Code. Here, we are using input() instead of raw_input().
#You can check on your local machine the version of python by typing "python --version" in the terminal.
try:
    (n, k) = map(int, input().split())
    ans = 0<N><N>    for i in range(n):
	    x = int(input())
	    if x % k == 0:
		    ans += 1<N><N>    print(ans)
except:
    pass
(n,k)=map(int,input().split())
count=0
for i in range(n):
    s=int(input())
    if s%k==0:
        count+=1 
        
print(count)
        
for z in range(int(input())):
  x,y,n = map(int,input().split())
  m = n//x
  while(m*x+y) > n:
    m -= 1
  print(m*x+y)
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
from collections import Counter
t=int(input())
for i in range(t):
    n=int(input())
    arr=list(map(int,input().split()))
    arr1=[]
    for i in range(len(arr)):
        bnr = bin(arr[i]).replace('0b','')
        if bnr[0]=="1":
            arr1.append(len(bnr)-1)
    p=Counter(arr1)
    count=0
    for i in p.values():
        if i>=2:
            count+=((i-1)*((i-1)+1))//2
    print(count)
     
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
for z in range(int(input())):
  x,y,n = map(int,input().split())
  m = n//x
  while(m*x+y) > n:
    m -= 1
  print(m*x+y)
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
w = int(input())
if 1 <= w <= 100:
    if w <= 2:
        print("NO")
    elif w > 2:
        second_half = w - 2
        if second_half % 2 == 0:
            print("YES")
        else:
            print("NO")
else:
    print("Enter something between 1 to 100")
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
n = input()
up = 0
low = 0
tam = len(n)
 for x in n:
    if x.isupper():
        up+=1
    else:
        low+=1
 if low>=up:
    n = n.lower()
else:
    n = n.upper()
 print(n)
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
from sys import stdin
from collections import deque,Counter,defaultdict
import sys
import math,os
import operator
import random
from fractions import Fraction
import functools
import bisect
import itertools
from heapq import *
import time
import random
    for _ in range(int(input())):
    a,b,n,m = map(int,input().split())
    ans = 'No'
    if a+b<n+m:
        print(ans)
        continue
    if m<=min(a,b):
        print("Yes")
    else:
        print("No")
 """8170069 7550734 7244506 4029542"""
n=int(input())
a=list(map(int,input().split(' ')))
for i in range(n):
    a[i]-=1
for j in range(n):
    if a[a[a[j]]]==j:
        print('Yes')
        exit()
print('No')
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
w = int(input())
if 1 <= w <= 100:
    if w <= 2:
        print("NO")
    elif w > 2:
        second_half = w - 2
        if second_half % 2 == 0:
            print("YES")
        else:
            print("NO")
else:
    print("Enter something between 1 to 100")
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
t=int(input())
for _ in range(t):
    n,k,d=list(map(int,input().split()))
    a=list(map(int,input().split()))
    z=len(set(a[:d]))
    for i in range(n-d+1):
        z1=len(set(a[i:i+d]))
        if z1<z:
            z=z1
    print(z)
         
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
for _ in range(int(input())):
    n, g = int(input()), [int(x) for x in input().split()]
    print(-1 if (v := sum(g)) % n else sum(1 for x in g if x > v // n))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
import math
wej = (input())
wejc = (wej.split())
n = int(wejc[0])
m = int(wejc[1])
a = int(wejc[2])
w = math.ceil(n / a)
s = math.ceil(m / a)
if w * s == 0:
    print("1")
else:
    print(w * s)
 
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
s=list(map(int,input().split(":")))
t=list(map(int,input().split(":")))
c=0
if t[1]>s[1]:
 s[1]+=60
 s[1]-=t[1]
 c+=1
else:
 s[1]-=t[1]
if s[0]-(t[0]+c)<0:
 d=t[0]+c-s[0]
 s[0]=24-d
else:
 s[0]-=(t[0]+c)
s=list(map(str,s))
for i in range(2):
 if len(s[i])==1:
  s[i]="0"+s[i]
print(":".join(s))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
for _ in range(int(input())):
    n = int(input())
    a = list(map(int, input().split()))
    x, y = a.index(1), a.index(n)
    if x > y:
        x, y = y, x
    print(min(y + 1, n - x, n + x - y + 1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
l=list(map(int,input().split()))
a=min(l[0],l[1])
b=max(l[0],l[1])
s=l[-1]
if a+s>b:
 s-=(b-a)
 c=b*2+(s//2)*2
else:
 c=(a+s)*2
print(c)
t = int(input())
for _ in range(t):
   x = int(input())
   print("1"+" "+str(x-1))
t,s,x=map(int,input().split())
a=x-t
b=x-t-1
if(t==x):
      print("YES")
else:
      if(a>=s)or(b>=s):
            if(a>=s)and(a%s==0):
                  print("YES")
            elif((b%s==0)and(b>=s)):
                  print("YES")
            else:
                 print("NO") 
      else:
            print("NO")
t,s,x=map(int,input().split())
a=x-t
b=x-t-1
if(t==x):
      print("YES")
else:
      if(a>=s)or(b>=s):
            if(a>=s)and(a%s==0):
                  print("YES")
            elif((b%s==0)and(b>=s)):
                  print("YES")
            else:
                 print("NO") 
      else:
            print("NO")
import math
wej = (input())
wejc = (wej.split())
n = int(wejc[0])
m = int(wejc[1])
a = int(wejc[2])
w = math.ceil(n / a)
s = math.ceil(m / a)
if w * s == 0:
    print("1")
else:
    print(w * s)
 
s = input().lower()
for i in s:
    if i not in 'aoyeui':
        print('.', i, sep='', end='')
t,s,x=map(int,input().split())
a=x-t
b=x-t-1
if(t==x):
      print("YES")
else:
      if(a>=s)or(b>=s):
            if(a>=s)and(a%s==0):
                  print("YES")
            elif((b%s==0)and(b>=s)):
                  print("YES")
            else:
                 print("NO") 
      else:
            print("NO")
t,s,x=map(int,input().split())
a=x-t
b=x-t-1
if(t==x):
      print("YES")
else:
      if(a>=s)or(b>=s):
            if(a>=s)and(a%s==0):
                  print("YES")
            elif((b%s==0)and(b>=s)):
                  print("YES")
            else:
                 print("NO") 
      else:
            print("NO")
for _ in range(int(input())):
    n, m = map(int, input().split())
    if sum(list(map(int, input().split()))) == m:
        print('YES')
    else:
        print('NO')
s = input().lower()
for i in s:
    if i not in 'aoyeui':
        print('.', i, sep='', end='')
for _ in range(int(input())):
    n, m = map(int, input().split())
    if sum(list(map(int, input().split()))) == m:
        print('YES')
    else:
        print('NO')
for _ in range(int(input())):
    n, m = map(int, input().split())
    if sum(list(map(int, input().split()))) == m:
        print('YES')
    else:
        print('NO')
n = input()
up = 0
low = 0
tam = len(n)
 for x in n:
    if x.isupper():
        up+=1
    else:
        low+=1
 if low>=up:
    n = n.lower()
else:
    n = n.upper()
 print(n)
import math
wej = (input())
wejc = (wej.split())
n = int(wejc[0])
m = int(wejc[1])
a = int(wejc[2])
w = math.ceil(n / a)
s = math.ceil(m / a)
if w * s == 0:
    print("1")
else:
    print(w * s)
 
t=int(input())
for _ in range(t):
    n,k,d=list(map(int,input().split()))
    a=list(map(int,input().split()))
    z=len(set(a[:d]))
    for i in range(n-d+1):
        z1=len(set(a[i:i+d]))
        if z1<z:
            z=z1
    print(z)
         
t = int(input())
 for casos in range(t):
 n, m, k = [int (x) for x in input ().split ()]
 cartas = n/k
 if cartas <= m:
  ganador = cartas
  m = m - ganador
 else:
  ganador = m
  m = 0
 maximo = 0
 if m > 0:
  maximo = int(m/(k-1))
  if m%(k-1) > 0:
   maximo = maximo + 1
 print(int(ganador-maximo))
t=int(input())
for _ in range(t):
    n,k,d=list(map(int,input().split()))
    a=list(map(int,input().split()))
    z=len(set(a[:d]))
    for i in range(n-d+1):
        z1=len(set(a[i:i+d]))
        if z1<z:
            z=z1
    print(z)
         
t = int(input())
 for casos in range(t):
 n, m, k = [int (x) for x in input ().split ()]
 cartas = n/k
 if cartas <= m:
  ganador = cartas
  m = m - ganador
 else:
  ganador = m
  m = 0
 maximo = 0
 if m > 0:
  maximo = int(m/(k-1))
  if m%(k-1) > 0:
   maximo = maximo + 1
 print(int(ganador-maximo))
for z in range(int(input())):
  x,y,n = map(int,input().split())
  m = n//x
  while(m*x+y) > n:
    m -= 1
  print(m*x+y)
for z in range(int(input())):
  x,y,n = map(int,input().split())
  m = n//x
  while(m*x+y) > n:
    m -= 1
  print(m*x+y)
from collections import Counter
t=int(input())
for i in range(t):
    n=int(input())
    arr=list(map(int,input().split()))
    arr1=[]
    for i in range(len(arr)):
        bnr = bin(arr[i]).replace('0b','')
        if bnr[0]=="1":
            arr1.append(len(bnr)-1)
    p=Counter(arr1)
    count=0
    for i in p.values():
        if i>=2:
            count+=((i-1)*((i-1)+1))//2
    print(count)
     
from collections import Counter
t=int(input())
for i in range(t):
    n=int(input())
    arr=list(map(int,input().split()))
    arr1=[]
    for i in range(len(arr)):
        bnr = bin(arr[i]).replace('0b','')
        if bnr[0]=="1":
            arr1.append(len(bnr)-1)
    p=Counter(arr1)
    count=0
    for i in p.values():
        if i>=2:
            count+=((i-1)*((i-1)+1))//2
    print(count)
     
n=int(input())
while n>0:
    a,b=input().split()
    a=int(a)
    b=int(b)
    c=0
    if a<b:
        for i in range(10,0,-1):
            diff=b-a
            a+=(diff//i)*i
            c+=diff//i
            if a==b:
                break
    elif a>b:
        for i in range(10,0,-1):
            diff=a-b
            a-=(diff//i)*i
            c+=diff//i
            if a==b:
                break
    print(c)
    n-=1
n=int(input())
while n>0:
    a,b=input().split()
    a=int(a)
    b=int(b)
    c=0
    if a<b:
        for i in range(10,0,-1):
            diff=b-a
            a+=(diff//i)*i
            c+=diff//i
            if a==b:
                break
    elif a>b:
        for i in range(10,0,-1):
            diff=a-b
            a-=(diff//i)*i
            c+=diff//i
            if a==b:
                break
    print(c)
    n-=1
w = int(input())
if 1 <= w <= 100:
    if w <= 2:
        print("NO")
    elif w > 2:
        second_half = w - 2
        if second_half % 2 == 0:
            print("YES")
        else:
            print("NO")
else:
    print("Enter something between 1 to 100")
import math
wej = (input())
wejc = (wej.split())
n = int(wejc[0])
m = int(wejc[1])
a = int(wejc[2])
w = math.ceil(n / a)
s = math.ceil(m / a)
if w * s == 0:
    print("1")
else:
    print(w * s)
 
import math
wej = (input())
wejc = (wej.split())
n = int(wejc[0])
m = int(wejc[1])
a = int(wejc[2])
w = math.ceil(n / a)
s = math.ceil(m / a)
if w * s == 0:
    print("1")
else:
    print(w * s)
 
a,b=list(map(int,input().split()))
l=[]
for i in range(a,b+1):
    f=0
    for j in range(2,i):
        if (i%j==0):
            f+=1
    if f==0:
        l.append(i)
if len(l)==1:
    print("NO")
else:
        if l[1]==b:
                print("YES")
    else:
        print("NO")
    
a,b=list(map(int,input().split()))
l=[]
for i in range(a,b+1):
    f=0
    for j in range(2,i):
        if (i%j==0):
            f+=1
    if f==0:
        l.append(i)
if len(l)==1:
    print("NO")
else:
        if l[1]==b:
                print("YES")
    else:
        print("NO")
    
#!/usr/bin/env python
# coding: utf-8<N><N># In[ ]:<N><N>
import cv2<N><N>face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")<N><N>img = cv2.imread("tim.jpg", 1)<N><N>gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>faces = face_cascade.detectMultiScale(gray_img, scaleFactor=1.05, minNeighbors=5)<N><N>for (x,y,w,h) in faces:
    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)<N><N>cv2.imshow("Face",img)<N><N>cv2.waitKey(0)<N><N>cv2.destroyAllWindows()<N><N>
#!/usr/bin/env python
# coding: utf-8<N><N>
# In[ ]:<N><N>

import numpy as np
import cv2<N><N>
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')<N><N>
img = cv2.imread('tim.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>
faces = face_cascade.detectMultiScale(gray, 1.3, 5)
for (x,y,w,h) in faces:
    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    roi_gray = gray[y:y+h, x:x+w]
    roi_color = img[y:y+h, x:x+w]
    eyes = eye_cascade.detectMultiScale(roi_gray)
    for (ex,ey,ew,eh) in eyes:
        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)<N><N>
cv2.imshow('img',img)
cv2.waitKey(0)
cv2.destroyAllWindows()<N><N>
from video_processing import df
from bokeh.plotting import figure,output_file,show
from bokeh.models import HoverTool,ColumnDataSource<N><N>
df["Start_string"] = df["Start"].dt.strftime("%Y-%m-%d %H:%M:%S")
df["End_string"] = df["End"].dt.strftime("%Y-%m-%d %H:%M:%S")
cds = ColumnDataSource(df)
p = figure(x_axis_type = "datetime",height=50,width=500,sizing_mode="scale_both",title="Motion graph",toolbar_location="below")<N><N>
p.quad(left="Start",right="End",bottom=0,top=1,color="red",source=cds)
p.yaxis.minor_tick_line_color = None
p.yaxis[0].ticker.desired_num_ticks = 1<N><N>
hover = HoverTool(tooltips = [("Start","@Start_string"),("End","@End_string")])
p.add_tools(hover)
output_file("motion_graph.html")
show(p)<N><N>
import cv2, time
from datetime import datetime 
import pandas<N><N>
 # github.com/DanyPalma #
# github.com/cfrankovich #
import cv2
import tkinter
from random import randrange<N><N>
# Helper Functions
def exitCallBack():
    cv2.destroyAllWindows()<N><N>
# boolean var to keep track of loop
running = True
#loading pre trained face data from opencv
trained_data = cv2.CascadeClassifier('TargetingSystem/Sources/haarcascade_frontalface_default.xml')
#grabbing webcam 
webcam = cv2.VideoCapture(0, cv2.CAP_DSHOW)<N><N>
while running:
    #reading webcam frames
    frame_read,frame = webcam.read()<N><N>
    # convert to black and white
    greyscale_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)<N><N>
    # detect faces
    face_cordinates = trained_data.detectMultiScale(greyscale_frame)
    #x and y is the upper left corner coordinate and w and h are the corresponding width and height of the rectangle
    for (x,y,w,h) in face_cordinates:
        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)
    # show image
    cv2.imshow("Final",frame)
    #agument 1 defines milliseconds
    
    if cv2.waitKey(33) == ord('q'):
        cv2.destroyAllWindows()
        running = False<N><N>
#   Author github.com/DanyPalma    #
# Forked by github.com/cfrankovich #<N><N>
import sys
import cv2
import numpy as np
from datetime import datetime
from colorama import Fore, Style<N><N>
trained_data = cv2.CascadeClassifier('TargetingSystem/Sources/haarcascade_frontalface_default.xml')
tCoord = (300,200)
capture = cv2.VideoCapture(0)
showHSV = False 
showCAM = False<N><N>

# Helper function aPos #
def aPos(mask):
	xA = 0
	yA = 0
	count = 0
	resolution = 10
	for y in range(0, height, resolution):
		for x in range(0, width, resolution):
			if mask[y][x] == 255:
				xA += x
				yA += y
				count += 1
	if count > 0:
		xA = xA / count
		yA = yA / count
	return (int(xA), int(yA))<N><N>
import cv2
import argparse
import numpy as np<N><N>ap = argparse.ArgumentParser()
ap.add_argument("--image", action = "append", type = str, help = "image path")
args = ap.parse_args()<N><N>if args.image:
    if len(args.image) > 1:
        image1 = cv2.imread(args.image[0])
        image2 = cv2.imread(args.image[1])
    else:
        image = cv2.imread(args.image[0])
import cv2 
import easygui 
import numpy as np 
import imageio <N><N>
import sys
import matplotlib.pyplot as plt
import os
import tkinter as tk
from tkinter import filedialog
from tkinter import *
from PIL import ImageTk, Image<N><N>
top=tk.Tk()
top.geometry('300x300')
top.title('Select image to cartoonify')
top.configure(background='white')
label=Label(top,background='#CDCDCD', font=('calibri',20,'bold'))<N><N>
def upload():
    ImagePath=easygui.fileopenbox()
    cartoonify(ImagePath)
    
def cartoonify(ImagePath):
    # read the image
    originalmage = cv2.imread(ImagePath)
    originalmage = cv2.cvtColor(originalmage, cv2.COLOR_BGR2RGB)
    ReSized1 = cv2.resize(originalmage, (460, 440))
 <N><N>
    #converting an image to grayscale
    grayScaleImage= cv2.cvtColor(originalmage, cv2.COLOR_BGR2GRAY)
    ReSized2 = cv2.resize(grayScaleImage, (460, 440))
   <N><N>

    #applying median blur to smoothen an image
    smoothGrayScale = cv2.medianBlur(grayScaleImage, 5)
    ReSized3 = cv2.resize(smoothGrayScale, (460, 440))
    <N><N>
    #retrieving the edges by using thresholding technique
    getEdge = cv2.adaptiveThreshold(smoothGrayScale, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 9)
    #getEdge = cv2.Canny(smoothGrayScale, 100,200)
    ReSized4 = cv2.resize(getEdge, (460, 440))<N><N>
    #applying bilateral filter 
    colorImage = cv2.bilateralFilter(originalmage, 9, 200, 200)
    ReSized5 = cv2.resize(colorImage, (460, 440))<N><N>
   
    #masking edged image with our "BEAUTIFY" image
    cartoonImage = cv2.bitwise_and(colorImage, colorImage, mask=getEdge)
    ReSized6 = cv2.resize(cartoonImage, (460, 440))<N><N>
    # Plotting the whole transition
    images=[ReSized1, ReSized2, ReSized3, ReSized4, ReSized5, ReSized6]<N><N>
    fig, axes = plt.subplots(3,2, figsize=(7,7), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i], cmap='gray')<N><N>
﻿
import cv2
import numpy as np
img = np.zeros(shape=[300, 800,1], dtype=np.uint8)
x="Sudoku - KI"
y="lade..."<N><N>
img = cv2.putText(img,str(x),(100,120),1,5,255,1)
img = cv2.putText(img,str(y),(100,250),1,5,255,1)
cv2.imshow("suso",img)<N><N>
import tkinter
cv2.moveWindow("suso",int(tkinter.Tk().winfo_screenwidth()/2-400),int(tkinter.Tk().winfo_screenheight()/2-150))
cv2.waitKey(10)<N><N>
import os<N><N>
zsize=300
Daten_sammeln = False<N><N>
if(Daten_sammeln==True):
	try:
		os.mkdir("dat")
	except:
		pass
	try:
		os.mkdir("dat/1")
	except:
		pass
	try:
		os.mkdir("dat/10")
	except:
		pass
	try:
		os.mkdir("dat/2")
	except:
		pass
	try:
		os.mkdir("dat/3")
	except:
		pass
	try:
		os.mkdir("dat/4")
	except:
		pass
	try:
		os.mkdir("dat/5")
	except:
		pass
	try:
		os.mkdir("dat/6")
	except:
		pass
	try:
		os.mkdir("dat/7")
	except:
		pass
	try:
		os.mkdir("dat/8")
	except:
		pass
	try:
		os.mkdir("dat/9")
	except:
		pass
	<N><N>

class feld:
	def __init__(self,_id,_x,_y,_num = 0, _v = False):
		self.id = _id
		self.num = _num
		self.v = _v
		self.x = _x
		self.y = _y
	
	anfang_da = False
	x = 0
	y = 0
	id = 0
	num = 0
	v = False<N><N>
class feld_liste_q:
	x = 0 
	y = 0
	felder = []<N><N>
class feld_liste_h:
	i = 0
	felder = []<N><N>
class feld_liste_v:
	i = 0
	felder = []<N><N>
fh = []
fv = []
fq = []<N><N>
def clv():
	for i in range(9):
		for o in range(9):
			fh[i].felder[o].v = False
			fv[o].felder[i].v = False
			fq[i].felder[o].v = False<N><N>
import tensorflow as tf
import glob
import cv2
import numpy as np
import random
import os
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)<N><N>
gen=True
read=False<N><N>
import numpy as np
import cv2
import time<N><N>
"""__
This program uses Haar-Cascades to detect cars. Uses Video6.mp4 as sample input. 
Detection is based off data in an XML file.<N><N>
"""<N><N>
W,H = 1280,720
car_cascade = cv2.CascadeClassifier('cars.xml')
cap = cv2.VideoCapture('cars.mp4')
prev_frame_time = 0
new_frame_time = 0
font = cv2.FONT_HERSHEY_SIMPLEX<N><N>
import numpy as np
import cv2
import time
"""
This program uses openCV to detect faces, smiles, and eyes. It uses haarcascades which are public domain. Haar cascades rely on
xml files which contain model training data. An xml file can be generated through training many positive and negative images. 
Try your built-in camera with 'cap = cv2.VideoCapture(0)' or use any video. cap = cv2.VideoCapture("videoNameHere.mp4")
"""<N><N>
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
cap = cv2.VideoCapture("face.mp4")
font = cv2.FONT_HERSHEY_SIMPLEX<N><N>
prev_frame_time, new_frame_time = 0,0
while 1:
    ret, img = cap.read()
    img = cv2.resize(img,(640,480))
    faces = face_cascade.detectMultiScale(img, 1.1, 5)
    eyes = eye_cascade.detectMultiScale(img,1.1,6)
    smiles = smile.detectMultiScale(img,1.1,100)<N><N>
    new_frame = time.time()
    fps = 1/(new_frame_time-prev_frame_time + 1)
    prev_frame_time = new_frame_time
    fps = int(fps)
    cv2.putText(img,"FPS: "+str(fps),(10,450), font, 3, (0,0,0), 5, cv2.LINE_AA)<N><N>
import cv2
import os
import numpy as np
import matplotlib.pyplot as plt<N><N>

imagesFolder = ''
croppedFolder = ''
height = 500
width = 500
BLACK_COLOR = (0, 0, 0)
RED_COLOR = (0, 0, 255)
averageCrop = []<N><N>

def renameImages():<N><N>
    '''
    Changes all the names of images to '1.png', '2.png' etc.
    '''<N><N>
    for imageNumber, image in enumerate(os.listdir(imagesFolder)):
        full_path = os.path.join(imagesFolder, image)
        try:
            os.rename(full_path, imagesFolder + str(imageNumber + 1) + '.png')
        except FileExistsError:
            pass<N><N>
def resizeImages():<N><N>
    '''
    Resized all images to a uniform size,
    You can change the size with the variables 'height' and 'width
    at the top of the file
    '''<N><N>
    for image in os.listdir(imagesFolder):
        img = cv2.imread(imagesFolder + image, 1)
        img = cv2.resize(img, (height, width), interpolation=cv2.INTER_AREA)
        isWritten = cv2.imwrite(imagesFolder + image, img)
        if not isWritten:
            print(image, ' Resized failded')<N><N>

def addText(img, text, size, textY):<N><N>
    '''
    Add text to display on image
    '''<N><N>
    font = cv2.FONT_HERSHEY_SIMPLEX
    textSize = cv2.getTextSize(text, font, size, 1)[0]
    textX = int((img.shape[1] - textSize[0]) / 2)
    cv2.putText(img, text, (textX, textY), font, size, RED_COLOR, 2)
    return img<N><N>

def updateImagePixelAverage(img):
    averageCrop.append(img.mean())<N><N>

def calculateFFT():<N><N>
    s = np.array(averageCrop)
    fft = np.fft.fft(s)
    fft = np.absolute(fft)
    plt.ylabel("Amplitude")
    plt.xlabel("Frequency [Hz]")
    plt.plot(fft)
    plt.show()<N><N>

def preProcessing(imgFolder):<N><N>
    '''
    Arranges and organizes the images
    '''<N><N>
    global imagesFolder, croppedFolder
    imagesFolder = imgFolder
    croppedFolder = os.path.dirname(imagesFolder[:-1]) + '/cropped/'
    #croppedFolder = os.path.dirname(imagesFolder + 'cropped/')
    Images.amountOfImages = len(os.listdir(imagesFolder))
    renameImages()
    resizeImages()<N><N>

class Images:<N><N>
    currentImage = 0
    amountOfImages = 0
    startPoint = None
    endPoint = None<N><N>
    def changeImage(self, request):<N><N>
        '''
        Change the image to display according to the user request
        '''<N><N>
import cv2
import cropImage
from cropImage import Images
from tkinter import *
from tkinter import filedialog<N><N>

imagesObj = Images()
startPoint = endPoint = pixel = None
croppedClickFlag = imagesCropped = False
BLACK_COLOR = (255, 0, 0)
height = 500
width = 500<N><N>

def startImagesDisplay():<N><N>
    '''
    This function manages the image viewer window
    '''<N><N>
    global croppedClickFlag, imagesCropped<N><N>
    def mouse_callback(event, x, y, flags, params):<N><N>
        '''
        Monitoring user clicks with the mouse on the image
        '''<N><N>
        global pixel, startPoint, endPoint, croppedClickFlag<N><N>
        if event == cv2.EVENT_LBUTTONDOWN:
            croppedClickFlag = False
            b, g, r = (img[x, y])
            pixel = (x, y, (r, g, b))
            imgT = imagesObj.dispalyPixel(pixel)
            cv2.imshow('image', imgT)
            startPoint = (x, y)<N><N>
        if event == cv2.EVENT_LBUTTONUP:
            endPoint = (x, y)
            if startPoint[0] != endPoint[0] and startPoint[1] != endPoint[1]:
                imgT = imagesObj.displayRectangleToCrop(startPoint, endPoint)
                croppedClickFlag = True
                cv2.imshow('image', imgT)<N><N>

    # Display the first image
    img = imagesObj.changeImage('next')
    cv2.namedWindow('image', cv2.WINDOW_AUTOSIZE)
    cv2.setMouseCallback('image', mouse_callback)
    cv2.imshow('image', img)<N><N>
    while True:<N><N>
        '''
        Monitoring user presses on the keyboard
        '''<N><N>
# # Gait Energy Image<N><N>
# In[1]:<N><N>

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
#from imageio import imread
from scipy.misc import imresize
from scipy.misc import imread
from skimage.transform import resize<N><N>

# In[2]:<N><N>

files = os.listdir('C:/Users/lenovo/Desktop/classification/fast/fq09')<N><N>
images = [imread('C:/Users/lenovo/Desktop/classification/fast/fq09/'+f) for f in files]<N><N>

# In[3]:<N><N>

plt.imshow(images[0])<N><N>

# In[4]:<N><N>

def mass_center(img,is_round=True):
    Y = img.mean(axis=1)
    X = img.mean(axis=0)
    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)
    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)
    if is_round:
        return int(round(X_)),int(round(Y_))
    return X_,Y_<N><N>
def image_extract(img,newsize):
    x_s = np.where(img.mean(axis=0)!=0)[0].min()
    x_e = np.where(img.mean(axis=0)!=0)[0].max()
    
    y_s = np.where(img.mean(axis=1)!=0)[0].min()
    y_e = np.where(img.mean(axis=1)!=0)[0].max()
    
    x_c,_ = mass_center(img)
#     x_c = (x_s+x_e)//2
    x_s = x_c-newsize[1]//2
    x_e = x_c+newsize[1]//2
    img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]
    img1=imresize(img,newsize)
    
    return  img1
    <N><N>
# In[5]:<N><N>

images = [image_extract(i,(128,64)) for i in images]<N><N>

# In[6]:<N><N>

plt.figure()
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(images[i])
plt.show()<N><N>

# In[7]:<N><N>

gei = np.mean(images,axis=0)
cv2.imwrite('/Users/lenovo/Desktop/classification/fastGEI/gei_fq09.png',gei)<N><N>
<N><N># In[8]:<N><N>

plt.imshow(gei)
plt.show()<N><N>
#if the data exist in keras then i need  too use these two lines
#but if i'm going to use my own data set then i don't need them
#from keras.datasets import mnist #replace mnist with any dataset
#(x_train, y_train), (x_test, y_test) = mnist.load_data()<N><N>

# to set up all the data
import numpy as np
import os
from matplotlib import pyplot as plt
import cv2
import random
import pickle<N><N>

file_list = []
class_list = []<N><N>
DATADIR = "C:/Users/lenovo/Desktop/projet vision final/dataGEI"<N><N>
# All the categories you want your neural network to detect
CATEGORIES = ["fastGEI", "normalGEI", "slowGEI"]<N><N>
# The size of the images that your neural network will use
IMG_SIZE = 50<N><N>
# Checking or all images in the data folder
for category in CATEGORIES :
    path = os.path.join(DATADIR, category)
    for img in os.listdir(path):
        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)
        #plt.imshow(img_array, cmap='gray')  # graph it
        
        #plt.show()  # display!<N><N>
        #break  # we just want one for now so break
        
training_data = []<N><N>
def create_training_data():
    for category in CATEGORIES :
        path = os.path.join(DATADIR, category)
        class_num = CATEGORIES.index(category)
        for img in os.listdir(path):
            try :
                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                training_data.append([new_array, class_num])
            except Exception as e:
                pass<N><N>
create_training_data()<N><N>
random.shuffle(training_data)<N><N>
X = [] #features
y = [] #labels
###############making our model
for features, label in training_data:
	X.append(features)
	y.append(label)<N><N>
X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)<N><N>
# Creating the files containing all the information about your model
#saving our data
pickle_out = open("X.pickle", "wb")
pickle.dump(X, pickle_out)
pickle_out.close()<N><N>
pickle_out = open("y.pickle", "wb")
pickle.dump(y, pickle_out)
pickle_out.close()<N><N>
pickle_in = open("X.pickle", "rb")
X = pickle.load(pickle_in)
#-----------------------------------------------------------------------
#Building the convolutional neural network
import tensorflow as tf 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
import pickle
from keras.models import model_from_json
from keras.models import load_model
import matplotlib.pyplot as plt<N><N>
# Opening the files about data
X = pickle.load(open("X.pickle", "rb"))
y = pickle.load(open("y.pickle", "rb"))<N><N>
# normalizing data (a pixel goes from 0 to 255)
X = X/255.0<N><N>
# Building the model
model = Sequential()
# 3 convolutional layers
model.add(Conv2D(32, (3, 3), input_shape = X.shape[1:]))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))<N><N>
model.add(Conv2D(64, (3, 3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))<N><N>
model.add(Conv2D(64, (3, 3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))<N><N>
# 2 hidden layers
model.add(Flatten())
model.add(Dense(128)) #nombre of categories
model.add(Activation("relu"))<N><N>
model.add(Dense(128))
model.add(Activation("relu"))<N><N>
# The output layer with 13 neurons, for 13 classes
model.add(Dense(13))
model.add(Activation("softmax"))<N><N>
# Compiling the model using some basic parameters
model.compile(loss="sparse_categorical_crossentropy",
				optimizer="adam",
				metrics=["accuracy"])<N><N>
# Training the model, with 40 iterations
# validation_split corresponds to the percentage of images used for the validation phase compared to all the images
history = model.fit(X, y, batch_size=32, epochs=40, validation_split=0.1)<N><N>
# Saving the model
model_json = model.to_json()
with open("model.json", "w") as json_file :
	json_file.write(model_json)<N><N>
model.save_weights("model.h5")
print("Saved model to disk")<N><N>
model.save('CNN.model')<N><N>
# Printing a graph showing the accuracy changes during the training phase
print(history.history.keys())
plt.figure(1)
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
<N><N>
# this version didn't work out
import tensorflow as tf
import keras.callbacks
#from keras.models import load_model<N><N>model = tf.keras.models.load_model("CNN.model")
image = "C:/Users/lenovo/Desktop/projet vision final/dataGEI/fastGEI/gei_fq01.png" #your image path<N><N>tb_test = keras.callbacks.TensorBoard(log_dir=strPath_model_test_logs,histogram_freq=0, write_graph=True, write_images=True)<N><N>y_test = model.predict(image, verbose=1, callbacks=[tb_test])

#if the data exist in keras then i need  too use these two lines
#but if i'm going to use my own data set then i don't need them
#from keras.datasets import mnist #replace mnist with any dataset
#(x_train, y_train), (x_test, y_test) = mnist.load_data()<N><N>

# to set up all the data
import numpy as np
import os
from matplotlib import pyplot as plt
import cv2
import random
import pickle<N><N>

file_list = []
class_list = []<N><N>
DATADIR = "C:/Users/lenovo/Desktop/projet vision final/dataGEI"<N><N>
# All the categories you want your neural network to detect
CATEGORIES = ["fastGEI", "normalGEI", "slowGEI"]<N><N>
# The size of the images that your neural network will use
IMG_SIZE = 50<N><N>
# Checking or all images in the data folder
for category in CATEGORIES :
    path = os.path.join(DATADIR, category)
    for img in os.listdir(path):
        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)
        #plt.imshow(img_array, cmap='gray')  # graph it
        
        #plt.show()  # display!<N><N>
        #break  # we just want one for now so break
        
training_data = []<N><N>
def create_training_data():
    for category in CATEGORIES :
        path = os.path.join(DATADIR, category)
        class_num = CATEGORIES.index(category)
        for img in os.listdir(path):
            try :
                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                training_data.append([new_array, class_num])
            except Exception as e:
                pass<N><N>
create_training_data()<N><N>
random.shuffle(training_data)<N><N>
X = [] #features
y = [] #labels
###############making our model
for features, label in training_data:
	X.append(features)
	y.append(label)<N><N>
X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)<N><N>
# Creating the files containing all the information about your model
#saving our data
pickle_out = open("X.pickle", "wb")
pickle.dump(X, pickle_out)
pickle_out.close()<N><N>
pickle_out = open("y.pickle", "wb")
pickle.dump(y, pickle_out)
pickle_out.close()<N><N>
pickle_in = open("X.pickle", "rb")
X = pickle.load(pickle_in)
#-----------------------------------------------------------------------
#Building the convolutional neural network
import tensorflow as tf 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
import pickle
from keras.models import model_from_json
from keras.models import load_model
import matplotlib.pyplot as plt<N><N>
# Opening the files about data
X = pickle.load(open("X.pickle", "rb"))
y = pickle.load(open("y.pickle", "rb"))<N><N>
# normalizing data (a pixel goes from 0 to 255)
X = X/255.0<N><N>
# Building the model
model = Sequential()
# 3 convolutional layers
model.add(Conv2D(32, (3, 3), input_shape = X.shape[1:]))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))<N><N>
model.add(Conv2D(64, (3, 3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))<N><N>
model.add(Conv2D(64, (3, 3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))<N><N>
# 2 hidden layers
model.add(Flatten())
model.add(Dense(128)) #nombre of categories
model.add(Activation("relu"))<N><N>
model.add(Dense(128))
model.add(Activation("relu"))<N><N>
# The output layer with 13 neurons, for 13 classes
model.add(Dense(13))
model.add(Activation("softmax"))<N><N>
# Compiling the model using some basic parameters
model.compile(loss="sparse_categorical_crossentropy",
				optimizer="adam",
				metrics=["accuracy"])<N><N>
# Training the model, with 40 iterations
# validation_split corresponds to the percentage of images used for the validation phase compared to all the images
history = model.fit(X, y, batch_size=32, epochs=40, validation_split=0.1)<N><N>
# Saving the model
model_json = model.to_json()
with open("model.json", "w") as json_file :
	json_file.write(model_json)<N><N>
model.save_weights("model.h5")
print("Saved model to disk")<N><N>
model.save('CNN.model')<N><N>
# Printing a graph showing the accuracy changes during the training phase
print(history.history.keys())
plt.figure(1)
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
<N><N>
# import the necessary packages
import numpy as np
import cv2
 
# initialize the HOG descriptor/person detector
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())<N><N>
cv2.startWindowThread()<N><N>
# open webcam video stream
#cap = cv2.VideoCapture(0)
#cap = cv2.VideoCapture('vtest.avi')
#cap = cv2.VideoCapture('speediMan.mp4')
cap = cv2.VideoCapture('2men.mp4')
#cap = cv2.VideoCapture('fn00.mp4')<N><N>
# the output will be written to output.avi
out = cv2.VideoWriter(
    'output.avi',
    cv2.VideoWriter_fourcc(*'MJPG'),
    15.,
    (640,480))<N><N>
while(True):
    # Capture frame-by-frame
    ret, frame = cap.read()<N><N>
    # resizing for faster detection
    frame = cv2.resize(frame, (640, 480))
    # using a greyscale picture, also for faster detection
    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)<N><N>
    # detect people in the image
    # returns the bounding boxes for the detected objects
    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )<N><N>
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])<N><N>
    for (xA, yA, xB, yB) in boxes:
        # display the detected boxes in the colour picture
        cv2.rectangle(frame, (xA, yA), (xB, yB),(0, 255, 0), 2)
    
    # Write the output video 
    out.write(frame.astype('uint8'))
    # Display the resulting frame
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>
# When everything done, release the capture
cap.release()
# and release the output
out.release()
# finally, close the window
cv2.destroyAllWindows()
cv2.waitKey(1)<N><N>
# # Gait Energy Image<N><N>
# In[1]:<N><N>

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
#from imageio import imread
from scipy.misc import imresize
from scipy.misc import imread
from skimage.transform import resize<N><N>

# In[2]:<N><N>

files = os.listdir('C:/Users/lenovo/Desktop/classification/fast/fq09')<N><N>
images = [imread('C:/Users/lenovo/Desktop/classification/fast/fq09/'+f) for f in files]<N><N>

# In[3]:<N><N>

plt.imshow(images[0])<N><N>

# In[4]:<N><N>

def mass_center(img,is_round=True):
    Y = img.mean(axis=1)
    X = img.mean(axis=0)
    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)
    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)
    if is_round:
        return int(round(X_)),int(round(Y_))
    return X_,Y_<N><N>
def image_extract(img,newsize):
    x_s = np.where(img.mean(axis=0)!=0)[0].min()
    x_e = np.where(img.mean(axis=0)!=0)[0].max()
    
    y_s = np.where(img.mean(axis=1)!=0)[0].min()
    y_e = np.where(img.mean(axis=1)!=0)[0].max()
    
    x_c,_ = mass_center(img)
#     x_c = (x_s+x_e)//2
    x_s = x_c-newsize[1]//2
    x_e = x_c+newsize[1]//2
    img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]
    img1=imresize(img,newsize)
    
    return  img1
    <N><N>
# In[5]:<N><N>

images = [image_extract(i,(128,64)) for i in images]<N><N>

# In[6]:<N><N>

plt.figure()
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(images[i])
plt.show()<N><N>

# In[7]:<N><N>

gei = np.mean(images,axis=0)
cv2.imwrite('/Users/lenovo/Desktop/classification/fastGEI/gei_fq09.png',gei)<N><N>
<N><N># In[8]:<N><N>

plt.imshow(gei)
plt.show()<N><N>
# # Gait Energy Image<N><N>
# In[1]:<N><N>

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
#from imageio import imread
from scipy.misc import imresize
from scipy.misc import imread
from skimage.transform import resize<N><N>

# In[2]:<N><N>

files = os.listdir('C:/Users/lenovo/Desktop/classification/fast/fq09')<N><N>
images = [imread('C:/Users/lenovo/Desktop/classification/fast/fq09/'+f) for f in files]<N><N>

# In[3]:<N><N>

plt.imshow(images[0])<N><N>

# In[4]:<N><N>

def mass_center(img,is_round=True):
    Y = img.mean(axis=1)
    X = img.mean(axis=0)
    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)
    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)
    if is_round:
        return int(round(X_)),int(round(Y_))
    return X_,Y_<N><N>
def image_extract(img,newsize):
    x_s = np.where(img.mean(axis=0)!=0)[0].min()
    x_e = np.where(img.mean(axis=0)!=0)[0].max()
    
    y_s = np.where(img.mean(axis=1)!=0)[0].min()
    y_e = np.where(img.mean(axis=1)!=0)[0].max()
    
    x_c,_ = mass_center(img)
#     x_c = (x_s+x_e)//2
    x_s = x_c-newsize[1]//2
    x_e = x_c+newsize[1]//2
    img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]
    img1=imresize(img,newsize)
    
    return  img1
    <N><N>
# In[5]:<N><N>

images = [image_extract(i,(128,64)) for i in images]<N><N>

# In[6]:<N><N>

plt.figure()
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(images[i])
plt.show()<N><N>

# In[7]:<N><N>

gei = np.mean(images,axis=0)
cv2.imwrite('/Users/lenovo/Desktop/classification/fastGEI/gei_fq09.png',gei)<N><N>
<N><N># In[8]:<N><N>

plt.imshow(gei)
plt.show()<N><N>
# # Gait Energy Image<N><N>
# In[1]:<N><N>

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
#from imageio import imread
from scipy.misc import imresize
from scipy.misc import imread
from skimage.transform import resize<N><N>

# In[2]:<N><N>

files = os.listdir('C:/Users/lenovo/Desktop/classification/normal/fn09')
images = [imread('C:/Users/lenovo/Desktop/classification/normal/fn09/'+f) for f in files]<N><N>

# In[3]:<N><N>

plt.imshow(images[0])<N><N>

# In[4]:<N><N>

def mass_center(img,is_round=True):
    Y = img.mean(axis=1)
    X = img.mean(axis=0)
    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)
    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)
    if is_round:
        return int(round(X_)),int(round(Y_))
    return X_,Y_<N><N>
def image_extract(img,newsize):
    x_s = np.where(img.mean(axis=0)!=0)[0].min()
    x_e = np.where(img.mean(axis=0)!=0)[0].max()
    
    y_s = np.where(img.mean(axis=1)!=0)[0].min()
    y_e = np.where(img.mean(axis=1)!=0)[0].max()
    
    x_c,_ = mass_center(img)
#     x_c = (x_s+x_e)//2
    x_s = x_c-newsize[1]//2
    x_e = x_c+newsize[1]//2
    img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]
    img1=imresize(img,newsize)
    
    return  img1
    <N><N>
# In[5]:<N><N>

images = [image_extract(i,(128,64)) for i in images]<N><N>

# In[6]:<N><N>

plt.figure()
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(images[i])
plt.show()<N><N>

# In[7]:<N><N>

gei = np.mean(images,axis=0)
cv2.imwrite('/Users/lenovo/Desktop/classification/normalGEI/gei_fn09.png',gei)<N><N>

# In[8]:<N><N>

plt.imshow(gei)
plt.show()<N><N>
# # Gait Energy Image<N><N>
# In[1]:<N><N>

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
#from imageio import imread
from scipy.misc import imresize
from scipy.misc import imread
from skimage.transform import resize<N><N>

# In[2]:<N><N>

files = os.listdir('C:/Users/lenovo/Desktop/classification/slow/fs09')
images = [imread('/Users/lenovo/Desktop/classification/slow/fs09/'+f) for f in files]<N><N>

# In[3]:<N><N>

plt.imshow(images[0])<N><N>

# In[4]:<N><N>

def mass_center(img,is_round=True):
    Y = img.mean(axis=1)
    X = img.mean(axis=0)
    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)
    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)
    if is_round:
        return int(round(X_)),int(round(Y_))
    return X_,Y_<N><N>
def image_extract(img,newsize):
    x_s = np.where(img.mean(axis=0)!=0)[0].min()
    x_e = np.where(img.mean(axis=0)!=0)[0].max()
    
    y_s = np.where(img.mean(axis=1)!=0)[0].min()
    y_e = np.where(img.mean(axis=1)!=0)[0].max()
    
    x_c,_ = mass_center(img)
#     x_c = (x_s+x_e)//2
    x_s = x_c-newsize[1]//2
    x_e = x_c+newsize[1]//2
    img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]
    img1=imresize(img,newsize)
    
    return  img1
    <N><N>
# In[5]:<N><N>

images = [image_extract(i,(128,64)) for i in images]<N><N>

# In[6]:<N><N>

plt.figure()
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(images[i])
plt.show()<N><N>

# In[7]:<N><N>

gei = np.mean(images,axis=0)
cv2.imwrite('/Users/lenovo/Desktop/classification/slowGEI/gei_fs09.png',gei)<N><N>

# In[8]:<N><N>

plt.imshow(gei)
plt.show()<N><N>
# import the necessary packages
import numpy as np
import cv2
 
# initialize the HOG descriptor/person detector
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())<N><N>
cv2.startWindowThread()<N><N>
# open webcam video stream
#cap = cv2.VideoCapture(0)
#cap = cv2.VideoCapture('vtest.avi')
#cap = cv2.VideoCapture('speediMan.mp4')
cap = cv2.VideoCapture('2men.mp4')
#cap = cv2.VideoCapture('fn00.mp4')<N><N>
# the output will be written to output.avi
out = cv2.VideoWriter(
    'output.avi',
    cv2.VideoWriter_fourcc(*'MJPG'),
    15.,
    (640,480))<N><N>
while(True):
    # Capture frame-by-frame
    ret, frame = cap.read()<N><N>
    # resizing for faster detection
    frame = cv2.resize(frame, (640, 480))
    # using a greyscale picture, also for faster detection
    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)<N><N>
    # detect people in the image
    # returns the bounding boxes for the detected objects
    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )<N><N>
    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])<N><N>
    for (xA, yA, xB, yB) in boxes:
        # display the detected boxes in the colour picture
        cv2.rectangle(frame, (xA, yA), (xB, yB),(0, 255, 0), 2)
    
    # Write the output video 
    out.write(frame.astype('uint8'))
    # Display the resulting frame
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>
# When everything done, release the capture
cap.release()
# and release the output
out.release()
# finally, close the window
cv2.destroyAllWindows()
cv2.waitKey(1)<N><N>
import requests
import cv2
import numpy as np
import imutils<N><N>
# Replace the below URL with your own. Make sure to add "/shot.jpg" at last.
url = "http://192.168.0.101:8080/shot.jpg"<N><N>
# While loop to continuously fetching data from the Url
while True:
    img_resp = requests.get(url)
    img_arr = np.array(bytearray(img_resp.content), dtype=np.uint8)
    img = cv2.imdecode(img_arr, -1)
    img = imutils.resize(img, width=1000, height=1800)
    cv2.imshow("Android_cam", img)<N><N>
    # Press Esc key to exit
    if cv2.waitKey(1) == 27:
        break<N><N>
cv2.destroyAllWindows()<N><N>
import cv2<N><N>
#img = cv2.imread('rand.JPG')
cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)<N><N>

classFile = 'coco.names'
with open(classFile,'rt') as f:
    classNames = f.read().rstrip('\n').split('\n')
print(classNames)<N><N>
configPath = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'
weightsPath = 'frozen_inference_graph.pb'<N><N>
net = cv2.dnn_DetectionModel(weightsPath,configPath)
net.setInputSize(320,320)
net.setInputScale(1.0 / 127.5)
net.setInputMean ((127.5, 127.5, 127.5))
net.setInputSwapRB(True)<N><N>
while True:
    success, img = cap.read()
    classIds, confs, bbox = net.detect(img,confThreshold=0.5)
    print(classIds,bbox)<N><N>
    if len(classIds) != 0:
        for classId, confidence, box in zip(classIds.flatten(),confs.flatten(),bbox):
            cv2.rectangle(img,box,color=(0,255,0))
            cv2.putText(img,classNames[classId-1] +" "+ str(int(confidence*100))+"%",(box[0]+10,box[1]+30),
                        cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255))<N><N>
<N><N>    cv2.imshow("Output",img)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
<N><N>
import numpy as np
from keras.models import load_model
from keras.preprocessing import image
import cv2<N><N>
my_model = load_model('FaceMask_model.h5')
c = cv2.VideoCapture(0)
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N>
#importing dependencies
import numpy as np
from matplotlib import pyplot as plt
import tensorflow
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense
from keras.preprocessing.image import ImageDataGenerator<N><N>
#Creating sequential cnn model for training<N><N>
model1 = Sequential()
model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model1.add(MaxPooling2D())
model1.add(Conv2D(32, (3, 3), activation='relu'))
model1.add(MaxPooling2D())
model1.add(Conv2D(32, (3, 3), activation='relu'))
model1.add(MaxPooling2D())
model1.add(Flatten())
model1.add(Dense(100, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))<N><N>
#Pre compilation before training model.
model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])<N><N>
train_datagen = ImageDataGenerator(rescale=1.0/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)<N><N>
test_datagen = ImageDataGenerator(rescale=1.0/255)
train='train'
test='test'<N><N>
train_set = train_datagen.flow_from_directory(
        train,
        target_size=(150, 150),
        batch_size=16,
        class_mode='binary')<N><N>
test_set = test_datagen.flow_from_directory(
        test,
        target_size=(150,150),
        batch_size=16,
        class_mode='binary')<N><N>
model_saved1=model1.fit(
        train_set,
        epochs=10,
        validation_data=test_set
        )<N><N>
model1.save('FaceMask_model_e10_2500_new.h5', model_saved1)<N><N>

N = 10
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), model_saved1.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), model_saved1.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), model_saved1.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), model_saved1.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Loss or Accuracy")
plt.legend(loc="upper right")
<N><N>
import urllib.request
import cv2
import numpy as np
import time<N><N>
# Replace the URL with your own IPwebcam shot.jpg IP:port
url='http://192.168.0.101:8080/shot.jpg'<N><N>

while True:
    # Use urllib to get the image from the IP camera
    imgResp = urllib.request.urlopen(url)
    
    # Numpy to convert into a array
    imgNp = np.array(bytearray(imgResp.read()),dtype=np.uint8)
    
    # Finally decode the array to OpenCV usable format ;) 
    img = cv2.imdecode(imgNp,-1)
	
	
	# put the image on screen
    cv2.imshow('IPWebcam',img)<N><N>
    #To give the processor some less stress
    #time.sleep(0.1) <N><N>
    # Quit if q is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
<N><N>
# -*- coding: utf-8 -*-
"""
Created on Fri May  7 09:50:00 2021
@author: Giulio Cornelio Grossi Ph.D
@email: giulio.cornelio.grossi@gmail.com<N><N>
The code below implements some useful 
function and classes used to perform
different basic image processing tasks
"""<N><N>
# Import necessary packages
import cv2
import numpy as np
import plotly.express as px
from scipy.spatial import distance as dist
from plotly.offline import plot
from plotly.subplots import make_subplots<N><N>
def contour_center(c):
    # - Computes the center of the contour  
    M = cv2.moments(c)
    if M["m00"]>0: 
        cX = int((M["m10"] / M["m00"]))
        cY = int((M["m01"] / M["m00"]))
    else:    
        cX=-999
        cY=-999 
    return cX,cY<N><N>
def create_contour_mask(c,img):
    # - Constructs a mask for the contour
    mask = np.zeros(img.shape[:2], dtype="uint8")
    cv2.drawContours(mask, [c], -1, 255, -1)
    return mask<N><N>
def mean_std(image,mask):
    # - Calculates mean and std of a masked region
    # - Returns a more handy tuple with the values
    mean, std = cv2.meanStdDev(image, mask=mask)
    m=(mean[0][0],mean[1][0],mean[2][0])
    s=(std[0][0],std[1][0],std[2][0]) 
    return m,s<N><N>
# Code that gets input of an image then attempts to locate this image from a videostream using feature detection
# by Khan,Haritha<N><N>
import cv2
import numpy as np
import time
import math
import imutils<N><N>
t = time.time()<N><N>
video = cv2.VideoCapture(0)<N><N>
while True:<N><N>
    # Obtain Colored Images<N><N>
    img1c = cv2.imread("shabnams_bag.jpg")  # ,cv2.IMREAD_GRAYSCALE)
    # print(type(img1c))
    # img2c = cv2.imread("shabnams_bag_test.jpg")#,cv2.IMREAD_GRAYSCALE)
    success, img2c = video.read()<N><N>
    img2c = imutils.resize(img2c, width=700)<N><N>
    # Convert to GrayScale
    img1 = cv2.cvtColor(img1c, cv2.COLOR_BGR2GRAY)
    img2 = cv2.cvtColor(img2c, cv2.COLOR_BGR2GRAY)<N><N>
    # Creat ORB object (Feature Detection Method)
    tracker = cv2.ORB_create(nfeatures=100)<N><N>
    # Get keypoints and descriptors
    kp1, des1 = tracker.detectAndCompute(img1, None)
    kp2, des2 = tracker.detectAndCompute(img2, None)<N><N>
    # Match keypoints by descriptors using BRUTE FORCE method
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(des1, des2)  # matching
    # sort shortest distances
    matches = sorted(matches, key=lambda x: x.distance)
    number_of_matches = 10  # choose 10 closest points<N><N>
    # Get X,Y Coordinates of both the base & test picture
    base_keypoints = np.float32(
        [kp1[m.queryIdx].pt for m in matches[:number_of_matches]]).reshape(-1, 1, 2)
    test_keypoints = np.float32(
        [kp2[m.trainIdx].pt for m in matches[:number_of_matches]]).reshape(-1, 1, 2)<N><N>
    # Create Array for Distances
    array_dis = np.empty(number_of_matches)<N><N>
    # Get the average x & y coordinates
    avg = np.mean(test_keypoints, axis=0)<N><N>
    x_avg = avg[0][0]  # average x coordinates
    y_avg = avg[0][1]  # average y coordinates<N><N>
    count = 0
    # loop over the test picture keypoint coordinates
    for i in test_keypoints:
        x = i[0][0]
        y = i[0][1]
        a = (x - x_avg)**2  # x_dist
        b = (y - y_avg)**2  # y_dist
        array_dis[count] = math.sqrt(a + b)
        count = count+1<N><N>
    dis_avg = np.mean(array_dis)  # get average dist
    diameter = 1.5*dis_avg  # create virtual diameter for marking<N><N>
    # Print Circle on test image for marking
    test_img = cv2.circle(img2c, (int(avg[0][0]), int(
        avg[0][1])), int(diameter), (0, 255, 0), 2)<N><N>
    # print(time.time()-t)
    # print("==========================================================")<N><N>
    cv2.imshow("image1", img1c)
    cv2.imshow("image2", img2)
    cv2.imshow("test_image", test_img)<N><N>
    key = cv2.waitKey(1)
    if key == ord("q"):
        break<N><N>
    # print("===============")<N><N>
# video.release()
cv2.destroyAllWindows
<N><N>
#Code that gets input of an image and matches the object found in another image
#by Khan, Haritha<N><N>
import cv2
import numpy as np
import time<N><N>
t= time.time()<N><N>
img1 = cv2.imread("drone_test_pic.jpg",cv2.IMREAD_GRAYSCALE)
img2 = cv2.imread("test_picture2.jpg",cv2.IMREAD_GRAYSCALE)<N><N>
#ORB creation
tracker =cv2.ORB_create(nfeatures=100)<N><N>
#Get keypoints and descriptors
kp1 , des1 = tracker.detectAndCompute(img1, None)
kp2 , des2 = tracker.detectAndCompute(img2, None)<N><N>
#Match keypoints by descriptors<N><N>
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) #bruteforce matching method
matches = bf.match(des1,des2)
matches = sorted(matches, key = lambda x:x.distance)<N><N>
number_of_matches = 5<N><N>
matching_result= cv2.drawMatches(img1, kp1, img2, kp2, matches[:number_of_matches], None, flags=2)<N><N>

# Debug print - Draw first 10 matches.<N><N>
#matches_img = cv2.drawMatches(base_gray, kp_base, test_gray, kp_test, matches[:number_of_matches], flags=2, outImg=base_gray)<N><N>
#output.debug_show("Matches", matches_img, debug_mode=debug_mode,fxy=fxy,waitkey=True)<N><N>

# calculate transformation matrix
base_keypoints = np.float32([kp1[m.queryIdx].pt for m in matches[:number_of_matches]]).reshape(-1, 1, 2)
test_keypoints = np.float32([kp2[m.trainIdx].pt for m in matches[:number_of_matches]]).reshape(-1, 1, 2)<N><N>
# print(base_keypoints)
# print(test_keypoints)
# Calculate Homography
h, status = cv2.findHomography(base_keypoints, test_keypoints)<N><N>
<N><N>#print(time.time()-t)<N><N>
cv2.imshow("image1", img1)
cv2.imshow("image2", img2)
cv2.imshow("Matched_Result",matching_result)<N><N>
# print(time.time()-t)
# print("======================================================")<N><N>
cv2.waitKey(0)
cv2.destroyAllWindows
<N><N>
# Code that tracks a bounding box selection made by the user after pressing 's'
# Press 'q' to quit video stream<N><N>
# import the necessary packages
from imutils.video import VideoStream
from imutils.video import FPS
import imutils
import time
import cv2
import math
from adafruit_servokit import ServoKit<N><N>

# Initialize servokit
kit = ServoKit(channels=16)<N><N>
#Trackers in opencv
OPENCV_OBJECT_TRACKERS = {
    "csrt": cv2.TrackerCSRT_create,
    "kcf": cv2.TrackerKCF_create,
    "boosting": cv2.TrackerBoosting_create,
    "mil": cv2.TrackerMIL_create,
    "tld": cv2.TrackerTLD_create,
    "medianflow": cv2.TrackerMedianFlow_create,
    "mosse": cv2.TrackerMOSSE_create
}<N><N>
# grab the appropriate object tracker using our dictionary of
# OpenCV object tracker objects
tracker = OPENCV_OBJECT_TRACKERS["csrt"]()<N><N>
# initialize the bounding box coordinates of the object we are going to track
initBB = None<N><N>

print("[INFO] starting video stream...")
vs = VideoStream(src=0).start()
time.sleep(1.0)<N><N>
# initialize the FPS throughput estimator
fps = None<N><N>
counter = 0<N><N>
old_anglex = 45
old_angley = 40<N><N>
kit.servo[1].angle = old_anglex
kit.servo[0].angle = old_angley<N><N>
# loop over frames from the video stream
while True:
    # grab the current frame, then handle if we are using a
    # VideoStream or VideoCapture object
    frame = vs.read()
    frame = cv2.flip(frame, 1)
    # frame = frame[1] if args.get("video", False) else frame
    # time.sleep(0.03)<N><N>
    # check to see if we have reached the end of the stream
    if frame is None:
        break<N><N>
    # resize the frame (so we can process it faster) and grab the
    # frame dimensions
    frame = imutils.resize(frame, width=700)
    (H, W) = frame.shape[:2]<N><N>
# check to see if we are currently tracking an object
    if initBB is not None:
        # grab the new bounding box coordinates of the object
        (success, box) = tracker.update(frame)<N><N>
        # check to see if the tracking was a success
        if success:
            (x, y, w, h) = [int(v) for v in box]
            cv2.rectangle(frame, (x, y), (x + w, y + h),
                          (0, 255, 0), 2)
        # update the FPS counter
        fps.update()
        fps.stop()<N><N>
        # get position of target
        center_pos_hor = x				# 0 - 700 (0 is left)
        center_pos_ver = y				# 0 - 500 (0 is top)
        x_pos = x + (w/2)
        y_pos = y + (h/2)<N><N>
        # for horizontal
        # Angles from 0 to 90
        # frame from 0 to 700
        servoxangle = abs(90 - (math.floor((x_pos / 700) * 90)))<N><N>
        # for vertical
        # Angles from 0 to 40
        # frame from 0 to 500
        servoyangle = abs(40 - (math.floor((y_pos / 500) * 40)))<N><N>
        kit.servo[1].angle = servoxangle
        kit.servo[0].angle = servoyangle<N><N>
        old_anglex = servoxangle
        old_angley = servoyangle<N><N>
        # initialize the set of information we'll be displaying on
        info = [
            ("Tracker", "csrt"),
            ("Success", "Yes" if success else "No"),
            ("FPS", "{:.2f}".format(fps.fps())),
            ("Vertical", "{:.2f}".format(center_pos_ver)),
            ("Horizontal", "{:.2f}".format(center_pos_hor)),
        ]<N><N>
        # loop over the info tuples and draw them on our frame
        for (i, (k, v)) in enumerate(info):
            text = "{}: {}".format(k, v)
            cv2.putText(frame, text, (10, H - ((i * 20) + 20)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)<N><N>
    counter = counter + 1
# show the output frame
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1) & 0xFF<N><N>
    # if the 's' key is selected, we are going to "select" a bounding
    # box to track
    if key == ord("s"):
        # select the bounding box of the object we want to track (make
        # sure you press ENTER or SPACE after selecting the ROI)
        initBB = cv2.selectROI("Frame", frame, fromCenter=False,
                               showCrosshair=True)<N><N>
        # start OpenCV object tracker using the supplied bounding box
        # coordinates, then start the FPS throughput estimator as well
        tracker.init(frame, initBB)
        fps = FPS().start()<N><N>
# if the `q` key was pressed, break from the loop
    elif key == ord("q"):
        break<N><N>
vs.stop()
# close all windows
cv2.destroyAllWindows()
<N><N>
# Code that tracks a bounding box selection made by the user after pressing 's'
# Press 'q' to quit video stream<N><N>
# by Khan, Haritha<N><N>
# import the necessary packages
from imutils.video import VideoStream
from imutils.video import FPS
import argparse
import imutils
import time
import cv2<N><N>
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-v", "--video", type=str, help="path to input video file")
ap.add_argument("-t", "--tracker", type=str, default="kcf",
                help="OpenCV object tracker type")
args = vars(ap.parse_args())<N><N>
#Trackers in opencv
OPENCV_OBJECT_TRACKERS = {
    "csrt": cv2.TrackerCSRT_create,
    "kcf": cv2.TrackerKCF_create,
    "boosting": cv2.TrackerBoosting_create,
    "mil": cv2.TrackerMIL_create,
    "tld": cv2.TrackerTLD_create,
    "medianflow": cv2.TrackerMedianFlow_create,
    "mosse": cv2.TrackerMOSSE_create
}<N><N>
# grab the appropriate object tracker using our dictionary of
# OpenCV object tracker objects
tracker = OPENCV_OBJECT_TRACKERS[args["tracker"]]()<N><N>
# initialize the bounding box coordinates of the object we are going to track
initBB = None
# if a video path was not supplied, grab the reference to the web cam
if not args.get("video", False):
    print("[INFO] starting video stream...")
    vs = VideoStream(src=0).start()
    time.sleep(1.0)<N><N>
# otherwise, grab a reference to the video file
else:
    vs = cv2.VideoCapture(args["video"])<N><N>
# initialize the FPS throughput estimator
fps = None<N><N>
# loop over frames from the video stream
while True:
    # grab the current frame, then handle if we are using a
    # VideoStream or VideoCapture object
    frame = vs.read()
    frame = cv2.flip(frame, 1)
    frame = frame[1] if args.get("video", False) else frame
    # time.sleep(0.03)<N><N>
    # check to see if we have reached the end of the stream
    if frame is None:
        break<N><N>
    # resize the frame (so we can process it faster) and grab the
    # frame dimensions
    frame = imutils.resize(frame, width=700)
    (H, W) = frame.shape[:2]<N><N>

# check to see if we are currently tracking an object
    if initBB is not None:
        # grab the new bounding box coordinates of the object
        (success, box) = tracker.update(frame)<N><N>
        # check to see if the tracking was a success
        if success:
            (x, y, w, h) = [int(v) for v in box]
            cv2.rectangle(frame, (x, y), (x + w, y + h),
                          (0, 255, 0), 2)<N><N>
        # update the FPS counter
        fps.update()
        fps.stop()<N><N>
        # initialize the set of information we'll be displaying on
        # the frame
        info = [
            ("Tracker", args["tracker"]),
            ("Success", "Yes" if success else "No"),
            ("FPS", "{:.2f}".format(fps.fps())),
        ]<N><N>
        # loop over the info tuples and draw them on our frame
        for (i, (k, v)) in enumerate(info):
            text = "{}: {}".format(k, v)
            cv2.putText(frame, text, (10, H - ((i * 20) + 20)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)<N><N>

# show the output frame
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1) & 0xFF<N><N>
    # if the 's' key is selected, we are going to "select" a bounding
    # box to track
    if key == ord("s"):
        # select the bounding box of the object we want to track (make
        # sure you press ENTER or SPACE after selecting the ROI)
        initBB = cv2.selectROI("Frame", frame, fromCenter=False,
                               showCrosshair=True)<N><N>
        # start OpenCV object tracker using the supplied bounding box
        # coordinates, then start the FPS throughput estimator as well
        tracker.init(frame, initBB)
        fps = FPS().start()<N><N>
# if the `q` key was pressed, break from the loop
    elif key == ord("q"):
        break<N><N>

# if we are using a webcam, release the pointer
if not args.get("video", False):
    vs.stop()<N><N>
# otherwise, release the file pointer
else:
    vs.release()<N><N>
# close all windows
cv2.destroyAllWindows()
<N><N>
# Code that tracks an object that is in a predifined box in the videostream, user can press 'enter' to start the tracker
# and can control the size of the predifined box by (8,6,4,2) on keyboard<N><N>
# by Muhammad Khan and Haritha Seddik<N><N>
# import the necessary packages
from imutils.video import VideoStream
from imutils.video import FPS
import argparse  # for functions
import imutils  # for image processing functions
import time
import cv2  # computer vision and machine learning<N><N>
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-v", "--video", type=str, help="path to input video file")
ap.add_argument("-t", "--tracker", type=str, default="kcf",
                help="OpenCV object tracker type")
args = vars(ap.parse_args())<N><N>

# OpenCV object tracker implementations
OPENCV_OBJECT_TRACKERS = {
    "csrt": cv2.TrackerCSRT_create,
    "kcf": cv2.TrackerKCF_create,
    "boosting": cv2.TrackerBoosting_create,
    "mil": cv2.TrackerMIL_create,
    "tld": cv2.TrackerTLD_create,
    "medianflow": cv2.TrackerMedianFlow_create,
    "mosse": cv2.TrackerMOSSE_create
}<N><N>
# grab the appropriate object tracker using our dictionary of
# OpenCV object tracker objects
tracker = OPENCV_OBJECT_TRACKERS[args["tracker"]]()<N><N>
# initialize the bounding box coordinates of the object we are going
# to track
initBB = None
flag = False
a = 10
b = 230
t_w = 100
t_h = 100<N><N>
# if a video path was not supplied, grab the reference to the web cam
if not args.get("video", False):
    print("[INFO] starting video stream...")
    vs = VideoStream(src=0).start()
    time.sleep(1.0)<N><N>
# otherwise, grab a reference to the video file
else:
    vs = cv2.VideoCapture(args["video"])<N><N>
# initialize the FPS throughput estimator
fps = None<N><N>
# loop over frames from the video stream
while True:<N><N>
    # grab the current frame, then handle if we are using a
    # VideoStream or VideoCapture object
    frame = vs.read()
    frame = frame[1] if args.get("video", False) else frame
    frame = cv2.flip(frame, 1)
    # time.sleep(0.03)<N><N>
    # check to see if we have reached the end of the stream
    if frame is None:
        break<N><N>
    # resize the frame (so we can process it faster) and grab the
    # frame dimensions
    frame = imutils.resize(frame, width=700)
    (H, W) = frame.shape[:2]<N><N>

# check to see if we are currently tracking an object
    if initBB is not None:
        flag = True
        # grab the new bounding box coordinates of the object
        (success, box) = tracker.update(frame)<N><N>
        # check to see if the tracking was a success
        if success:
            (x, y, w, h) = [int(v) for v in box]
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)<N><N>
            #hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)<N><N>
        # update the FPS counter
        fps.update()
        fps.stop()<N><N>
        # initialize the set of information we'll be displaying on
        # the frame
        info = [
            ("Tracker", args["tracker"]),
            ("Success", "Yes" if success else "No"),
            ("FPS", "{:.2f}".format(fps.fps())),
        ]<N><N>
        # loop over the info tuples and draw them on our frame
        for (i, (k, v)) in enumerate(info):
            text = "{}: {}".format(k, v)
            cv2.putText(frame, text, (10, H - ((i * 20) + 20)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)<N><N>
    # show the output frame
    # print(int((frame.shape[0]/2)-(t_h/2)))
    if flag == False:
        cv2.rectangle(frame, (a, b), (a+t_w, b+t_h), (0, 0, 255), 2)<N><N>
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1)  # & 0xFF<N><N>
    if key == ord(" "):
        initBB = (a, b, t_w, t_h)
        tracker.init(frame, initBB)
        fps = FPS().start()
    elif int(key) == 52:
        a = a + 2
        t_w = t_w - 4
    elif int(key) == 53:
        b = b + 2
        t_h = t_h - 4
    elif int(key) == 54:
        a = a - 2
        t_w = t_w + 4
    elif int(key) == 56:
        b = b - 2
        t_h = t_h + 4
    elif key == ord("q"):
        break<N><N>
    # if the 's' key is selected, we are going to "select" a bounding
    # box to track
    # if key == ord("s"):<N><N>
    # 	initBB = cv2.selectROI("Frame", frame, fromCenter=False, showCrosshair=True)
    # 	print(initBB)
    # 	tracker.init(frame, initBB)
    # 	fps = FPS().start()<N><N>
# # if the `q` key was pressed, break from the loop
    # elif key == ord("q"):
    # 	break<N><N>

# if we are using a webcam, release the pointer
if not args.get("video", False):
    vs.stop()<N><N>
# otherwise, release the file pointer
else:
    vs.release()<N><N>
# close all windows
cv2.destroyAllWindows()
<N><N>
from flask import Flask, render_template, Response, request
import cv2
import datetime, time
import os, sys
import numpy as np
from threading import Thread<N><N>

global capture,rec_frame, grey, switch, neg, face, rec, out 
capture=0
grey=0
neg=0
face=0
switch=1
rec=0<N><N>
#make shots directory to save pics
try:
    os.mkdir('./shots')
except OSError as error:
    pass<N><N>
#Load pretrained face detection model    
net = cv2.dnn.readNetFromCaffe('./saved_model/deploy.prototxt.txt', './saved_model/res10_300x300_ssd_iter_140000.caffemodel')<N><N>
#instatiate flask app  
app = Flask(__name__, template_folder='./templates')<N><N>

camera = cv2.VideoCapture(0)<N><N>
def record(out):
    global rec_frame
    while(rec):
        time.sleep(0.05)
        out.write(rec_frame)<N><N>

def detect_face(frame):
    global net
    (h, w) = frame.shape[:2]
    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,
        (300, 300), (104.0, 177.0, 123.0))   
    net.setInput(blob)
    detections = net.forward()
    confidence = detections[0, 0, 0, 2]<N><N>
    if confidence < 0.5:            
            return frame           <N><N>
    box = detections[0, 0, 0, 3:7] * np.array([w, h, w, h])
    (startX, startY, endX, endY) = box.astype("int")
    try:
        frame=frame[startY:endY, startX:endX]
        (h, w) = frame.shape[:2]
        r = 480 / float(h)
        dim = ( int(w * r), 480)
        frame=cv2.resize(frame,dim)
    except Exception as e:
        pass
    return frame
 <N><N>
import cv2<N><N>img = cv2.imread("./flower.jpg")
print(img.shape)
img_dtype = img.dtype
print(img_dtype)<N><N>(b, g, r) = img[6, 40]
print(b, g, r)<N><N>b = img[6, 40, 0]
g = img[6, 40, 1]
r = img[6, 40, 2]
print(b, g, r)<N><N>cv2.imshow("raw", img)
img[6, 40] = (0, 0, 255)
cv2.imshow("change", img)
cv2.waitKey(0)

import cv2<N><N>gray_img = cv2.imread("./gray.png", cv2.IMREAD_GRAYSCALE)
print(gray_img.shape)
value = gray_img[6, 40]
print(value)

import matplotlib.pyplot as plt
import cv2<N><N>img_logo = cv2.imread("./logo.png")<N><N>b, g, r = cv2.split(img_logo)<N><N>img_new = cv2.merge([r, g, b])<N><N># plt.subplot(121)
# plt.imshow(img_logo)
# plt.subplot(122)
# plt.imshow(img_new)
#
# plt.show()<N><N>cv2.imshow("bgr_image", img_logo)
cv2.imshow("rgb_image", img_new)
cv2.waitKey(0)
cv2.destroyAllWindows()

import argparse<N><N># 获取所有参数
parse = argparse.ArgumentParser()
parse.add_argument("number1", help="第一个参数", type=int)
parse.add_argument("number2", help="第二个参数", type=int)<N><N># 解析所有参数
args = parse.parse_args()<N><N>print("第一个参数: ", args.number1)
print("第二个参数: ", args.number1)
print("所有参数: ", args)

import cv2
import argparse<N><N>
# img = cv2.imread("./logo.png")
# cv2.imshow("LOGO", img)
# cv2.waitKey(0)
# cv2.destroyAllWindows()<N><N>
parse = argparse.ArgumentParser()
parse.add_argument("path_image", help="path to input the image")<N><N>
args = parse.parse_args()<N><N>
img = cv2.imread(args.path_image)
cv2.imshow("LOGO", img)<N><N>
args_dict = vars(parse.parse_args())  # {"path_image" : "./logo.png"}
img2 = cv2.imread(args_dict["path_image"])
cv2.imshow("logo_two", img2)
cv2.waitKey(0)<N><N>
cv2.destroyAllWindows()
<N><N>
import cv2
import argparse<N><N>parse = argparse.ArgumentParser()<N><N>parse.add_argument("img_input", help="read one image")
parse.add_argument("img_output", help="save the processed image")<N><N>args = vars(parse.parse_args())<N><N>img = cv2.imread(args["img_input"])<N><N>img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>cv2.imwrite(args["img_output"], img_gray)<N><N>cv2.imshow("original image", img)
cv2.imshow("gray image", img_gray)<N><N>cv2.waitKey(0)
cv2.destroyAllWindows()

import cv2
import argparse<N><N>
parse = argparse.ArgumentParser()<N><N>
parse.add_argument("index_camera", help="the camera id", type=int)<N><N>
args = vars(parse.parse_args())
print("the camera index: ", args["index_camera"])<N><N>
capture = cv2.VideoCapture(args["index_camera"])<N><N>
# CAP: capture, PROP: property
frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)
frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)
# fps: frames per second
fps = capture.get(cv2.CAP_PROP_FPS)
print("frame_width is: {}".format(frame_width))
print("frame_height is: {}".format(frame_height))
print("fps is: {}".format(fps))<N><N>
if not capture.isOpened():
    print("Camera Error!")<N><N>
while capture.isOpened():
    ret, frame = capture.read()
    if ret:
        cv2.imshow("frame", frame)
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        cv2.imshow("gray frame", gray_frame)
        if cv2.waitKey(20) & 0xFF == ord('q'):
            break
    else:
        break<N><N>
capture.release()
cv2.destroyAllWindows()
<N><N>
import cv2
import argparse<N><N>parse = argparse.ArgumentParser()
parse.add_argument("video_path", help="the path to the video file")<N><N>args = vars(parse.parse_args())<N><N>capture = cv2.VideoCapture(args["video_path"])<N><N># ret是否读取到了帧（图片）
ret, frame = capture.read()<N><N>while ret:
    cv2.imshow("video", frame)
    ret, frame = capture.read()
    if cv2.waitKey(20) & ord('q') == 0xFF:
        break<N><N>capture.release()
cv2.destroyAllWindows()

import cv2
import argparse<N><N>
parse = argparse.ArgumentParser()
parse.add_argument("index_camera", help="camera index", type=int)
parse.add_argument("video_output", help="the path to the output video")<N><N>
args = vars(parse.parse_args())<N><N>
capture = cv2.VideoCapture(args["index_camera"])<N><N>
if not capture.isOpened():
    print("camera error")<N><N>
frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)
frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)
fps = capture.get(cv2.CAP_PROP_FPS)<N><N>
# 对视频进行编码
fourcc = cv2.VideoWriter_fourcc(*"XVID")
output_gray = cv2.VideoWriter(args["video_output"], fourcc, int(
    fps), (int(frame_width), int(frame_height)), False)<N><N>
while capture.isOpened():
    ret, frame = capture.read()
    if ret:
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        output_gray.write(gray_frame)
        cv2.imshow("gray_frame", gray_frame)
        if cv2.waitKey(1) & ord('q') == 0xFF:
            break
    else:
        break<N><N>
capture.release()
output_gray.release()
cv2.destroyAllWindows()
<N><N>
import cv2
import matplotlib.pyplot as plt
import numpy as np<N><N>
rows, cols = 5, 5<N><N>
img = cv2.imread("./logo.png")
plt.subplot(rows, cols, 1)
plt.imshow(img)<N><N>
height, width, channel = img.shape
print(height, width, channel)<N><N>
# 放大
resized_img = cv2.resize(img, (width * 2, height * 2),
                         interpolation=cv2.INTER_LINEAR)
plt.subplot(rows, cols, 2)
plt.imshow(resized_img)<N><N>
# 缩小
small_img = cv2.resize(img, None, fx=0.5, fy=0.5,
                       interpolation=cv2.INTER_LINEAR)
plt.subplot(rows, cols, 3)
plt.imshow(small_img)<N><N>
height, width = img.shape[:2]
# 平移 平移矩阵 上下平移
M1 = np.float32([[1, 0, 100], [0, 1, 50]])
move_img = cv2.warpAffine(img, M1, (width, height))
plt.subplot(rows, cols, 4)
plt.imshow(move_img)<N><N>
# 左右平移
M2 = np.float32([[1, 0, -100], [0, 1, -50]])
move_img2 = cv2.warpAffine(img, M2, (width, height))
plt.subplot(rows, cols, 5)
plt.imshow(move_img2)<N><N>
# 旋转
height, width = img.shape[:2]
center = (width // 2, height // 2)
M3 = cv2.getRotationMatrix2D(center, 180, 1)
rotation_img = cv2.warpAffine(img, M3, (width, height))
plt.subplot(rows, cols, 6)
plt.imshow(rotation_img)<N><N>
# 仿射变换
p1 = np.float32([[120, 35], [215, 45], [135, 120]])
p2 = np.float32([[135, 45], [300, 110], [130, 230]])
M4 = cv2.getAffineTransform(p1, p2)
trans_img = cv2.warpAffine(img, M4, (width, height))
plt.subplot(rows, cols, 7)
plt.imshow(trans_img)<N><N>
# 裁剪
crop_img = img[20:500, 200:400]
plt.subplot(rows, cols, 8)
plt.imshow(crop_img)<N><N>
# 位运算
rectangle = np.zeros((300, 300), dtype='uint8')
rect_img = cv2.rectangle(rectangle, (25, 25), (275, 275), 255, -1)
plt.subplot(rows, cols, 9)
plt.imshow(rect_img)<N><N>
circle = np.zeros((300, 300), dtype="uint8")
circle_img = cv2.circle(circle, (150, 150), 150, 255, -1)
plt.subplot(rows, cols, 10)
plt.imshow(circle_img)<N><N>
# 与运算
and_img = cv2.bitwise_and(rect_img, circle_img)
plt.subplot(rows, cols, 11)
plt.imshow(and_img)<N><N>
or_img = cv2.bitwise_or(rect_img, circle_img)
plt.subplot(rows, cols, 12)
plt.imshow(or_img)<N><N>
not_img = cv2.bitwise_not(rect_img, circle_img)
plt.subplot(rows, cols, 13)
plt.imshow(not_img)<N><N>
xor_img = cv2.bitwise_xor(rect_img, circle_img)
plt.subplot(rows, cols, 14)
plt.imshow(xor_img)<N><N>
# 图像分离和融合
(B, G, R) = cv2.split(img)
plt.subplot(rows, cols, 15)
plt.imshow(B)<N><N>
plt.subplot(rows, cols, 16)
plt.imshow(G)<N><N>
plt.subplot(rows, cols, 17)
plt.imshow(R)<N><N>
zeros = np.zeros(img.shape[:2], dtype="uint8")
plt.subplot(rows, cols, 18)
plt.imshow(cv2.merge([zeros, zeros, R]))<N><N>
plt.subplot(rows, cols, 19)
plt.imshow(cv2.merge([G, zeros, zeros]))<N><N>
plt.subplot(rows, cols, 20)
plt.imshow(cv2.merge([zeros, B, zeros]))<N><N>
# 颜色空间
# 灰度
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
plt.subplot(rows, cols, 21)
plt.imshow(gray)
# HSV(色度, 饱和度, 纯度)
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
plt.subplot(rows, cols, 22)
plt.imshow(hsv)
# Lab
lab = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)
plt.subplot(rows, cols, 23)
plt.imshow(lab)<N><N>
plt.show()
<N><N>
import cv2
import argparse
from matplotlib import pyplot as plt
import numpy as np
from matplotlib import font_manager<N><N>
my_font = font_manager.FontProperties(
    fname="/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc")<N><N>

def show_image(image, title, pos):
    image_RGB = image[:, :, ::-1]
    plt.title(title, fontproperties=my_font)
    plt.subplot(2, 3, pos)
    plt.imshow(image_RGB)<N><N>

def show_histogram(hist, title, pos, color):
    plt.title(title, fontproperties=my_font)
    plt.subplot(2, 3, pos)
    plt.xlabel("Bins", fontproperties=my_font)
    plt.ylabel("Pixels", fontproperties=my_font)
    plt.xlim([0, 256])
    plt.plot(hist, color=color)<N><N>

def main():
    plt.figure(figsize=(15, 6))
    plt.suptitle("灰度直方图", fontsize=14, fontweight="bold",
                 fontproperties=my_font)
    img = cv2.imread("./children.jpg")
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>
    hist_img = cv2.calcHist([img_gray], [0], None, [256], [0, 256])
    # hist_img2 = cv2.calcHist(img_gray, [0], None, [256], [0, 256])<N><N>
    img_BGR = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)
    show_image(img_BGR, "BGR image", 1)<N><N>
    show_histogram(hist_img, "gray image histogram", 4, "m")
    # show_histogram(hist_img2, "gray image histogram", 5, "m")
    plt.show()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
import argparse
from matplotlib import pyplot as plt
import numpy as np
from matplotlib import font_manager<N><N>
my_font = font_manager.FontProperties(
    fname="/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc")<N><N>

def show_image(image, title, pos):
    image_RGB = image[:, :, ::-1]
    plt.subplot(2, 3, pos)
    plt.imshow(image_RGB)
    plt.title(title, fontproperties=my_font)<N><N>

def show_histogram(hist, title, pos, color):
    plt.subplot(2, 3, pos)
    plt.xlabel("Bins", fontproperties=my_font)
    plt.ylabel("Pixels", fontproperties=my_font)
    plt.xlim([0, 256])
    plt.plot(hist, color=color)
    plt.title(title, fontproperties=my_font)<N><N>

def main():
    plt.figure(figsize=(15, 6))
    plt.suptitle("灰度直方图", fontsize=14, fontweight="bold",
                 fontproperties=my_font)
    img = cv2.imread("./children.jpg")
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>
    hist_img = cv2.calcHist([img_gray], [0], None, [256], [0, 256])
    # hist_img2 = cv2.calcHist(img_gray, [0], None, [256], [0, 256])<N><N>
    img_BGR = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)
    show_image(img_BGR, "BGR image", 1)
    show_histogram(hist_img, "gray image histogram", 4, "m")
    # show_histogram(hist_img2, "gray image histogram", 5, "m")<N><N>
    M = np.ones(img_gray.shape, np.uint8) * 50
    added_img = cv2.add(img_gray, M)
    add_img_hist = cv2.calcHist([added_img], [0], None, [256], [0, 256])
    added_img_BGR = cv2.cvtColor(added_img, cv2.COLOR_GRAY2BGR)
    show_image(added_img_BGR, "added image", 2)
    show_histogram(add_img_hist, "added image hist", 5, 'm')<N><N>
    subtract_img = cv2.subtract(img_gray, M)
    subtract_img_hist = cv2.calcHist(
        [subtract_img], [0], None, [256], [0, 256])
    subtract_img_BGR = cv2.cvtColor(subtract_img, cv2.COLOR_GRAY2BGR)
    show_image(subtract_img_BGR, "subtract image", 3)
    show_histogram(subtract_img_hist, "subtract image hist", 6, 'm')
    plt.show()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
from matplotlib import pyplot as plt
import numpy as np<N><N>

def show_image(image, title, pos):
    image_RGB = image[:, :, ::-1]
    plt.subplot(2, 2, pos)
    plt.title(title)
    plt.imshow(image_RGB)<N><N>

def show_histogram(hist, title, pos, color):
    plt.subplot(2, 2, pos)
    plt.title(title)
    plt.xlim([0, 256])
    plt.plot(hist, color=color)<N><N>

def main():
    plt.figure(figsize=(12, 7))
    plt.suptitle("Gray Image and Histogram with mask", fontsize=14,
                 fontweight="bold")
    img = cv2.imread("./children.jpg")
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_gray_hist = cv2.calcHist([img_gray], [0], None, [256], [0, 256])
    show_image(img, "image gray", 1)
    show_histogram(img_gray_hist, "image gray histogram", 2, "m")<N><N>
    mask = np.zeros(img_gray.shape, np.uint8)
    mask[130:500, 600:1400] = 255
    img_mask_hist = cv2.calcHist([img_gray], [0], mask, [256], [0, 256])<N><N>
    mask_img = cv2.bitwise_and(img_gray, img_gray, mask=mask)
    mask_img_BGR = cv2.cvtColor(mask_img, cv2.COLOR_GRAY2BGR)
    show_image(mask_img_BGR, "gray image with mask", 3)
    show_histogram(img_mask_hist, "histogram with masked gray image", 4, 'm')
    plt.show()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
from matplotlib import pyplot as plt
import numpy as np<N><N>

def show_image(image, title, pos):
    plt.subplot(3, 2, pos)
    plt.title(title)
    image_RGB = image[:, :, ::-1]
    plt.imshow(image_RGB)
    plt.axis("off")<N><N>

def show_histogram(hist, title, pos, color):
    plt.subplot(3, 2, pos)
    plt.title(title)
    plt.xlim([0, 256])
    for h, c in zip(hist, color):
        plt.plot(h, color=c)<N><N>

def calc_color_hist(image):
    hist = []
    hist.append(cv2.calcHist([image], [0], None, [256], [0, 256]))
    hist.append(cv2.calcHist([image], [1], None, [256], [0, 256]))
    hist.append(cv2.calcHist([image], [2], None, [256], [0, 256]))
    return hist<N><N>

def main():
    plt.figure(figsize=(12, 8))
    plt.suptitle("Color Histogram", fontsize=8, fontweight="bold")
    img = cv2.imread("./children.jpg")<N><N>
    img_hist = calc_color_hist(img)
    show_image(img, "RGB Image", 1)
    show_histogram(img_hist, "RGB Image Hist", 2, ('b', 'g', 'r'))<N><N>
    M = np.ones(img.shape, dtype="uint8") * 50<N><N>
    added_image = cv2.add(img, M)
    added_image_hist = calc_color_hist(added_image)
    show_image(added_image, "Added Image", 3)
    show_histogram(added_image_hist, "Added Image Hist", 4, ('b', 'g', 'r'))<N><N>
    subtract_image = cv2.subtract(img, M)
    subtract_image_hist = calc_color_hist(subtract_image)
    show_image(subtract_image, "Subtract Image Hist", 5)
    show_histogram(subtract_image_hist,
                   "Subtract Image Hist", 6, ('b', 'g', 'r'))
    plt.show()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
from matplotlib import pyplot as plt
import numpy as np<N><N>
colors = {'blue': (255, 0, 0),
          'green': (0, 255, 0),
          'red': (0, 0, 255),
          'yellow': (0, 255, 255),
          'magenta': (255, 0, 255),
          'cyan': (255, 255, 0),
          'white': (255, 255, 255),
          'black': (0, 0, 0),
          'gray': (125, 125, 125),
          'rand': np.random.randint(0, high=256, size=(3,)).tolist(),
          'dark_gray': (50, 50, 50),
          'light_gray': (220, 220, 220)
          }<N><N>
rows, cols = 3, 3<N><N>

def show_image(image, title, pos):
    img_RGB = image[:, :, ::-1]
    plt.subplot(rows, cols, pos)
    plt.title(title)
    plt.imshow(img_RGB)<N><N>

canvas = np.zeros((400, 400, 3), np.uint8)
canvas[:] = colors['white']
show_image(canvas, "Background", 1)<N><N>
cv2.line(canvas, (0, 0), (400, 400), colors['green'], 5)
cv2.line(canvas, (400, 0), (0, 400), colors['black'], 5)
show_image(canvas, "cv2.line()", 2)<N><N>
cv2.rectangle(canvas, (10, 20), (70, 120), colors['green'], 3)
cv2.rectangle(canvas, (150, 50), (200, 300), colors['blue'], -1)
show_image(canvas, "cv2.rectangle()", 3)<N><N>
cv2.circle(canvas, (200, 200), 150, colors['black'], 3)
cv2.circle(canvas, (200, 200), 50, colors['green'], -1)
show_image(canvas, "cv2.circle()", 4)<N><N>
pts = np.array([[250, 5], [220, 80], [280, 80]], np.int32)
pts = pts.reshape((-1, 1, 2))
cv2.polylines(canvas, [pts], True, colors['red'], 3)<N><N>
pts2 = np.array([[150, 200], [90, 130], [280, 180]], np.int32)
pts2 = pts2.reshape((-1, 1, 2))
cv2.polylines(canvas, [pts2], False, colors['black'], 5)
show_image(canvas, "cv2.polylines()", 5)
plt.show()
<N><N>
import cv2
from matplotlib import pyplot as plt
import numpy as np<N><N>

colors = {'blue': (255, 0, 0),
          'green': (0, 255, 0),
          'red': (0, 0, 255),
          'yellow': (0, 255, 255),
          'magenta': (255, 0, 255),
          'cyan': (255, 255, 0),
          'white': (255, 255, 255),
          'black': (0, 0, 0),
          'gray': (125, 125, 125),
          'rand': np.random.randint(0, high=256, size=(3,)).tolist(),
          'dark_gray': (50, 50, 50),
          'light_gray': (220, 220, 220)
          }<N><N>

def show_image(image, title):
    image_RGB = image[:, :, ::-1]
    plt.title(title)
    plt.imshow(image_RGB)
    plt.show()<N><N>

canvas = np.zeros((400, 400, 3), np.uint8)
canvas.fill(255)<N><N>
cv2.putText(canvas, "Hello Wolrd", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0,
            colors['red'], cv2.LINE_4)
cv2.putText(canvas, "Welcome to world", (50, 150), cv2.FONT_HERSHEY_SIMPLEX,
            1.4, colors['blue'], cv2.LINE_8)
show_image(canvas, "Canvas")
<N><N>
import cv2
from matplotlib import pyplot as plt
import numpy as np<N><N>

def show_image(image, title, pos):
    """
    显示图片
    """
    image_RGB = image[:, :, ::-1]
    plt.subplot(2, 2, pos)
    plt.title(title)
    plt.imshow(image_RGB)
    plt.axis("off")<N><N>

def plot_rectangle(image, faces):
    """
    绘制图片中检测到的人脸
    """
    # 拿到检测到的人脸数据, 返回4个值:  坐标(x, y), 宽高width, height
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 3)
    return image<N><N>
import cv2<N><N>

def plot_rectangle(image, faces):
    """
    绘制图片中检测到的人脸
    """
    # 拿到检测到的人脸数据, 返回4个值:  坐标(x, y), 宽高width, height
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 3)
    return image<N><N>

def main():
    # 读取摄像头
    capture = cv2.VideoCapture(0)
    # 通过OpenCV自带的方法cv2.CascadeClassifier()加载级联分类器
    face_alt2 = cv2.CascadeClassifier("./haarcascade_frontalface_alt2.xml")
    if not capture.isOpened():
        print("Camera error!")<N><N>
    while True:
        ret, frame = capture.read()
        if ret:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # 通过分类器对人脸进行检测
            face_alt2_detect = face_alt2.detectMultiScale(gray)
            # 绘制图片中检测到的人脸
            face_alt2_result = plot_rectangle(frame.copy(), face_alt2_detect)
            cv2.imshow("face detection", face_alt2_result)
            if cv2.waitKey(10) & 0xFF == ord('q'):
                break
    capture.release()
    cv2.destroyAllWindows()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
from matplotlib import pyplot as plt
import numpy as np
import dlib<N><N>

def show_image(image, title):
    """
    显示图片
    """
    image_RGB = image[:, :, ::-1]
    plt.title(title)
    plt.imshow(image_RGB)
    plt.axis("off")<N><N>

def plot_rectangle(image, faces):
    """
    绘制人脸矩阵
    """
    for face in faces:
        cv2.rectangle(image, (face.left(), face.top()),
                      (face.right(), face.bottom()), (255, 0, 0), 4)
    return image<N><N>

def main():
    # 读取图片
    image = cv2.imread("./family.jpg")
    # image = cv2.imread("./girls.jpg")
    # 灰度装换
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<N><N>
    detector = dlib.get_frontal_face_detector()
    # 1代表將图片放大一倍
    dets_result = detector(gray, 1)
    img_result = plot_rectangle(image.copy(), dets_result)<N><N>
    plt.figure(figsize=(9, 6))
    plt.suptitle("face detection with dlib", fontsize=14, fontweight="bold")
    show_image(img_result, "face detection")
    plt.show()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
import dlib<N><N>

def plot_rectangle(image, faces):
    """
    绘制人脸矩阵
    """
    for face in faces:
        cv2.rectangle(image, (face.left(), face.top()),
                      (face.right(), face.bottom()), (255, 0, 0), 4)
    return image<N><N>

def main():
    # 读取摄像头
    capture = cv2.VideoCapture(0)
    if not capture.isOpened():
        print("Camera error!")<N><N>
    while True:
        ret, frame = capture.read()
        if ret:
            # 灰度装换
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<N><N>
            detector = dlib.get_frontal_face_detector()
            # 1代表將图片放大一倍
            dets_result = detector(gray, 1)
            img_result = plot_rectangle(frame.copy(), dets_result)<N><N>
            cv2.imshow("face detction with dlib", img_result)
            if cv2.waitKey(10) & 0xFF == ord('q'):
                break
    capture.release()
    cv2.destroyAllWindows()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
import dlib
from matplotlib import pyplot as plt
import numpy as np<N><N>

# 读取一张图片
# image = cv2.imread("./Tom.jpeg")
image = cv2.imread("./Tom2.jpeg")<N><N>
# 调用人脸检测器
detector = dlib.get_frontal_face_detector()<N><N>
# 加载预测关键点模型 68个
predictor = dlib.shape_predictor("./shape_predictor_68_face_landmarks.dat")<N><N>
# 灰度转化
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<N><N>
# 人脸检测
faces = detector(gray, 1)<N><N>
# 循环 遍历每一张人脸 给人脸绘制矩阵框和关键点<N><N>
for face in faces:
    # 绘制矩形框
    cv2.rectangle(image, (face.left(), face.top()),
                  (face.right(), face.bottom()), (0, 255, 0), 5)
    # 预测关键点
    shape = predictor(image, face)
    # 获取关键点坐标
    for pt in shape.parts():
        # 获取横坐标
        pt_position = (pt.x, pt.y)
        # 绘制关键点坐标
        cv2.circle(image, pt_position, 2, (0, 0, 255), -1)<N><N>
# 显示整个效果图
plt.imshow(image)
plt.show()
<N><N>
import cv2
import dlib<N><N>
capture = cv2.VideoCapture(2)
if not capture.isOpened():
    print("Camera error!")<N><N>
# 调用人脸检测器
detector = dlib.get_frontal_face_detector()<N><N>
# 加载预测关键点模型 68个
predictor = dlib.shape_predictor("./shape_predictor_68_face_landmarks.dat")<N><N>

while True:
    ret, frame = capture.read()
    if ret:
        # 灰度转化
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<N><N>
        # 人脸检测
        faces = detector(gray, 1)<N><N>
        # 循环 遍历每一张人脸 给人脸绘制矩阵框和关键点
        for face in faces:
            # 绘制矩形框
            cv2.rectangle(frame, (face.left(), face.top()),
                          (face.right(), face.bottom()), (0, 255, 0), 3)
            # 预测关键点
            shape = predictor(gray, face)
            # 获取关键点坐标
            for pt in shape.parts():
                # 获取圆点
                pt_position = (pt.x, pt.y)
                # 绘制关键点坐标
                cv2.circle(frame, pt_position, 2, (0, 0, 255), -1)<N><N>
        # 显示整个效果图
        cv2.imshow("detection with dlib", frame)
        if cv2.waitKey(20) & 0xFF == ord('q'):
            break<N><N>
capture.release()
cv2.destroyAllWindows()
<N><N>
import cv2
import face_recognition
from matplotlib import pyplot as plt<N><N>

def show_image(image, title):
    plt.title(title)
    image_RGB = image[:, :, ::-1]
    plt.imshow(image_RGB)
    plt.axis("off")<N><N>

def show_landmarks(image, landmarks):
    for landmarks_dict in landmarks:
        for landmarks_key in landmarks_dict.keys():
            for point in landmarks_dict[landmarks_key]:
                cv2.circle(image, point, 2, (0, 0, 255), -1)
    return image<N><N>

def main():
    image = cv2.imread("./Tom.jpeg")
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    face_marks = face_recognition.face_landmarks(gray, None, "large")
    img_result = show_landmarks(image.copy(), face_marks)
    plt.figure(figsize=(9, 6))
    plt.suptitle("Face Landmarks with face_recognition", fontsize=14,
                 fontweight="bold")
    show_image(img_result, "landmarks")
    plt.show()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
import dlib<N><N>

def main():
    capture = cv2.VideoCapture(0)
    if not capture.isOpened():
        print("Camera error!")
        return
    detector = dlib.get_frontal_face_detector()
    tractor = dlib.correlation_tracker()<N><N>
    tracking_state = False<N><N>
    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_fps = int(capture.get(cv2.CAP_PROP_FPS))<N><N>
    fourcc = cv2.VideoWriter_fourcc(*"XVID")
    output = cv2.VideoWriter("record.avi", fourcc,
                             frame_fps, (frame_width, frame_height), True)<N><N>
    while True:
        ret, frame = capture.read()
        if ret:
            if tracking_state is False:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                dets = detector(gray, 1)
                if len(dets) > 0:
                    tractor.start_track(frame, dets[0])
                    tracking_state = True<N><N>
            if tracking_state:
                tractor.update(frame)
                position = tractor.get_position()
                cv2.rectangle(frame, (int(position.left()), int(position.top())), (int(
                    position.right()), int(position.bottom())), (0, 255, 0), 3)
            cv2.imshow("face tracking", frame)
            output.write(frame)
            if cv2.waitKey(20) & 0xFF == ord('q'):
                break
    capture.release()
    cv2.destroyAllWindows()<N><N>

if __name__ == "__main__":
    main()
<N><N>
import cv2
import dlib<N><N>

def show_info(frame, track_state):
    pos1 = (20, 40)
    pos2 = (20, 80)
    cv2.putText(frame, "'1' : reset", pos1, cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                (0, 0, 0))
    if track_state:
        cv2.putText(frame, "tracking now...", pos2, cv2.FONT_HERSHEY_SIMPLEX,
                    0.5, (255, 0, 0))
    else:
        cv2.putText(frame, "no tracking...", pos2, cv2.FONT_HERSHEY_SIMPLEX,
                    0.5, (0, 255, 0))<N><N>

def main():
    capture = cv2.VideoCapture(0)
    if not capture.isOpened():
        print("Camera error!")
        return
    detector = dlib.get_frontal_face_detector()
    tractor = dlib.correlation_tracker()<N><N>
    tracking_state = False<N><N>
    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_fps = int(capture.get(cv2.CAP_PROP_FPS))<N><N>
    fourcc = cv2.VideoWriter_fourcc(*"XVID")
    output = cv2.VideoWriter("record.avi", fourcc,
                             frame_fps, (frame_width, frame_height), True)<N><N>
    while True:
        ret, frame = capture.read()
        show_info(frame, tracking_state)
        if ret:
            if not tracking_state:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                dets = detector(gray, 1)
                if len(dets) > 0:
                    tractor.start_track(frame, dets[0])
                    tracking_state = True<N><N>
import cv2
import dlib<N><N>

def show_info(frame, tracking_state):
    pos1 = (10, 20)
    pos2 = (10, 40)
    pos3 = (10, 60)<N><N>
# 1 导入库
import cv2
import dlib
import numpy as np<N><N>
# 定义：关键点编码为128D<N><N>

def encoder_face(image, detector, predictor, encoder, upsample=1, jet=1):
    # 检测人脸
    faces = detector(image, upsample)
    # 对每张人脸进行关键点检测
    faces_keypoints = [predictor(image, face) for face in faces]  # 每张人脸的关键点
    return [np.array(encoder.compute_face_descriptor(image, face_keypoint, jet)) for face_keypoint in faces_keypoints]<N><N>

# 定义：人脸比较，通过欧氏距离
def compare_faces(face_encoding, test_encoding):
    return list(np.linalg.norm(np.array(face_encoding) - np.array(test_encoding), axis=1))<N><N>
# 定义：人脸比较，输出对应的名称<N><N>

def comapre_faces_order(face_encoding, test_encoding, names):
    distance = list(np.linalg.norm(np.array(face_encoding) -
                                   np.array(test_encoding), axis=1))
    return zip(*sorted(zip(distance, names)))<N><N>

def main():
    # 2 读取4张图片
    img1 = cv2.imread("./guo.jpg")
    img2 = cv2.imread("./liu1.jpg")
    img3 = cv2.imread("./liu2.jpg")
    img4 = cv2.imread("./liu3.jpg")
    test = cv2.imread("./liu4.jpg")<N><N>
    img_names = ["guo,jpg", "liu1.jpg", "liu2.jpg", "liu3.jpg"]<N><N>
    # 3  加载人脸检测器
    detector = dlib.get_frontal_face_detector()<N><N>
    # 4 加载关键点的检测器
    predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")<N><N>
    # 5 加载人脸特征编码模型
    encoder = dlib.face_recognition_model_v1(
        "dlib_face_recognition_resnet_model_v1.dat")<N><N>
    # 6 调用方法：128D特征向量输出
    img1_128D = encoder_face(img1, detector, predictor, encoder)[0]
    img2_128D = encoder_face(img2, detector, predictor, encoder)[0]
    img3_128D = encoder_face(img3, detector, predictor, encoder)[0]
    img4_128D = encoder_face(img4, detector, predictor, encoder)[0]
    test_128D = encoder_face(test, detector, predictor, encoder)[0]<N><N>
    four_images_128D = [img1_128D, img2_128D, img3_128D, img4_128D]<N><N>
    # 7 调用方法：比较人脸，计算特征向量之间的距离，判断是否为同一人
    distance = compare_faces(four_images_128D, test_128D)
    print(distance)<N><N>
    distance, name = comapre_faces_order(
        four_images_128D, test_128D, img_names)<N><N>
    print("distance: {}, \n names: {} ".format(distance, name))<N><N>

if __name__ == '__main__':
    main()
<N><N>
# 1 加载库
import cv2
import numpy as np
import face_recognition<N><N>
# 2 加载图片
liu = cv2.imread("liu.jpeg")
guo = cv2.imread("guo.jpg")<N><N>
# 3 BGR 转 RGB
liu_RGB = liu[:, :, ::-1]
guo_RGB = guo[:, :, ::-1]<N><N>
# 4 检测人脸
liu_face = face_recognition.face_locations(liu_RGB)
guo_face = face_recognition.face_locations(guo_RGB)<N><N>
# 5 人脸特征编码
liu_encoding = face_recognition.face_encodings(liu_RGB, liu_face)[0]
guo_encoding = face_recognition.face_encodings(guo_RGB, guo_face)[0]<N><N>
# 6 把所有人脸放在一起，当做数据库使用
encodings = [liu_encoding, guo_encoding]
names = ["liu de hua", "guo fu cheng"]<N><N>
# 7 打开摄像头，读取视频流
cap = cv2.VideoCapture(2)
if not cap.isOpened():
    raise IOError("Camera Error !")<N><N>
import cv2
import numpy as np
import face_recognition<N><N>

class Face:
    def __init__(self, image_file, name_list, video_index):
        self.image_file = image_file
        self.encodings = []
        self.names = name_list
        self.video_index = int(video_index)<N><N>
    def face_save(self):
        image_list = [cv2.imread(image) for image in self.image_file]
        for image in image_list:
            face = face_recognition.face_locations(image)
            face_encoding = face_recognition.face_encodings(image, face)[0]
            self.encodings.append(face_encoding)<N><N>
    def face_dectection(self):
        capture = cv2.VideoCapture(self.video_index)
        if not capture.isOpened():
            print("Camera Error !")
            raise IOError("Camera Error !")<N><N>
from PIL import Image
import cv2<N><N>sourceFileName = "./Tom.jpeg"
im = Image.open(sourceFileName)
width = im.size[0]
height = im.size[1]
im = im.convert('RGB')
array = []
for x in range(width):
    for y in range(height):
        r, g, b = im.getpixel((x, y))
        rgb = (r, g, b)
        array.append(rgb)
print(array)
image = cv2.imread(sourceFileName)
print(cv2.split(image))

# -*- coding:utf-8 -*-<N><N>
import numpy as np
import cv2<N><N>

img = np.zeros((512, 512, 3), np.uint8)<N><N>
# 上－外圆－红色填充
img = cv2.circle(img, (256, 176), 40, (0, 0, 255), -1)
# 上－内圆－黑色填充
img = cv2.circle(img, (256, 176), 16, (0, 0, 0), -1)<N><N>
# 左下－外圆－绿色填充
img = cv2.circle(img, (210, 256), 40, (0, 255, 0), -1)
# 左下－内圆－黑色填充
img = cv2.circle(img, (210, 256), 16, (0, 0, 0), -1)<N><N>
# 右下－外圆－蓝色填充
img = cv2.circle(img, (302, 256), 40, (255, 0, 0), -1)
# 右下－内圆－黑色填充
img = cv2.circle(img, (302, 256), 16, (0, 0, 0), -1)<N><N>

# 用一个四角的多边形来填充覆盖
pts = np.array([[256, 176], [210, 256], [250, 256], [276, 210]], np.int32)
pts = pts.reshape((-1, 1, 2))
img = cv2.fillPoly(img, [pts], (0, 0, 0))<N><N>
# 第2个覆盖，使用三角形
pts2 = np.array([[276, 210], [302, 256], [322, 210]], np.int32)
pts2 = pts2.reshape((-1, 1, 2))
img = cv2.fillPoly(img, [pts2], (0, 0, 0))<N><N>
cv2.imshow('draw exercise', img)
cv2.waitKey()
cv2.destroyAllWindows()
<N><N>
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt<N><N>img = cv.imread("./flower.jpg", 1)
img = cv.cvtColor(img, cv.COLOR_BGR2RGB)
plt.imshow(img, cmap='gray', interpolation='bicubic')
plt.xticks([])
plt.yticks([])
plt.show()

import argparse
import cv2 as cv
import imutils
import numpy as np
from imutils.object_detection import non_max_suppression<N><N>
subject_label = 1
font = cv.FONT_HERSHEY_SIMPLEX
list_of_videos = []
hog = cv.HOGDescriptor()
hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())<N><N>
output_image_width = 500<N><N>
import cv2 as cv
import numpy as np<N><N>
camera = cv.VideoCapture(0)<N><N>
while True:
    _, frame = camera.read()<N><N>
    #cv.imshow('Camera',frame)<N><N>
    laplacian = cv.Laplacian(frame, cv.CV_64F) # using the laplacian function<N><N>
    laplacian = np.uint8(laplacian) # converting the float to int and into the pizel range of 0-256
    #cv.imshow('Laplacian', laplacian)<N><N>
    edges = cv.Canny(frame, 120, 120)
    # bascially the parameters in canny are the threshold if more detailing is required we can use
    # lower thresholds and less detailing then the higher thresholds.
    cv.imshow('Canny', edges)<N><N>
    if cv.waitKey(5) == ord('x'):
        break<N><N>

camera.release()
camera.destroyAllWindows()
<N><N>
import cv2
from Text_Detection import detect_characters, detect_string, detect_words
import re
from live_recognition import facial_recognition
#
def rescale_frame(frame, percent=75):
    width = int(frame.shape[1] * percent/ 100)
    height = int(frame.shape[0] * percent/ 100)
    dim = (width, height)
    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)<N><N>
import cv2
import numpy as np
import face_recognition
import os<N><N>
def findEncoding(images):
    encodeList = []
    for img in images:
        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        encode = face_recognition.face_encodings(img)[0]
        encodeList.append(encode)
    return encodeList<N><N>
import cv2
import pytesseract<N><N>
pytesseract.pytesseract.tesseract_cmd = "C:\\Program Files\\Tesseract-OCR\\tesseract.exe"
# img = cv2.imread("../../Learn-OpenCV-in-3-hours-master/TEXT.png")
# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# print(pytesseract.image_to_string(img))<N><N>
def detect_string(img):
    return pytesseract.image_to_string(img)<N><N>
<N><N>def detect_characters(img):
    ### Detecting Characters
    hImg, wImg, _ = img.shape
    boxes = pytesseract.image_to_boxes(img)
    arr=[]
    for b in boxes.splitlines():
        # print(b)
        b = b.split(' ')
        # print(b)
        # x, y, w, h = int(b[1]), int(b[2]), int(b[3]), int(b[4])
        # cv2.rectangle(img, (x, hImg - y), (w, hImg - h), (0, 0, 255), thickness=3)
        arr.append(b[0])<N><N>
        # cv2.putText(img, b[0], (x, hImg - y + 25), cv2.FONT_HERSHEY_COMPLEX, fontScale=1, color=(50, 50, 255), thickness=2)
    return "".join(arr)<N><N>
def detect_words(img):
    ### Detecting Words
    hImg, wImg, _ = img.shape
    config = r'--oem 3 --psm 6'
    boxes = pytesseract.image_to_data(img,config=config)
    arr=[]
    for x,b in enumerate(boxes.splitlines()):
        if x!=0:
            b = b.split()
            # print(b)
            if len(b)==12:<N><N>
                x, y, w, h = int(b[6]), int(b[7]), int(b[8]), int(b[9])
                arr.append(b[11])
                # cv2.rectangle(img, (x, y), (w+x, h+y), (0, 0, 255), thickness=3)
                # cv2.putText(img, b[11], (x, y), cv2.FONT_HERSHEY_COMPLEX, fontScale=1, color=(50, 50, 255), thickness=2)<N><N>
import cv2
import numpy as np
import time<N><N>
import random<N><N>

import sys
from PySide2.QtCore import *
# from PySide2.QtWidgets import (QWidget,QPushButton,QComboBox,QSpinBox,QHBoxLayout,QVBoxLayout,QApplication,QLineEdit,
#                                 QLabel,QGridLayout)
from PySide2.QtWidgets import *<N><N>
from PySide2.QtGui import *
import numpy as np
import cv2
from os.path import exists
from os import mkdir<N><N>

from time import sleep,time<N><N>
import xml.etree.ElementTree as ET
from xml.dom import minidom<N><N>
import sys
import numpy as np
import cv2<N><N>
args = sys.argv<N><N>
img = cv2.imread(args[1])<N><N>
imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
imgYUV = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)<N><N>
imgY = imgYUV[:,:,0]<N><N>
laplacian = cv2.Laplacian(imgY, cv2.CV_64F)
canny = cv2.Canny(imgY, 100, 200)
dx = cv2.Sobel(imgY, cv2.CV_64F, 1, 0, ksize=3)
dy = cv2.Sobel(imgY, cv2.CV_64F, 0, 1, ksize=3)
grad = np.sqrt(dx ** 2 + dy ** 2)<N><N>
cv2.imshow('imgY', imgY)
cv2.imshow('laplacian', laplacian)
cv2.imshow('dx', dx)
cv2.imshow('dy', dy)
cv2.imshow('grad', grad)
cv2.imshow('canny', canny)<N><N>
cv2.waitKey(0)<N><N>
cv2.destroyAllWindows()
<N><N>
import sys
import numpy as np
import cv2<N><N>
args = sys.argv<N><N>
img  = cv2.imread(args[1])
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
eye_cascade  = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_eye.xml")<N><N>
faces = face_cascade.detectMultiScale(gray, 1.3, 5)<N><N>
for (x, y, w, h) in faces:
	img = cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 2)
	roi_gray = gray[y:y+h, x:x+w]
	roi_color = img[y:y+h, x:x+w]
	eyes = eye_cascade.detectMultiScale(roi_gray)
	for (ex, ey, ew, eh) in eyes:
		cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)<N><N>
cv2.imshow('img', img)
cv2.waitKey(0)
cv2.destroyAllWindows()
<N><N>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import argparse
from collections import deque<N><N>
import cv2
import numpy as np<N><N>
click_points = deque(maxlen=4)<N><N>

def get_args():
    parser = argparse.ArgumentParser()<N><N>
    parser.add_argument("--device", type=int, default=0)
    parser.add_argument("--width", type=int, default=640)
    parser.add_argument("--height", type=int, default=480)
    parser.add_argument("--crop_width", type=int, default=96)
    parser.add_argument("--crop_height", type=int, default=96)
    parser.add_argument("--extension", type=str, default='jpg')
    parser.add_argument("--start_count", type=int, default=0)<N><N>
    args = parser.parse_args()<N><N>
    return args<N><N>

def mouse_callback(event, x, y, flags, param):
    global click_points
    if event == cv2.EVENT_LBUTTONDOWN:
        click_points.append([x, y])<N><N>

def main():
    global click_points<N><N>
    # コマンドライン引数
    args = get_args()
    cap_device = args.device<N><N>
    cap_width = args.width
    cap_height = args.height
    crop_width = args.crop_width
    crop_height = args.crop_height<N><N>
    extension = args.extension
    image_count = args.start_count<N><N>
    # GUI準備
    window_name = 'Image Capture & Class Annotation'
    cv2.namedWindow(window_name)
    cv2.setMouseCallback(window_name, mouse_callback)<N><N>
    # カメラ準備
    cap = cv2.VideoCapture(cap_device)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_height)<N><N>
    # 認識対象座標 格納用
    click_points = deque(maxlen=4)<N><N>
    while True:
        extract_image = None<N><N>
        # カメラキャプチャ
        ret, frame = cap.read()
        if not ret:
            print('cap.read() error')
        resize_frame = cv2.resize(frame, (int(cap_width), int(cap_height)))<N><N>
import cv2
from facenet_pytorch import MTCNN
import numpy as np<N><N>
class FaceDetector(object):<N><N>
    def __init__(self, mtcnn):
        self.mtcnn = mtcnn<N><N>
import numpy as np
import streamlit as st
from PIL import Image, ImageOps
import cv2 as cv
# from matplotlib import pyplot as plt
import sys
import io
import base64<N><N>
import streamlit as st
from PIL import Image, ImageEnhance
import cv2 
import numpy as np
import scipy
import seaborn as sns
import copy
from scipy.interpolate import UnivariateSpline
from streamlit_lottie import st_lottie
import requests<N><N>
st.set_page_config(layout="wide")<N><N>
def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()<N><N>
lottie_book = load_lottieurl('https://assets6.lottiefiles.com/datafiles/9jPPC5ogUyD6oQq/data.json')
st_lottie(lottie_book, speed=1, height=200, key="initial")<N><N>
sns.set_style('darkgrid')
row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.beta_columns(
    (.1, 2, .2, 1, .1))<N><N>
row0_1.title('Image Filtering Using OpenCV')<N><N>
with row0_2:
    st.write('')<N><N>
row0_2.subheader(
    'A Web App by [Kritanjali Jain](https://www.linkedin.com/in/kritanjali-jain-009180191/)')<N><N>
def test2():
    
    st.write('')
    row1_spacer1, row1_1, row1_spacer2 = st.beta_columns((.1, 3.2, .1))<N><N>
    with row1_1:
        st.markdown("Hey there! Welcome to Image Filtering App. This app demonstrates OpenCV image preprocessing techniques. These are often used by machine learning developers to preprocess their images for Computer Vision models. This app never stores your image data.One last tip, if you're on a mobile device, switch over to landscape for viewing ease. Give it a go!")<N><N>
    col1, col2 = st.beta_columns(2)<N><N>
import tkinter as tk
from tkinter import filedialog
from tkinter import *
from PIL import ImageTk, Image
import imutils<N><N>
import numpy as np
import tensorflow as tf
from tensorflow import keras
import cv2
import matplotlib.pyplot as plt<N><N>
import cv2 as cv
import numpy as np
import os<N><N>

def recognize(img):
    path = r'..\IMAGES'  # path do bazy znaków
    size = 80
    counter_result = []
    signs = []
    signs_names = []
    main_image = cv.imread(img)<N><N>
    try:
        for image_path in os.listdir(path):
            input_path = os.path.join(path, image_path)
            image = cv.imread(input_path)<N><N>
            signs.append(input_path)  # tworzymy listę znaków z bazy
            image_path = image_path.replace("-", " ")
            signs_names.append(image_path.rstrip(".png"))<N><N>
            counter = 0<N><N>
import PySimpleGUI as sg
from BACKEND import process<N><N>
lewo = [<N><N>
    [
        sg.Text("Sciezka do obrazu: "),<N><N>
        sg.In(size=(25, 1), enable_events=True, key="DIRECTORY"),<N><N>
        # sg.filedialog.askopenfilename(),
        sg.Button(button_text="CHOSE A FILE", key="FILE", auto_size_button=True, ),
        # sg.Button(button_text="OK", key="FIRE", auto_size_button=True)<N><N>
    ],
]<N><N>
prawo = [<N><N>
    [sg.Text(size=(50, 1), key="PATH")],<N><N>
    [sg.Image(key="IMAGE"),<N><N>
    sg.Image(key="IMAGE_1")],<N><N>
    [sg.Text(size=(50, 2), key="PROB")]<N><N>
]<N><N>layout = [<N><N>
    [
        sg.Column(lewo),
        sg.Column(prawo)
    ]<N><N>
]<N><N>window = sg.Window("Rozpoznawanie znakow", layout)<N><N>
while True:<N><N>
    event, values = window.read()<N><N>
    if event == sg.WIN_CLOSED:
        break<N><N>
    if event == "FILE":
        try:
            filename = sg.filedialog.askopenfilename()
            result = process.recognize(filename)
            window["IMAGE"].update(filename=filename)<N><N>
            if not result:
                window["PATH"].update("Błąd w formacie obrazu")
                window["PROB"].update("")
                window["IMAGE_1"].update()
                continue<N><N>
            # filename = '...' + filename.split('GitHub')[1]
            window["DIRECTORY"].update(filename)
            window["PATH"].update(result[0])<N><N>
            if result[2] >= 20:
                window["PROB"].update("zgodnosc: " + str(result[2]) + "%")
                window["IMAGE_1"].update(str(result[1]))
            else:
                window["PROB"].update("zgodnosc z: " + str(result[0]) + " w: " + str(result[2]) + "%")
                window["PATH"].update("Nie wykryto znaku")
                window["IMAGE_1"].update(str(result[1]))<N><N>
        except:
            pass<N><N>
window.close()
<N><N>
import numpy as np
import cv2<N><N>cap = cv2.VideoCapture(0)<N><N># Standard Video Dimensions Sizes
STD_DIMENSIONS =  {
    "480p": (640, 480),
    "720p": (1280, 720),
    "1080p": (1920, 1080),
    "4k": (3840, 2160),
}<N><N>width, height = STD_DIMENSIONS["720p"]<N><N>cap.set(3, width)
cap.set(4, height)<N><N>ret, frame = cap.read()<N><N>cv2.imwrite('frame.jpg',frame)<N><N># cv2.imshow('frame',frame)<N><N># When everything done, release the capture
cap.release()

import numpy as np
import cv2<N><N>cap = cv2.VideoCapture(0)<N><N>ret, frame = cap.read()<N><N>cv2.imwrite('frame.jpg',frame)<N><N># cv2.imshow('frame',frame)<N><N># When everything done, release the capture
cap.release()

import numpy as np
import cv2<N><N>cap = cv2.VideoCapture(0)<N><N>while(True):
    # Capture frame-by-frame
    ret, frame = cap.read()
    
    # Our operations on the frame come here
    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
   
    # Display the resulting frame
    cv2.imshow('frame',frame)
    # cv2.imshow('gray',gray)
    
    if cv2.waitKey(20) & 0xFF == ord('q'):
        break<N><N># When everything done, release the capture
cap.release()
cv2.destroyAllWindows()
import numpy as np
import os
import cv2<N><N>
# Setting Video Format, Frame Rate, Resolution
filename = 'video.avi'
frames_per_second = 24.0
res = '720p'<N><N>
# Set resolution for the video capture
# Function adapted from https://kirr.co/0l6qmh
def change_res(cap, width, height):
    cap.set(3, width)
    cap.set(4, height)<N><N>
# Standard Video Dimensions Sizes
STD_DIMENSIONS =  {
    "480p": (640, 480),
    "720p": (1280, 720),
    "1080p": (1920, 1080),
    "4k": (3840, 2160),
}<N><N>
# grab resolution dimensions and set video capture to it.
def get_dims(cap, res='1080p'):
    width, height = STD_DIMENSIONS["480p"]
    if res in STD_DIMENSIONS:
        width,height = STD_DIMENSIONS[res]
    ## change the current caputre device
    ## to the resulting resolution
    change_res(cap, width, height)
    return width, height<N><N>
# Video Encoding, might require additional installs
# Types of Codes: http://www.fourcc.org/codecs.php
VIDEO_TYPE = {
    'avi': cv2.VideoWriter_fourcc(*'XVID'),
    'mp4': cv2.VideoWriter_fourcc(*'H264'),
    #'mp4': cv2.VideoWriter_fourcc(*'XVID'),
}<N><N>
def get_video_type(filename):
    filename, ext = os.path.splitext(filename)
    if ext in VIDEO_TYPE:
      return  VIDEO_TYPE[ext]
    return VIDEO_TYPE['avi']<N><N>
<N><N>cap = cv2.VideoCapture(0)
out = cv2.VideoWriter(filename, get_video_type(filename), 25, get_dims(cap, res))<N><N>
while True:
    ret, frame = cap.read()
    out.write(frame)
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>

cap.release()
out.release()
cv2.destroyAllWindows()<N><N>
from manimlib import *
import numpy as np<N><N>
# To watch one of these scenes, run the following:
# python -m manim example_scenes.py SquareToCircle
# Use -s to skip to the end and just save the final frame
# Use -w to write the animation to a file
# Use -o to write it to a file and open it once done
# Use -n <number> to skip ahead to the n'th animation of a scene.<N><N>

class OpeningManimExample(Scene):
    def construct(self):
        intro_words = Text("""
            The original motivation for manim was to
            better illustrate mathematical functions
            as transformations.
        """)
        intro_words.to_edge(UP)<N><N>
        self.play(Write(intro_words))
        self.wait(2)<N><N>
        # Linear transform
        grid = NumberPlane((-10, 10), (-5, 5))
        matrix = [[1, 1], [0, 1]]
        linear_transform_words = VGroup(
            Text("This is what the matrix"),
            IntegerMatrix(matrix, include_background_rectangle=True),
            Text("looks like")
        )
        linear_transform_words.arrange(RIGHT)
        linear_transform_words.to_edge(UP)
        linear_transform_words.set_stroke(BLACK, 10, background=True)<N><N>
        self.play(
            ShowCreation(grid),
            FadeTransform(intro_words, linear_transform_words)
        )
        self.wait()
        self.play(grid.animate.apply_matrix(matrix), run_time=3)
        self.wait()<N><N>
        # Complex map
        c_grid = ComplexPlane()
        moving_c_grid = c_grid.copy()
        moving_c_grid.prepare_for_nonlinear_transform()
        c_grid.set_stroke(BLUE_E, 1)
        c_grid.add_coordinate_labels(font_size=24)
        complex_map_words = TexText("""
            Or thinking of the plane as $\\mathds{C}$,\\\\
            this is the map $z \\rightarrow z^2$
        """)
        complex_map_words.to_corner(UR)
        complex_map_words.set_stroke(BLACK, 5, background=True)<N><N>
        self.play(
            FadeOut(grid),
            Write(c_grid, run_time=3),
            FadeIn(moving_c_grid),
            FadeTransform(linear_transform_words, complex_map_words),
        )
        self.wait()
        self.play(
            moving_c_grid.animate.apply_complex_function(lambda z: z**2),
            run_time=6,
        )
        self.wait(2)<N><N>

class AnimatingMethods(Scene):
    def construct(self):
        grid = Tex(r"\pi").get_grid(10, 10, height=4)
        self.add(grid)<N><N>
        # You can animate the application of mobject methods with the
        # ".animate" syntax:
        self.play(grid.animate.shift(LEFT))<N><N>
        # Alternatively, you can use the older syntax by passing the
        # method and then the arguments to the scene's "play" function:
        self.play(grid.shift, LEFT)<N><N>
        # Both of those will interpolate between the mobject's initial
        # state and whatever happens when you apply that method.
        # For this example, calling grid.shift(LEFT) would shift the
        # grid one unit to the left, but both of the previous calls to
        # "self.play" animate that motion.<N><N>
        # The same applies for any method, including those setting colors.
        self.play(grid.animate.set_color(YELLOW))
        self.wait()
        self.play(grid.animate.set_submobject_colors_by_gradient(BLUE, GREEN))
        self.wait()
        self.play(grid.animate.set_height(TAU - MED_SMALL_BUFF))
        self.wait()<N><N>
        # The method Mobject.apply_complex_function lets you apply arbitrary
        # complex functions, treating the points defining the mobject as
        # complex numbers.
        self.play(grid.animate.apply_complex_function(np.exp), run_time=5)
        self.wait()<N><N>
        # Even more generally, you could apply Mobject.apply_function,
        # which takes in functions form R^3 to R^3
        self.play(
            grid.animate.apply_function(
                lambda p: [
                    p[0] + 0.5 * math.sin(p[1]),
                    p[1] + 0.5 * math.sin(p[0]),
                    p[2]
                ]
            ),
            run_time=5,
        )
        self.wait()<N><N>
#!/usr/bin/env python<N><N>from setuptools import setup<N><N>setup(
    setup_requires=['pbr'],
    pbr=True,
)

from manimlib.imports import *<N><N>
class SquareToCircle(Scene):
    def construct(self):
        circle = Circle()
        circle.set_fill(BLUE, opacity=0.5)
        circle.set_stroke(BLUE_E, width=4)
        square = Square()<N><N>
        self.play(ShowCreation(square))
        self.wait()
        self.play(ReplacementTransform(square, circle))
        self.wait()
        # Try typing the following lines
        # self.play(circle.animate.stretch(4, dim=0))
        # self.play(Rotate(circle, TAU / 4))
        # self.play(circle.animate.shift(2 * RIGHT), circle.animate.scale(0.25))
        # circle.insert_n_curves(10)
        # self.play(circle.animate.apply_complex_function(lambda z: z**2))<N><N>
class SquareToCircleEmbed(Scene):
    def construct(self):
        circle = Circle()
        circle.set_fill(BLUE, opacity=0.5)
        circle.set_stroke(BLUE_E, width=4)<N><N>
        self.add(circle)
        self.wait()
        self.play(circle.animate.stretch(4, dim=0))
        self.wait(1.5)
        self.play(Rotate(circle, TAU / 4))
        self.wait(1.5)
        self.play(circle.animate.shift(2 * RIGHT), circle.animate.scale(0.25))
        self.wait(1.5)
        circle.insert_n_curves(10)
        self.play(circle.animate.apply_complex_function(lambda z: z**2))
        self.wait(2)
<N><N>
import os
import sys
sys.path.insert(0, os.path.abspath("."))
sys.path.insert(0, os.path.abspath('../../'))<N><N>

project = 'manim'
copyright = '- This document has been placed in the public domain.'
author = 'TonyCrane'<N><N>
release = ''<N><N>
extensions = [
    'sphinx.ext.todo',
    'sphinx.ext.githubpages',
    'sphinx.ext.mathjax',
    'sphinx.ext.intersphinx',
    'sphinx.ext.autodoc', 
    'sphinx.ext.coverage',
    'sphinx.ext.napoleon',
    'sphinx_copybutton',
    'manim_example_ext'
]<N><N>
autoclass_content = 'both'
mathjax_path = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"<N><N>
templates_path = ['_templates']
source_suffix = '.rst'
master_doc = 'index'
pygments_style = 'default'<N><N>
html_static_path = ["_static"]
html_css_files = ["custom.css", "colors.css"]
html_theme = 'furo'  # pip install furo==2020.10.5b9
html_favicon = '_static/icon.png'
html_logo = '../../logo/transparent_graph.png'
html_theme_options = {
    "sidebar_hide_name": True,
}
<N><N>
from docutils import nodes
from docutils.parsers.rst import directives, Directive<N><N>
import jinja2
import os<N><N>

class skip_manim_node(nodes.Admonition, nodes.Element):
    pass<N><N>

def visit(self, node, name=""):
    self.visit_admonition(node, name)<N><N>

def depart(self, node):
    self.depart_admonition(node)<N><N>

class ManimExampleDirective(Directive):
    has_content = True
    required_arguments = 1
    optional_arguments = 0
    option_spec = {
        "hide_code": bool,
        "media": str,
    }
    final_argument_whitespace = True<N><N>
    def run(self):
        hide_code = "hide_code" in self.options
        scene_name = self.arguments[0]
        media_file_name = self.options["media"]<N><N>
        source_block = [
            ".. code-block:: python",
            "",
            *["    " + line for line in self.content],
        ]
        source_block = "\n".join(source_block)<N><N>
        state_machine = self.state_machine
        document = state_machine.document<N><N>
        if any(media_file_name.endswith(ext) for ext in [".png", ".jpg", ".gif"]):
            is_video = False
        else:
            is_video = True<N><N>
        rendered_template = jinja2.Template(TEMPLATE).render(
            scene_name=scene_name,
            scene_name_lowercase=scene_name.lower(),
            hide_code=hide_code,
            is_video=is_video,
            media_file_name=media_file_name,
            source_block=source_block,
        )
        state_machine.insert_input(
            rendered_template.split("\n"), source=document.attributes["source"]
        )<N><N>
        return []<N><N>

def setup(app):
    app.add_node(skip_manim_node, html=(visit, depart))<N><N>
    setup.app = app
    setup.config = app.config
    setup.confdir = app.confdir<N><N>
    app.add_directive("manim-example", ManimExampleDirective)<N><N>
    metadata = {"parallel_read_safe": False, "parallel_write_safe": True}
    return metadata<N><N>

TEMPLATE = r"""
{% if not hide_code %}<N><N>
.. raw:: html<N><N>
    <div class="manim-example"><N><N>
{% endif %}<N><N>
{% if is_video %}
.. raw:: html<N><N>
    <video id="{{ scene_name_lowercase }}" class="manim-video" controls loop autoplay src="{{ media_file_name }}"></video>
{% else %}
.. image:: {{ media_file_name }}
    :align: center
    :name: {{ scene_name_lowercase }}
{% endif %}<N><N>
{% if not hide_code %}
.. raw:: html<N><N>
    <h5 class="example-header">{{ scene_name }}<a class="headerlink" href="#{{ scene_name_lowercase }}">¶</a></h5><N><N>
{{ source_block }}
{% endif %}<N><N>
.. raw:: html<N><N>
    </div>
"""<N><N>
from manimlib.imports import *<N><N>
NEW_BLUE = "#68a8e1"<N><N>
class Thumbnail(GraphScene):
    CONFIG = {
        "y_max": 8,
        "y_axis_height": 5,
    }<N><N>
    def construct(self):
        self.show_function_graph()<N><N>
    def show_function_graph(self):
        self.setup_axes(animate=False)
        def func(x):
            return 0.1 * (x + 3-5) * (x - 3-5) * (x-5) + 5<N><N>
        def rect(x):
            return 2.775*(x-1.5)+3.862
        recta = self.get_graph(rect,x_min=-1,x_max=5)
        graph = self.get_graph(func,x_min=0.2,x_max=9)
        graph.set_color(NEW_BLUE)
        input_tracker_p1 = ValueTracker(1.5)
        input_tracker_p2 = ValueTracker(3.5)<N><N>
        def get_x_value(input_tracker):
            return input_tracker.get_value()<N><N>
        def get_y_value(input_tracker):
            return graph.underlying_function(get_x_value(input_tracker))<N><N>
        def get_x_point(input_tracker):
            return self.coords_to_point(get_x_value(input_tracker), 0)<N><N>
        def get_y_point(input_tracker):
            return self.coords_to_point(0, get_y_value(input_tracker))<N><N>
        def get_graph_point(input_tracker):
            return self.coords_to_point(get_x_value(input_tracker), get_y_value(input_tracker))<N><N>
        def get_v_line(input_tracker):
            return DashedLine(get_x_point(input_tracker), get_graph_point(input_tracker), stroke_width=2)<N><N>
import argparse
import colour
import inspect
import importlib
import os
import sys
import yaml
from screeninfo import get_monitors<N><N>
from manimlib.utils.config_ops import merge_dicts_recursively
from manimlib.utils.init_config import init_customization<N><N>
import numpy as np<N><N>
# Sizes relevant to default camera frame
ASPECT_RATIO = 16.0 / 9.0
FRAME_HEIGHT = 8.0
FRAME_WIDTH = FRAME_HEIGHT * ASPECT_RATIO
FRAME_Y_RADIUS = FRAME_HEIGHT / 2
FRAME_X_RADIUS = FRAME_WIDTH / 2<N><N>
DEFAULT_PIXEL_HEIGHT = 1080
DEFAULT_PIXEL_WIDTH = 1920
DEFAULT_FRAME_RATE = 30<N><N>
SMALL_BUFF = 0.1
MED_SMALL_BUFF = 0.25
MED_LARGE_BUFF = 0.5
LARGE_BUFF = 1<N><N>
DEFAULT_MOBJECT_TO_EDGE_BUFFER = MED_LARGE_BUFF
DEFAULT_MOBJECT_TO_MOBJECT_BUFFER = MED_SMALL_BUFF<N><N>

# All in seconds
DEFAULT_POINTWISE_FUNCTION_RUN_TIME = 3.0
DEFAULT_WAIT_TIME = 1.0<N><N>

ORIGIN = np.array((0., 0., 0.))
UP = np.array((0., 1., 0.))
DOWN = np.array((0., -1., 0.))
RIGHT = np.array((1., 0., 0.))
LEFT = np.array((-1., 0., 0.))
IN = np.array((0., 0., -1.))
OUT = np.array((0., 0., 1.))
X_AXIS = np.array((1., 0., 0.))
Y_AXIS = np.array((0., 1., 0.))
Z_AXIS = np.array((0., 0., 1.))<N><N>
# Useful abbreviations for diagonals
UL = UP + LEFT
UR = UP + RIGHT
DL = DOWN + LEFT
DR = DOWN + RIGHT<N><N>
TOP = FRAME_Y_RADIUS * UP
BOTTOM = FRAME_Y_RADIUS * DOWN
LEFT_SIDE = FRAME_X_RADIUS * LEFT
RIGHT_SIDE = FRAME_X_RADIUS * RIGHT<N><N>
PI = np.pi
TAU = 2 * PI
DEGREES = TAU / 360
# Nice to have a constant for readability
# when juxtaposed with expressions like 30 * DEGREES
RADIANS = 1<N><N>
FFMPEG_BIN = "ffmpeg"<N><N>
JOINT_TYPE_MAP = {
    "auto": 0,
    "round": 1,
    "bevel": 2,
    "miter": 3,
}<N><N>
# Related to Text
START_X = 30
START_Y = 20
NORMAL = "NORMAL"
ITALIC = "ITALIC"
OBLIQUE = "OBLIQUE"
BOLD = "BOLD"<N><N>
DEFAULT_STROKE_WIDTH = 4<N><N>
import inspect
import sys
import logging<N><N>
from manimlib.scene.scene import Scene
from manimlib.config import get_custom_config<N><N>

class BlankScene(Scene):
    def construct(self):
        exec(get_custom_config()["universal_import_line"])
        self.embed()<N><N>

def is_child_scene(obj, module):
    if not inspect.isclass(obj):
        return False
    if not issubclass(obj, Scene):
        return False
    if obj == Scene:
        return False
    if not obj.__module__.startswith(module.__name__):
        return False
    return True<N><N>
import os
import re
import moderngl
import numpy as np
import copy<N><N>
from manimlib.utils.directories import get_shader_dir
from manimlib.utils.file_ops import find_file<N><N>
# Mobjects that should be rendered with
# the same shader will be organized and
# clumped together based on keeping track
# of a dict holding all the relevant information
# to that shader<N><N>
import moderngl_window as mglw
from moderngl_window.context.pyglet.window import Window as PygletWindow
from moderngl_window.timers.clock import Timer
from screeninfo import get_monitors<N><N>
from manimlib.utils.config_ops import digest_config
from manimlib.utils.customization import get_customization<N><N>

class Window(PygletWindow):
    fullscreen = False
    resizable = True
    gl_version = (3, 3)
    vsync = True
    cursor = True<N><N>
    def __init__(self, scene, size=(1280, 720), **kwargs):
        super().__init__(size=size)
        digest_config(self, kwargs)<N><N>
        self.scene = scene
        self.pressed_keys = set()
        self.title = str(scene)
        self.size = size<N><N>
        mglw.activate_context(window=self)
        self.timer = Timer()
        self.config = mglw.WindowConfig(ctx=self.ctx, wnd=self, timer=self.timer)
        self.timer.start()<N><N>
        # No idea why, but when self.position is set once
        # it sometimes doesn't actually change the position
        # to the specified tuple on the rhs, but doing it
        # twice seems to make it work.  ¯\_(ツ)_/¯
        initial_position = self.find_initial_position(size)
        self.position = initial_position
        self.position = initial_position<N><N>
    def find_initial_position(self, size):
        custom_position = get_customization()["window_position"]
        monitors = get_monitors()
        mon_index = get_customization()["window_monitor"]
        monitor = monitors[min(mon_index, len(monitors) - 1)]
        window_width, window_height = size
        # Position might be specified with a string of the form
        # x,y for integers x and y
        if "," in custom_position:
            return tuple(map(int, custom_position.split(",")))<N><N>
        # Alternatively, it might be specified with a string like
        # UR, OO, DL, etc. specifiying what corner it should go to
        char_to_n = {"L": 0, "U": 0, "O": 1, "R": 2, "D": 2}
        width_diff = monitor.width - window_width
        height_diff = monitor.height - window_height
        return (
            monitor.x + char_to_n[custom_position[1]] * width_diff // 2,
            -monitor.y + char_to_n[custom_position[0]] * height_diff // 2,
        )<N><N>
    # Delegate event handling to scene
    def pixel_coords_to_space_coords(self, px, py, relative=False):
        return self.scene.camera.pixel_coords_to_space_coords(px, py, relative)<N><N>
    def on_mouse_motion(self, x, y, dx, dy):
        super().on_mouse_motion(x, y, dx, dy)
        point = self.pixel_coords_to_space_coords(x, y)
        d_point = self.pixel_coords_to_space_coords(dx, dy, relative=True)
        self.scene.on_mouse_motion(point, d_point)<N><N>
    def on_mouse_drag(self, x, y, dx, dy, buttons, modifiers):
        super().on_mouse_drag(x, y, dx, dy, buttons, modifiers)
        point = self.pixel_coords_to_space_coords(x, y)
        d_point = self.pixel_coords_to_space_coords(dx, dy, relative=True)
        self.scene.on_mouse_drag(point, d_point, buttons, modifiers)<N><N>
    def on_mouse_press(self, x: int, y: int, button, mods):
        super().on_mouse_press(x, y, button, mods)
        point = self.pixel_coords_to_space_coords(x, y)
        self.scene.on_mouse_press(point, button, mods)<N><N>
    def on_mouse_release(self, x: int, y: int, button, mods):
        super().on_mouse_release(x, y, button, mods)
        point = self.pixel_coords_to_space_coords(x, y)
        self.scene.on_mouse_release(point, button, mods)<N><N>
    def on_mouse_scroll(self, x, y, x_offset: float, y_offset: float):
        super().on_mouse_scroll(x, y, x_offset, y_offset)
        point = self.pixel_coords_to_space_coords(x, y)
        offset = self.pixel_coords_to_space_coords(x_offset, y_offset, relative=True)
        self.scene.on_mouse_scroll(point, offset)<N><N>
    def on_key_press(self, symbol, modifiers):
        self.pressed_keys.add(symbol)  # Modifiers?
        super().on_key_press(symbol, modifiers)
        self.scene.on_key_press(symbol, modifiers)<N><N>
    def on_key_release(self, symbol, modifiers):
        self.pressed_keys.difference_update({symbol})  # Modifiers?
        super().on_key_release(symbol, modifiers)
        self.scene.on_key_release(symbol, modifiers)<N><N>
    def on_resize(self, width: int, height: int):
        super().on_resize(width, height)
        self.scene.on_resize(width, height)<N><N>
    def on_show(self):
        super().on_show()
        self.scene.on_show()<N><N>
    def on_hide(self):
        super().on_hide()
        self.scene.on_hide()<N><N>
    def on_close(self):
        super().on_close()
        self.scene.on_close()<N><N>
    def is_key_pressed(self, symbol):
        return (symbol in self.pressed_keys)
<N><N>
from manimlib.constants import *<N><N>
#!/usr/bin/env python
import manimlib.config
import manimlib.extract_scene
import manimlib.utils.init_config<N><N>
def main():
    args = manimlib.config.parse_cli()<N><N>    if args.config:
        manimlib.utils.init_config.init_customization()
    else:
        config = manimlib.config.get_configuration(args)
        scenes = manimlib.extract_scene.main(config)<N><N>        for scene in scenes:
            scene.run()<N><N>if __name__ == '__main__':
    main()

from copy import deepcopy<N><N>
from manimlib.mobject.mobject import _AnimationBuilder
from manimlib.mobject.mobject import Mobject
from manimlib.utils.config_ops import digest_config
from manimlib.utils.rate_functions import smooth
from manimlib.utils.simple_functions import clip<N><N>

DEFAULT_ANIMATION_RUN_TIME = 1.0
DEFAULT_ANIMATION_LAG_RATIO = 0<N><N>
import numpy as np<N><N>
from manimlib.animation.animation import Animation, prepare_animation
from manimlib.mobject.mobject import Group
from manimlib.utils.bezier import integer_interpolate
from manimlib.utils.bezier import interpolate
from manimlib.utils.config_ops import digest_config
from manimlib.utils.iterables import remove_list_redundancies
from manimlib.utils.rate_functions import linear
from manimlib.utils.simple_functions import clip<N><N>

DEFAULT_LAGGED_START_LAG_RATIO = 0.05<N><N>

class AnimationGroup(Animation):
    CONFIG = {
        # If None, this defaults to the sum of all
        # internal animations
        "run_time": None,
        "rate_func": linear,
        # If 0, all animations are played at once.
        # If 1, all are played successively.
        # If >0 and <1, they start at lagged times
        # from one and other.
        "lag_ratio": 0,
        "group": None,
    }<N><N>
    def __init__(self, *animations, **kwargs):
        digest_config(self, kwargs)
        self.animations = [prepare_animation(anim) for anim in animations]
        if self.group is None:
            self.group = Group(*remove_list_redundancies(
                [anim.mobject for anim in animations]
            ))
        self.init_run_time()
        Animation.__init__(self, self.group, **kwargs)<N><N>
    def get_all_mobjects(self):
        return self.group<N><N>
    def begin(self):
        for anim in self.animations:
            anim.begin()
        # self.init_run_time()<N><N>
    def finish(self):
        for anim in self.animations:
            anim.finish()<N><N>
    def clean_up_from_scene(self, scene):
        for anim in self.animations:
            anim.clean_up_from_scene(scene)<N><N>
    def update_mobjects(self, dt):
        for anim in self.animations:
            anim.update_mobjects(dt)<N><N>
    def init_run_time(self):
        self.build_animations_with_timings()
        if self.anims_with_timings:
            self.max_end_time = np.max([
                awt[2] for awt in self.anims_with_timings
            ])
        else:
            self.max_end_time = 0
        if self.run_time is None:
            self.run_time = self.max_end_time<N><N>
from manimlib.animation.animation import Animation
from manimlib.animation.composition import Succession
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.mobject.mobject import Group
from manimlib.utils.bezier import integer_interpolate
from manimlib.utils.config_ops import digest_config
from manimlib.utils.rate_functions import linear
from manimlib.utils.rate_functions import double_smooth
from manimlib.utils.rate_functions import smooth<N><N>
import numpy as np
import itertools as it<N><N>

class ShowPartial(Animation):
    """
    Abstract class for ShowCreation and ShowPassingFlash
    """
    CONFIG = {
        "should_match_start": False,
    }<N><N>
    def begin(self):
        super().begin()
        if not self.should_match_start:
            self.mobject.lock_matching_data(self.mobject, self.starting_mobject)<N><N>
    def finish(self):
        super().finish()
        self.mobject.unlock_data()<N><N>
    def interpolate_submobject(self, submob, start_submob, alpha):
        submob.pointwise_become_partial(
            start_submob, *self.get_bounds(alpha)
        )<N><N>
    def get_bounds(self, alpha):
        raise Exception("Not Implemented")<N><N>

class ShowCreation(ShowPartial):
    CONFIG = {
        "lag_ratio": 1,
    }<N><N>
    def get_bounds(self, alpha):
        return (0, alpha)<N><N>

class Uncreate(ShowCreation):
    CONFIG = {
        "rate_func": lambda t: smooth(1 - t),
        "remover": True,
        "should_match_start": True,
    }<N><N>

class DrawBorderThenFill(Animation):
    CONFIG = {
        "run_time": 2,
        "rate_func": double_smooth,
        "stroke_width": 2,
        "stroke_color": None,
        "draw_border_animation_config": {},
        "fill_animation_config": {},
    }<N><N>
    def __init__(self, vmobject, **kwargs):
        assert(isinstance(vmobject, VMobject))
        self.sm_to_index = dict([
            (hash(sm), 0)
            for sm in vmobject.get_family()
        ])
        super().__init__(vmobject, **kwargs)<N><N>
    def begin(self):
        # Trigger triangulation calculation
        for submob in self.mobject.get_family():
            submob.get_triangulation()<N><N>
        self.outline = self.get_outline()
        super().begin()
        self.mobject.match_style(self.outline)
        self.mobject.lock_matching_data(self.mobject, self.outline)<N><N>
    def finish(self):
        super().finish()
        self.mobject.unlock_data()<N><N>
    def get_outline(self):
        outline = self.mobject.copy()
        outline.set_fill(opacity=0)
        for sm in outline.get_family():
            sm.set_stroke(
                color=self.get_stroke_color(sm),
                width=float(self.stroke_width)
            )
        return outline<N><N>
    def get_stroke_color(self, vmobject):
        if self.stroke_color:
            return self.stroke_color
        elif vmobject.get_stroke_width() > 0:
            return vmobject.get_stroke_color()
        return vmobject.get_color()<N><N>
    def get_all_mobjects(self):
        return [*super().get_all_mobjects(), self.outline]<N><N>
    def interpolate_submobject(self, submob, start, outline, alpha):
        index, subalpha = integer_interpolate(0, 2, alpha)<N><N>
        if index == 1 and self.sm_to_index[hash(submob)] == 0:
            # First time crossing over
            submob.set_data(outline.data)
            submob.unlock_data()
            if not self.mobject.has_updaters:
                submob.lock_matching_data(submob, start)
            submob.needs_new_triangulation = False
            self.sm_to_index[hash(submob)] = 1<N><N>
        if index == 0:
            submob.pointwise_become_partial(outline, 0, subalpha)
        else:
            submob.interpolate(outline, start, subalpha)<N><N>

class Write(DrawBorderThenFill):
    CONFIG = {
        # To be figured out in
        # set_default_config_from_lengths
        "run_time": None,
        "lag_ratio": None,
        "rate_func": linear,
    }<N><N>
    def __init__(self, mobject, **kwargs):
        digest_config(self, kwargs)
        self.set_default_config_from_length(mobject)
        super().__init__(mobject, **kwargs)<N><N>
    def set_default_config_from_length(self, mobject):
        length = len(mobject.family_members_with_points())
        if self.run_time is None:
            if length < 15:
                self.run_time = 1
            else:
                self.run_time = 2
        if self.lag_ratio is None:
            self.lag_ratio = min(4.0 / length, 0.2)<N><N>

class ShowIncreasingSubsets(Animation):
    CONFIG = {
        "suspend_mobject_updating": False,
        "int_func": np.round,
    }<N><N>
    def __init__(self, group, **kwargs):
        self.all_submobs = list(group.submobjects)
        super().__init__(group, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        n_submobs = len(self.all_submobs)
        index = int(self.int_func(alpha * n_submobs))
        self.update_submobject_list(index)<N><N>
    def update_submobject_list(self, index):
        self.mobject.set_submobjects(self.all_submobs[:index])<N><N>

class ShowSubmobjectsOneByOne(ShowIncreasingSubsets):
    CONFIG = {
        "int_func": np.ceil,
    }<N><N>
    def __init__(self, group, **kwargs):
        new_group = Group(*group)
        super().__init__(new_group, **kwargs)<N><N>
    def update_submobject_list(self, index):
        # N = len(self.all_submobs)
        if index == 0:
            self.mobject.set_submobjects([])
        else:
            self.mobject.set_submobjects(self.all_submobs[index - 1])<N><N>

# TODO, this is broken...
class AddTextWordByWord(Succession):
    CONFIG = {
        # If given a value for run_time, it will
        # override the time_per_char
        "run_time": None,
        "time_per_char": 0.06,
    }<N><N>
    def __init__(self, text_mobject, **kwargs):
        digest_config(self, kwargs)
        tpc = self.time_per_char
        anims = it.chain(*[
            [
                ShowIncreasingSubsets(word, run_time=tpc * len(word)),
                Animation(word, run_time=0.005 * len(word)**1.5),
            ]
            for word in text_mobject
        ])
        super().__init__(*anims, **kwargs)
<N><N>
import numpy as np<N><N>
from manimlib.animation.animation import Animation
from manimlib.animation.transform import Transform
from manimlib.mobject.mobject import Group
from manimlib.constants import ORIGIN
from manimlib.utils.bezier import interpolate
from manimlib.utils.rate_functions import there_and_back<N><N>

DEFAULT_FADE_LAG_RATIO = 0<N><N>

class Fade(Transform):
    CONFIG = {
        "lag_ratio": DEFAULT_FADE_LAG_RATIO,
    }<N><N>
    def __init__(self, mobject, shift=ORIGIN, scale=1, **kwargs):
        self.shift_vect = shift
        self.scale_factor = scale
        super().__init__(mobject, **kwargs)<N><N>

class FadeIn(Fade):
    CONFIG = {
        "lag_ratio": DEFAULT_FADE_LAG_RATIO,
    }<N><N>
    def create_target(self):
        return self.mobject<N><N>
    def create_starting_mobject(self):
        start = super().create_starting_mobject()
        start.set_opacity(0)
        start.scale(1.0 / self.scale_factor)
        start.shift(-self.shift_vect)
        return start<N><N>

class FadeOut(Fade):
    CONFIG = {
        "remover": True,
        # Put it back in original state when done
        "final_alpha_value": 0,
    }<N><N>
    def create_target(self):
        result = self.mobject.copy()
        result.set_opacity(0)
        result.shift(self.shift_vect)
        result.scale(self.scale_factor)
        return result<N><N>

class FadeInFromPoint(FadeIn):
    def __init__(self, mobject, point, **kwargs):
        super().__init__(
            mobject,
            shift=mobject.get_center() - point,
            scale=np.inf,
            **kwargs,
        )<N><N>

class FadeOutToPoint(FadeOut):
    def __init__(self, mobject, point, **kwargs):
        super().__init__(
            mobject,
            shift=point - mobject.get_center(),
            scale=0,
            **kwargs,
        )<N><N>

class FadeTransform(Transform):
    CONFIG = {
        "stretch": True,
        "dim_to_match": 1,
    }<N><N>
    def __init__(self, mobject, target_mobject, **kwargs):
        self.to_add_on_completion = target_mobject
        mobject.save_state()
        super().__init__(
            Group(mobject, target_mobject.copy()),
            **kwargs
        )<N><N>
    def begin(self):
        self.ending_mobject = self.mobject.copy()
        Animation.begin(self)
        # Both 'start' and 'end' consists of the source and target mobjects.
        # At the start, the traget should be faded replacing the source,
        # and at the end it should be the other way around.
        start, end = self.starting_mobject, self.ending_mobject
        for m0, m1 in ((start[1], start[0]), (end[0], end[1])):
            self.ghost_to(m0, m1)<N><N>
    def ghost_to(self, source, target):
        source.replace(target, stretch=self.stretch, dim_to_match=self.dim_to_match)
        source.set_opacity(0)<N><N>
    def get_all_mobjects(self):
        return [
            self.mobject,
            self.starting_mobject,
            self.ending_mobject,
        ]<N><N>
    def get_all_families_zipped(self):
        return Animation.get_all_families_zipped(self)<N><N>
    def clean_up_from_scene(self, scene):
        Animation.clean_up_from_scene(self, scene)
        scene.remove(self.mobject)
        self.mobject[0].restore()
        scene.add(self.to_add_on_completion)<N><N>

class FadeTransformPieces(FadeTransform):
    def begin(self):
        self.mobject[0].align_family(self.mobject[1])
        super().begin()<N><N>
    def ghost_to(self, source, target):
        for sm0, sm1 in zip(source.get_family(), target.get_family()):
            super().ghost_to(sm0, sm1)<N><N>

class VFadeIn(Animation):
    """
    VFadeIn and VFadeOut only work for VMobjects,
    """
    CONFIG = {
        "suspend_mobject_updating": False,
    }<N><N>
    def interpolate_submobject(self, submob, start, alpha):
        submob.set_stroke(
            opacity=interpolate(0, start.get_stroke_opacity(), alpha)
        )
        submob.set_fill(
            opacity=interpolate(0, start.get_fill_opacity(), alpha)
        )<N><N>

class VFadeOut(VFadeIn):
    CONFIG = {
        "remover": True,
        # Put it back in original state when done
        "final_alpha_value": 0,
    }<N><N>
    def interpolate_submobject(self, submob, start, alpha):
        super().interpolate_submobject(submob, start, 1 - alpha)<N><N>

class VFadeInThenOut(VFadeIn):
    CONFIG = {
        "rate_func": there_and_back,
        "remover": True,
        # Put it back in original state when done
        "final_alpha_value": 0.5,
    }
<N><N>
from manimlib.animation.transform import Transform
# from manimlib.utils.paths import counterclockwise_path
from manimlib.constants import PI<N><N>

class GrowFromPoint(Transform):
    CONFIG = {
        "point_color": None,
    }<N><N>
    def __init__(self, mobject, point, **kwargs):
        self.point = point
        super().__init__(mobject, **kwargs)<N><N>
    def create_target(self):
        return self.mobject<N><N>
    def create_starting_mobject(self):
        start = super().create_starting_mobject()
        start.scale(0)
        start.move_to(self.point)
        if self.point_color:
            start.set_color(self.point_color)
        return start<N><N>

class GrowFromCenter(GrowFromPoint):
    def __init__(self, mobject, **kwargs):
        point = mobject.get_center()
        super().__init__(mobject, point, **kwargs)<N><N>

class GrowFromEdge(GrowFromPoint):
    def __init__(self, mobject, edge, **kwargs):
        point = mobject.get_bounding_box_point(edge)
        super().__init__(mobject, point, **kwargs)<N><N>

class GrowArrow(GrowFromPoint):
    def __init__(self, arrow, **kwargs):
        point = arrow.get_start()
        super().__init__(arrow, point, **kwargs)<N><N>

class SpinInFromNothing(GrowFromCenter):
    CONFIG = {
        "path_arc": PI,
    }
<N><N>
import numpy as np
import math<N><N>
from manimlib.animation.animation import Animation
from manimlib.utils.rate_functions import linear<N><N>

class Homotopy(Animation):
    CONFIG = {
        "run_time": 3,
        "apply_function_kwargs": {},
    }<N><N>
    def __init__(self, homotopy, mobject, **kwargs):
        """
        Homotopy is a function from
        (x, y, z, t) to (x', y', z')
        """
        self.homotopy = homotopy
        super().__init__(mobject, **kwargs)<N><N>
    def function_at_time_t(self, t):
        return lambda p: self.homotopy(*p, t)<N><N>
    def interpolate_submobject(self, submob, start, alpha):
        submob.match_points(start)
        submob.apply_function(
            self.function_at_time_t(alpha),
            **self.apply_function_kwargs
        )<N><N>

class SmoothedVectorizedHomotopy(Homotopy):
    CONFIG = {
        "apply_function_kwargs": {"make_smooth": True},
    }<N><N>

class ComplexHomotopy(Homotopy):
    def __init__(self, complex_homotopy, mobject, **kwargs):
        """
        Given a function form (z, t) -> w, where z and w
        are complex numbers and t is time, this animates
        the state over time
        """
        def homotopy(x, y, z, t):
            c = complex_homotopy(complex(x, y), t)
            return (c.real, c.imag, z)
        super().__init__(homotopy, mobject, **kwargs)<N><N>

class PhaseFlow(Animation):
    CONFIG = {
        "virtual_time": 1,
        "rate_func": linear,
        "suspend_mobject_updating": False,
    }<N><N>
    def __init__(self, function, mobject, **kwargs):
        self.function = function
        super().__init__(mobject, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        if hasattr(self, "last_alpha"):
            dt = self.virtual_time * (alpha - self.last_alpha)
            self.mobject.apply_function(
                lambda p: p + dt * self.function(p)
            )
        self.last_alpha = alpha<N><N>

class MoveAlongPath(Animation):
    CONFIG = {
        "suspend_mobject_updating": False,
    }<N><N>
    def __init__(self, mobject, path, **kwargs):
        self.path = path
        super().__init__(mobject, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        point = self.path.point_from_proportion(alpha)
        self.mobject.move_to(point)
<N><N>
from manimlib.animation.animation import Animation
from manimlib.mobject.numbers import DecimalNumber
from manimlib.utils.bezier import interpolate<N><N>

class ChangingDecimal(Animation):
    CONFIG = {
        "suspend_mobject_updating": False,
    }<N><N>
    def __init__(self, decimal_mob, number_update_func, **kwargs):
        assert(isinstance(decimal_mob, DecimalNumber))
        self.number_update_func = number_update_func
        super().__init__(decimal_mob, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        self.mobject.set_value(
            self.number_update_func(alpha)
        )<N><N>

class ChangeDecimalToValue(ChangingDecimal):
    def __init__(self, decimal_mob, target_number, **kwargs):
        start_number = decimal_mob.number
        super().__init__(
            decimal_mob,
            lambda a: interpolate(start_number, target_number, a),
            **kwargs
        )<N><N>

class CountInFrom(ChangingDecimal):
    def __init__(self, decimal_mob, source_number=0, **kwargs):
        start_number = decimal_mob.number
        super().__init__(
            decimal_mob,
            lambda a: interpolate(source_number, start_number, a),
            **kwargs
        )
<N><N>
from manimlib.animation.animation import Animation
from manimlib.constants import OUT
from manimlib.constants import PI
from manimlib.constants import TAU
from manimlib.constants import ORIGIN
from manimlib.utils.rate_functions import linear
from manimlib.utils.rate_functions import smooth<N><N>

class Rotating(Animation):
    CONFIG = {
        # "axis": OUT,
        # "radians": TAU,
        "run_time": 5,
        "rate_func": linear,
        "about_point": None,
        "about_edge": None,
        "suspend_mobject_updating": False,
    }<N><N>
    def __init__(self, mobject, angle=TAU, axis=OUT, **kwargs):
        self.angle = angle
        self.axis = axis
        super().__init__(mobject, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        for sm1, sm2 in self.get_all_families_zipped():
            sm1.set_points(sm2.get_points())
        self.mobject.rotate(
            alpha * self.angle,
            axis=self.axis,
            about_point=self.about_point,
            about_edge=self.about_edge,
        )<N><N>

class Rotate(Rotating):
    CONFIG = {
        "run_time": 1,
        "rate_func": smooth,
        "about_edge": ORIGIN,
    }<N><N>
    def __init__(self, mobject, angle=PI, axis=OUT, **kwargs):
        super().__init__(mobject, angle, axis, **kwargs)
<N><N>
from manimlib.animation.composition import LaggedStart
from manimlib.animation.transform import Restore
from manimlib.constants import WHITE
from manimlib.constants import BLACK
from manimlib.mobject.geometry import Circle
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.utils.config_ops import digest_config<N><N>

class Broadcast(LaggedStart):
    CONFIG = {
        "small_radius": 0.0,
        "big_radius": 5,
        "n_circles": 5,
        "start_stroke_width": 8,
        "color": WHITE,
        "remover": True,
        "lag_ratio": 0.2,
        "run_time": 3,
        "remover": True,
    }<N><N>
import inspect<N><N>
import numpy as np<N><N>
import numpy as np<N><N>
from manimlib.animation.composition import AnimationGroup
from manimlib.animation.fading import FadeTransformPieces
from manimlib.animation.fading import FadeInFromPoint
from manimlib.animation.fading import FadeOutToPoint
from manimlib.animation.transform import Transform<N><N>
from manimlib.mobject.mobject import Mobject
from manimlib.mobject.mobject import Group
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.utils.config_ops import digest_config<N><N>

class TransformMatchingParts(AnimationGroup):
    CONFIG = {
        "mobject_type": Mobject,
        "group_type": Group,
        "transform_mismatches": False,
        "fade_transform_mismatches": False,
        "key_map": dict(),
    }<N><N>
    def __init__(self, mobject, target_mobject, **kwargs):
        digest_config(self, kwargs)
        assert(isinstance(mobject, self.mobject_type))
        assert(isinstance(target_mobject, self.mobject_type))
        source_map = self.get_shape_map(mobject)
        target_map = self.get_shape_map(target_mobject)<N><N>
import operator as op<N><N>
from manimlib.animation.animation import Animation<N><N>

class UpdateFromFunc(Animation):
    """
    update_function of the form func(mobject), presumably
    to be used when the state of one mobject is dependent
    on another simultaneously animated mobject
    """
    CONFIG = {
        "suspend_mobject_updating": False,
    }<N><N>
    def __init__(self, mobject, update_function, **kwargs):
        self.update_function = update_function
        super().__init__(mobject, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        self.update_function(self.mobject)<N><N>

class UpdateFromAlphaFunc(UpdateFromFunc):
    def interpolate_mobject(self, alpha):
        self.update_function(self.mobject, alpha)<N><N>

class MaintainPositionRelativeTo(Animation):
    def __init__(self, mobject, tracked_mobject, **kwargs):
        self.tracked_mobject = tracked_mobject
        self.diff = op.sub(
            mobject.get_center(),
            tracked_mobject.get_center(),
        )
        super().__init__(mobject, **kwargs)<N><N>
    def interpolate_mobject(self, alpha):
        target = self.tracked_mobject.get_center()
        location = self.mobject.get_center()
        self.mobject.shift(target - location + self.diff)
<N><N>
import moderngl
from colour import Color
import OpenGL.GL as gl<N><N>
from PIL import Image
import numpy as np
import itertools as it<N><N>
import numpy as np<N><N>
from manimlib.event_handler.event_type import EventType
from manimlib.event_handler.event_listner import EventListner<N><N>

class EventDispatcher(object):
    def __init__(self):
        self.event_listners = {
            event_type: []
            for event_type in EventType
        }
        self.mouse_point = np.array((0., 0., 0.))
        self.mouse_drag_point = np.array((0., 0., 0.))
        self.pressed_keys = set()
        self.draggable_object_listners = []<N><N>
    def add_listner(self, event_listner):
        assert(isinstance(event_listner, EventListner))
        self.event_listners[event_listner.event_type].append(event_listner)
        return self<N><N>
    def remove_listner(self, event_listner):
        assert(isinstance(event_listner, EventListner))
        try:
            while event_listner in self.event_listners[event_listner.event_type]:
                self.event_listners[event_listner.event_type].remove(event_listner)
        except:
            # raise ValueError("Handler is not handling this event, so cannot remove it.")
            pass
        return self<N><N>
    def dispatch(self, event_type, **event_data):<N><N>
class EventListner(object):
    def __init__(self, mobject, event_type, event_callback):
        self.mobject = mobject
        self.event_type = event_type
        self.callback = event_callback<N><N>    def __eq__(self, o: object) -> bool:
        return_val = False
        try:
            return_val = self.callback == o.callback \
                and self.mobject == o.mobject \
                and self.event_type == o.event_type
        except:
            pass
        return return_val

from enum import Enum<N><N>
class EventType(Enum):
    MouseMotionEvent = 'mouse_motion_event'
    MousePressEvent = 'mouse_press_event'
    MouseReleaseEvent = 'mouse_release_event'
    MouseDragEvent = 'mouse_drag_event'
    MouseScrollEvent = 'mouse_scroll_event'
    KeyPressEvent = 'key_press_event'
    KeyReleaseEvent = 'key_release_event'

from manimlib.event_handler.event_dispatcher import EventDispatcher<N><N>
# This is supposed to be a Singleton
# i.e., during runtime there should be only one object of Event Dispatcher
EVENT_DISPATCHER = EventDispatcher()

from manimlib.constants import *
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.utils.rate_functions import smooth
from manimlib.utils.space_ops import get_norm<N><N>

class AnimatedBoundary(VGroup):
    CONFIG = {
        "colors": [BLUE_D, BLUE_B, BLUE_E, GREY_BROWN],
        "max_stroke_width": 3,
        "cycle_rate": 0.5,
        "back_and_forth": True,
        "draw_rate_func": smooth,
        "fade_rate_func": smooth,
    }<N><N>
    def __init__(self, vmobject, **kwargs):
        super().__init__(**kwargs)
        self.vmobject = vmobject
        self.boundary_copies = [
            vmobject.copy().set_style(
                stroke_width=0,
                fill_opacity=0
            )
            for x in range(2)
        ]
        self.add(*self.boundary_copies)
        self.total_time = 0
        self.add_updater(
            lambda m, dt: self.update_boundary_copies(dt)
        )<N><N>
    def update_boundary_copies(self, dt):
        # Not actual time, but something which passes at
        # an altered rate to make the implementation below
        # cleaner
        time = self.total_time * self.cycle_rate
        growing, fading = self.boundary_copies
        colors = self.colors
        msw = self.max_stroke_width
        vmobject = self.vmobject<N><N>
        index = int(time % len(colors))
        alpha = time % 1
        draw_alpha = self.draw_rate_func(alpha)
        fade_alpha = self.fade_rate_func(alpha)<N><N>
        if self.back_and_forth and int(time) % 2 == 1:
            bounds = (1 - draw_alpha, 1)
        else:
            bounds = (0, draw_alpha)
        self.full_family_become_partial(growing, vmobject, *bounds)
        growing.set_stroke(colors[index], width=msw)<N><N>
        if time >= 1:
            self.full_family_become_partial(fading, vmobject, 0, 1)
            fading.set_stroke(
                color=colors[index - 1],
                width=(1 - fade_alpha) * msw
            )<N><N>
        self.total_time += dt<N><N>
    def full_family_become_partial(self, mob1, mob2, a, b):
        family1 = mob1.family_members_with_points()
        family2 = mob2.family_members_with_points()
        for sm1, sm2 in zip(family1, family2):
            sm1.pointwise_become_partial(sm2, a, b)
        return self<N><N>

class TracedPath(VMobject):
    CONFIG = {
        "stroke_width": 2,
        "stroke_color": WHITE,
        "min_distance_to_new_point": 0.1,
    }<N><N>
    def __init__(self, traced_point_func, **kwargs):
        super().__init__(**kwargs)
        self.traced_point_func = traced_point_func
        self.add_updater(lambda m: m.update_path())<N><N>
    def update_path(self):
        new_point = self.traced_point_func()
        if not self.has_points():
            self.start_new_path(new_point)
            self.add_line_to(new_point)
        else:
            # Set the end to be the new point
            self.get_points()[-1] = new_point<N><N>
            # Second to last point
            nppcc = self.n_points_per_curve
            dist = get_norm(new_point - self.get_points()[-nppcc])
            if dist >= self.min_distance_to_new_point:
                self.add_line_to(new_point)
<N><N>
import numpy as np
import numbers<N><N>
from manimlib.constants import *
from manimlib.mobject.geometry import Rectangle
from manimlib.utils.config_ops import digest_config<N><N>

class ScreenRectangle(Rectangle):
    CONFIG = {
        "aspect_ratio": 16.0 / 9.0,
        "height": 4
    }<N><N>
    def __init__(self, **kwargs):
        Rectangle.__init__(self, **kwargs)
        self.set_width(
            self.aspect_ratio * self.get_height(),
            stretch=True
        )<N><N>

class FullScreenRectangle(ScreenRectangle):
    CONFIG = {
        "height": FRAME_HEIGHT,
        "fill_color": GREY_E,
        "fill_opacity": 1,
        "stroke_width": 0,
    }<N><N>

class FullScreenFadeRectangle(FullScreenRectangle):
    CONFIG = {
        "stroke_width": 0,
        "fill_color": BLACK,
        "fill_opacity": 0.7,
    }<N><N>

class PictureInPictureFrame(Rectangle):
    CONFIG = {
        "height": 3,
        "aspect_ratio": 16.0 / 9.0
    }<N><N>
    def __init__(self, **kwargs):
        digest_config(self, kwargs)
        Rectangle.__init__(
            self,
            width=self.aspect_ratio * self.height,
            height=self.height,
            **kwargs
        )
<N><N>
from manimlib.constants import *
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.utils.config_ops import digest_config<N><N>

class ParametricCurve(VMobject):
    CONFIG = {
        "t_range": [0, 1, 0.1],
        "epsilon": 1e-8,
        # TODO, automatically figure out discontinuities
        "discontinuities": [],
        "use_smoothing": True,
    }<N><N>
import numpy as np<N><N>
import numpy as np
from pyglet.window import key as PygletWindowKeys<N><N>
import numpy as np
import itertools as it<N><N>
from manimlib.constants import *
from manimlib.mobject.numbers import DecimalNumber
from manimlib.mobject.numbers import Integer
from manimlib.mobject.shape_matchers import BackgroundRectangle
from manimlib.mobject.svg.tex_mobject import Tex
from manimlib.mobject.svg.tex_mobject import TexText
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.mobject.types.vectorized_mobject import VMobject<N><N>
VECTOR_LABEL_SCALE_FACTOR = 0.8<N><N>

def matrix_to_tex_string(matrix):
    matrix = np.array(matrix).astype("str")
    if matrix.ndim == 1:
        matrix = matrix.reshape((matrix.size, 1))
    n_rows, n_cols = matrix.shape
    prefix = "\\left[ \\begin{array}{%s}" % ("c" * n_cols)
    suffix = "\\end{array} \\right]"
    rows = [
        " & ".join(row)
        for row in matrix
    ]
    return prefix + " \\\\ ".join(rows) + suffix<N><N>

def matrix_to_mobject(matrix):
    return Tex(matrix_to_tex_string(matrix))<N><N>

def vector_coordinate_label(vector_mob, integer_labels=True,
                            n_dim=2, color=WHITE):
    vect = np.array(vector_mob.get_end())
    if integer_labels:
        vect = np.round(vect).astype(int)
    vect = vect[:n_dim]
    vect = vect.reshape((n_dim, 1))
    label = Matrix(vect, add_background_rectangles_to_entries=True)
    label.scale(VECTOR_LABEL_SCALE_FACTOR)<N><N>
    shift_dir = np.array(vector_mob.get_end())
    if shift_dir[0] >= 0:  # Pointing right
        shift_dir -= label.get_left() + DEFAULT_MOBJECT_TO_MOBJECT_BUFFER * LEFT
    else:  # Pointing left
        shift_dir -= label.get_right() + DEFAULT_MOBJECT_TO_MOBJECT_BUFFER * RIGHT
    label.shift(shift_dir)
    label.set_color(color)
    label.rect = BackgroundRectangle(label)
    label.add_to_back(label.rect)
    return label<N><N>

class Matrix(VMobject):
    CONFIG = {
        "v_buff": 0.8,
        "h_buff": 1.3,
        "bracket_h_buff": 0.2,
        "bracket_v_buff": 0.25,
        "add_background_rectangles_to_entries": False,
        "include_background_rectangle": False,
        "element_to_mobject": Tex,
        "element_to_mobject_config": {},
        "element_alignment_corner": DOWN,
    }<N><N>
import copy
import itertools as it
import random
import sys
import moderngl
from functools import wraps<N><N>
import numpy as np<N><N>
import inspect<N><N>
from manimlib.constants import DEGREES
from manimlib.constants import RIGHT
from manimlib.mobject.mobject import Mobject
from manimlib.utils.simple_functions import clip<N><N>

def assert_is_mobject_method(method):
    assert(inspect.ismethod(method))
    mobject = method.__self__
    assert(isinstance(mobject, Mobject))<N><N>

def always(method, *args, **kwargs):
    assert_is_mobject_method(method)
    mobject = method.__self__
    func = method.__func__
    mobject.add_updater(lambda m: func(m, *args, **kwargs))
    return mobject<N><N>

def f_always(method, *arg_generators, **kwargs):
    """
    More functional version of always, where instead
    of taking in args, it takes in functions which ouput
    the relevant arguments.
    """
    assert_is_mobject_method(method)
    mobject = method.__self__
    func = method.__func__<N><N>
    def updater(mob):
        args = [
            arg_generator()
            for arg_generator in arg_generators
        ]
        func(mob, *args, **kwargs)<N><N>
    mobject.add_updater(updater)
    return mobject<N><N>

def always_redraw(func, *args, **kwargs):
    mob = func(*args, **kwargs)
    mob.add_updater(lambda m: mob.become(func(*args, **kwargs)))
    return mob<N><N>

def always_shift(mobject, direction=RIGHT, rate=0.1):
    mobject.add_updater(
        lambda m, dt: m.shift(dt * rate * direction)
    )
    return mobject<N><N>

def always_rotate(mobject, rate=20 * DEGREES, **kwargs):
    mobject.add_updater(
        lambda m, dt: m.rotate(dt * rate, **kwargs)
    )
    return mobject<N><N>

def turn_animation_into_updater(animation, cycle=False, **kwargs):
    """
    Add an updater to the animation's mobject which applies
    the interpolation and update functions of the animation<N><N>
    If cycle is True, this repeats over and over.  Otherwise,
    the updater will be popped uplon completion
    """
    mobject = animation.mobject
    animation.update_config(**kwargs)
    animation.suspend_mobject_updating = False
    animation.begin()
    animation.total_time = 0<N><N>
    def update(m, dt):
        run_time = animation.get_run_time()
        time_ratio = animation.total_time / run_time
        if cycle:
            alpha = time_ratio % 1
        else:
            alpha = clip(time_ratio, 0, 1)
            if alpha >= 1:
                animation.finish()
                m.remove_updater(update)
                return
        animation.interpolate(alpha)
        animation.update_mobjects(dt)
        animation.total_time += dt<N><N>
    mobject.add_updater(update)
    return mobject<N><N>

def cycle_animation(animation, **kwargs):
    return turn_animation_into_updater(
        animation, cycle=True, **kwargs
    )
<N><N>
from manimlib.constants import *
from manimlib.mobject.svg.tex_mobject import SingleStringTex
from manimlib.mobject.svg.text_mobject import Text
from manimlib.mobject.types.vectorized_mobject import VMobject<N><N>

string_to_mob_map = {}<N><N>

class DecimalNumber(VMobject):
    CONFIG = {
        "stroke_width": 0,
        "fill_opacity": 1.0,
        "num_decimal_places": 2,
        "include_sign": False,
        "group_with_commas": True,
        "digit_buff_per_font_unit": 0.001,
        "show_ellipsis": False,
        "unit": None,  # Aligned to bottom unless it starts with "^"
        "include_background_rectangle": False,
        "edge_to_fix": LEFT,
        "font_size": 48,
    }<N><N>
    def __init__(self, number=0, **kwargs):
        super().__init__(**kwargs)
        self.set_submobjects_from_number(number)
        self.init_colors()<N><N>
    def set_submobjects_from_number(self, number):
        self.number = number
        self.set_submobjects([])<N><N>
        num_string = self.get_num_string(number)
        self.add(*map(self.string_to_mob, num_string))<N><N>
        # Add non-numerical bits
        if self.show_ellipsis:
            self.add(self.string_to_mob("..."))
        if self.unit is not None:
            self.unit_sign = self.string_to_mob(self.unit, SingleStringTex)
            self.add(self.unit_sign)<N><N>
        self.arrange(
            buff=self.digit_buff_per_font_unit * self.get_font_size(),
            aligned_edge=DOWN
        )<N><N>
        # Handle alignment of parts that should be aligned
        # to the bottom
        for i, c in enumerate(num_string):
            if c == "–" and len(num_string) > i + 1:
                self[i].align_to(self[i + 1], UP)
                self[i].shift(self[i + 1].get_height() * DOWN / 2)
            elif c == ",":
                self[i].shift(self[i].get_height() * DOWN / 2)
        if self.unit and self.unit.startswith("^"):
            self.unit_sign.align_to(self, UP)<N><N>
        if self.include_background_rectangle:
            self.add_background_rectangle()<N><N>
    def get_num_string(self, number):
        if isinstance(number, complex):
            formatter = self.get_complex_formatter()
        else:
            formatter = self.get_formatter()
        num_string = formatter.format(number)<N><N>
        rounded_num = np.round(number, self.num_decimal_places)
        if num_string.startswith("-") and rounded_num == 0:
            if self.include_sign:
                num_string = "+" + num_string[1:]
            else:
                num_string = num_string[1:]
        num_string = num_string.replace("-", "–")
        return num_string<N><N>
    def init_data(self):
        super().init_data()
        self.data["font_size"] = np.array([self.font_size], dtype=float)<N><N>
    def get_font_size(self):
        return self.data["font_size"][0]<N><N>
    def string_to_mob(self, string, mob_class=Text):
        if string not in string_to_mob_map:
            string_to_mob_map[string] = mob_class(string, font_size=1)
        mob = string_to_mob_map[string].copy()
        mob.scale(self.get_font_size())
        return mob<N><N>
from manimlib.constants import *
from manimlib.mobject.geometry import Line
from manimlib.mobject.numbers import DecimalNumber
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.utils.bezier import interpolate
from manimlib.utils.config_ops import digest_config
from manimlib.utils.config_ops import merge_dicts_recursively
from manimlib.utils.simple_functions import fdiv
from manimlib.utils.space_ops import normalize<N><N>
from manimlib.constants import *
from manimlib.mobject.geometry import Line
from manimlib.mobject.geometry import Rectangle
from manimlib.mobject.mobject import Mobject
from manimlib.mobject.svg.brace import Brace
from manimlib.mobject.svg.tex_mobject import Tex
from manimlib.mobject.svg.tex_mobject import TexText
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.utils.color import color_gradient
from manimlib.utils.iterables import listify<N><N>
EPSILON = 0.0001<N><N>

class SampleSpace(Rectangle):
    CONFIG = {
        "height": 3,
        "width": 3,
        "fill_color": GREY_D,
        "fill_opacity": 1,
        "stroke_width": 0.5,
        "stroke_color": GREY_B,
        ##
        "default_label_scale_val": 1,
    }<N><N>
    def add_title(self, title="Sample space", buff=MED_SMALL_BUFF):
        # TODO, should this really exist in SampleSpaceScene
        title_mob = TexText(title)
        if title_mob.get_width() > self.get_width():
            title_mob.set_width(self.get_width())
        title_mob.next_to(self, UP, buff=buff)
        self.title = title_mob
        self.add(title_mob)<N><N>
    def add_label(self, label):
        self.label = label<N><N>
    def complete_p_list(self, p_list):
        new_p_list = listify(p_list)
        remainder = 1.0 - sum(new_p_list)
        if abs(remainder) > EPSILON:
            new_p_list.append(remainder)
        return new_p_list<N><N>
    def get_division_along_dimension(self, p_list, dim, colors, vect):
        p_list = self.complete_p_list(p_list)
        colors = color_gradient(colors, len(p_list))<N><N>
        last_point = self.get_edge_center(-vect)
        parts = VGroup()
        for factor, color in zip(p_list, colors):
            part = SampleSpace()
            part.set_fill(color, 1)
            part.replace(self, stretch=True)
            part.stretch(factor, dim)
            part.move_to(last_point, -vect)
            last_point = part.get_edge_center(vect)
            parts.add(part)
        return parts<N><N>
    def get_horizontal_division(
        self, p_list,
        colors=[GREEN_E, BLUE_E],
        vect=DOWN
    ):
        return self.get_division_along_dimension(p_list, 1, colors, vect)<N><N>
    def get_vertical_division(
        self, p_list,
        colors=[MAROON_B, YELLOW],
        vect=RIGHT
    ):
        return self.get_division_along_dimension(p_list, 0, colors, vect)<N><N>
    def divide_horizontally(self, *args, **kwargs):
        self.horizontal_parts = self.get_horizontal_division(*args, **kwargs)
        self.add(self.horizontal_parts)<N><N>
    def divide_vertically(self, *args, **kwargs):
        self.vertical_parts = self.get_vertical_division(*args, **kwargs)
        self.add(self.vertical_parts)<N><N>
from manimlib.constants import *
from manimlib.mobject.geometry import Line
from manimlib.mobject.geometry import Rectangle
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.utils.color import Color
from manimlib.utils.customization import get_customization
from manimlib.utils.config_ops import digest_config<N><N>

class SurroundingRectangle(Rectangle):
    CONFIG = {
        "color": YELLOW,
        "buff": SMALL_BUFF,
    }<N><N>
    def __init__(self, mobject, **kwargs):
        digest_config(self, kwargs)
        kwargs["width"] = mobject.get_width() + 2 * self.buff
        kwargs["height"] = mobject.get_height() + 2 * self.buff
        Rectangle.__init__(self, **kwargs)
        self.move_to(mobject)<N><N>

class BackgroundRectangle(SurroundingRectangle):
    CONFIG = {
        "stroke_width": 0,
        "stroke_opacity": 0,
        "fill_opacity": 0.75,
        "buff": 0
    }<N><N>
    def __init__(self, mobject, color=None, **kwargs):
        if color is None:
            color = get_customization()['style']['background_color']
        SurroundingRectangle.__init__(self, mobject, color=color, **kwargs)
        self.original_fill_opacity = self.fill_opacity<N><N>
    def pointwise_become_partial(self, mobject, a, b):
        self.set_fill(opacity=b * self.original_fill_opacity)
        return self<N><N>
import math<N><N>
from manimlib.constants import *
from manimlib.mobject.types.surface import Surface
from manimlib.mobject.types.surface import SGroup
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.utils.config_ops import digest_config
from manimlib.utils.space_ops import get_norm
from manimlib.utils.space_ops import z_to_vector<N><N>

class SurfaceMesh(VGroup):
    CONFIG = {
        "resolution": (21, 21),
        "stroke_width": 1,
        "normal_nudge": 1e-2,
        "depth_test": True,
        "flat_stroke": False,
    }<N><N>
    def __init__(self, uv_surface, **kwargs):
        if not isinstance(uv_surface, Surface):
            raise Exception("uv_surface must be of type Surface")
        self.uv_surface = uv_surface
        super().__init__(**kwargs)<N><N>
    def init_points(self):
        uv_surface = self.uv_surface<N><N>
        full_nu, full_nv = uv_surface.resolution
        part_nu, part_nv = self.resolution
        u_indices = np.linspace(0, full_nu, part_nu).astype(int)
        v_indices = np.linspace(0, full_nv, part_nv).astype(int)<N><N>
        points, du_points, dv_points = uv_surface.get_surface_points_and_nudged_points()
        normals = uv_surface.get_unit_normals()
        nudge = 1e-2
        nudged_points = points + nudge * normals<N><N>
        for ui in u_indices:
            path = VMobject()
            full_ui = full_nv * ui
            path.set_points_smoothly(nudged_points[full_ui:full_ui + full_nv])
            self.add(path)
        for vi in v_indices:
            path = VMobject()
            path.set_points_smoothly(nudged_points[vi::full_nv])
            self.add(path)<N><N>

# 3D shapes<N><N>
class Sphere(Surface):
    CONFIG = {
        "resolution": (101, 51),
        "radius": 1,
        "u_range": (0, TAU),
        "v_range": (0, PI),
    }<N><N>
    def uv_func(self, u, v):
        return self.radius * np.array([
            np.cos(u) * np.sin(v),
            np.sin(u) * np.sin(v),
            -np.cos(v)
        ])<N><N>

class Torus(Surface):
    CONFIG = {
        "u_range": (0, TAU),
        "v_range": (0, TAU),
        "r1": 3,
        "r2": 1,
    }<N><N>
    def uv_func(self, u, v):
        P = np.array([math.cos(u), math.sin(u), 0])
        return (self.r1 - self.r2 * math.cos(v)) * P - math.sin(v) * OUT<N><N>

class Cylinder(Surface):
    CONFIG = {
        "height": 2,
        "radius": 1,
        "axis": OUT,
        "u_range": (0, TAU),
        "v_range": (-1, 1),
        "resolution": (101, 11),
    }<N><N>
    def init_points(self):
        super().init_points()
        self.scale(self.radius)
        self.set_depth(self.height, stretch=True)
        self.apply_matrix(z_to_vector(self.axis))
        return self<N><N>
    def uv_func(self, u, v):
        return [np.cos(u), np.sin(u), v]<N><N>

class Line3D(Cylinder):
    CONFIG = {
        "width": 0.05,
        "resolution": (21, 25)
    }<N><N>
    def __init__(self, start, end, **kwargs):
        digest_config(self, kwargs)
        axis = end - start
        super().__init__(
            height=get_norm(axis),
            radius=self.width / 2,
            axis=axis
        )
        self.shift((start + end) / 2)<N><N>

class Disk3D(Surface):
    CONFIG = {
        "radius": 1,
        "u_range": (0, 1),
        "v_range": (0, TAU),
        "resolution": (2, 25),
    }<N><N>
    def init_points(self):
        super().init_points()
        self.scale(self.radius)<N><N>
    def uv_func(self, u, v):
        return [
            u * np.cos(v),
            u * np.sin(v),
            0
        ]<N><N>

class Square3D(Surface):
    CONFIG = {
        "side_length": 2,
        "u_range": (-1, 1),
        "v_range": (-1, 1),
        "resolution": (2, 2),
    }<N><N>
    def init_points(self):
        super().init_points()
        self.scale(self.side_length / 2)<N><N>
    def uv_func(self, u, v):
        return [u, v, 0]<N><N>

class Cube(SGroup):
    CONFIG = {
        "color": BLUE,
        "opacity": 1,
        "gloss": 0.5,
        "square_resolution": (2, 2),
        "side_length": 2,
    }<N><N>
    def init_points(self):
        for vect in [OUT, RIGHT, UP, LEFT, DOWN, IN]:
            face = Square3D(resolution=self.square_resolution)
            face.shift(OUT)
            face.apply_matrix(z_to_vector(vect))
            self.add(face)
        self.set_height(self.side_length)<N><N>

class Prism(Cube):
    CONFIG = {
        "dimensions": [3, 2, 1]
    }<N><N>
    def init_points(self):
        Cube.init_points(self)
        for dim, value in enumerate(self.dimensions):
            self.rescale_to_fit(value, dim, stretch=True)
<N><N>
import numpy as np<N><N>
from manimlib.mobject.mobject import Mobject<N><N>

class ValueTracker(Mobject):
    """
    Not meant to be displayed.  Instead the position encodes some
    number, often one which another animation or continual_animation
    uses for its update function, and by treating it as a mobject it can
    still be animated and manipulated just like anything else.
    """
    CONFIG = {
        "value_type": np.float64,
    }<N><N>
    def __init__(self, value=0, **kwargs):
        super().__init__(**kwargs)
        self.set_value(value)<N><N>
    def init_data(self):
        super().init_data()
        self.data["value"] = np.zeros((1, 1), dtype=self.value_type)<N><N>
    def get_value(self):
        return self.data["value"][0, 0]<N><N>
    def set_value(self, value):
        self.data["value"][0, 0] = value
        return self<N><N>
    def increment_value(self, d_value):
        self.set_value(self.get_value() + d_value)<N><N>

class ExponentialValueTracker(ValueTracker):
    """
    Operates just like ValueTracker, except it encodes the value as the
    exponential of a position coordinate, which changes how interpolation
    behaves
    """<N><N>
    def get_value(self):
        return np.exp(ValueTracker.get_value(self))<N><N>
    def set_value(self, value):
        return ValueTracker.set_value(self, np.log(value))<N><N>

class ComplexValueTracker(ValueTracker):
    CONFIG = {
        "value_type": np.complex128
    }
<N><N>
import numpy as np
import itertools as it
import random<N><N>
from manimlib.constants import *<N><N>
import numpy as np
import math<N><N>
import itertools as it
import re
import string
import warnings
import os
import hashlib<N><N>
from xml.dom import minidom<N><N>
from manimlib.constants import DEFAULT_STROKE_WIDTH
from manimlib.constants import ORIGIN, UP, DOWN, LEFT, RIGHT
from manimlib.constants import BLACK
from manimlib.constants import WHITE<N><N>
from manimlib.mobject.geometry import Circle
from manimlib.mobject.geometry import Rectangle
from manimlib.mobject.geometry import RoundedRectangle
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.utils.color import *
from manimlib.utils.config_ops import digest_config
from manimlib.utils.directories import get_mobject_data_dir
from manimlib.utils.images import get_full_vector_image_path<N><N>

def string_to_numbers(num_string):
    num_string = num_string.replace("-", ",-")
    num_string = num_string.replace("e,-", "e-")
    return [
        float(s)
        for s in re.split("[ ,]", num_string)
        if s != ""
    ]<N><N>

class SVGMobject(VMobject):
    CONFIG = {
        "should_center": True,
        "height": 2,
        "width": None,
        # Must be filled in in a subclass, or when called
        "file_name": None,
        "unpack_groups": True,  # if False, creates a hierarchy of VGroups
        # TODO, style components should be read in, not defaulted
        "stroke_width": DEFAULT_STROKE_WIDTH,
        "fill_opacity": 1.0,
        "path_string_config": {}
    }<N><N>
    def __init__(self, file_name=None, **kwargs):
        digest_config(self, kwargs)
        self.file_name = file_name or self.file_name
        if file_name is None:
            raise Exception("Must specify file for SVGMobject")
        self.file_path = get_full_vector_image_path(file_name)<N><N>
        super().__init__(**kwargs)
        self.move_into_position()<N><N>
    def move_into_position(self):
        if self.should_center:
            self.center()
        if self.height is not None:
            self.set_height(self.height)
        if self.width is not None:
            self.set_width(self.width)<N><N>
    def init_points(self):
        doc = minidom.parse(self.file_path)
        self.ref_to_element = {}<N><N>
        for svg in doc.getElementsByTagName("svg"):
            mobjects = self.get_mobjects_from(svg)
            if self.unpack_groups:
                self.add(*mobjects)
            else:
                self.add(*mobjects[0].submobjects)
        doc.unlink()<N><N>
import copy
import hashlib
import os
import re
import typing
from contextlib import contextmanager
from pathlib import Path<N><N>
import manimpango
from manimlib.constants import *
from manimlib.mobject.geometry import Dot
from manimlib.mobject.svg.svg_mobject import SVGMobject
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.utils.config_ops import digest_config
from manimlib.utils.customization import get_customization
from manimlib.utils.directories import get_downloads_dir, get_text_dir
from manimpango import PangoUtils
from manimpango import TextSetting<N><N>
TEXT_MOB_SCALE_FACTOR = 0.001048<N><N>

class Text(SVGMobject):
    CONFIG = {
        # Mobject
        "color": WHITE,
        "height": None,
        "stroke_width": 0,
        # Text
        "font": '',
        "gradient": None,
        "lsh": -1,
        "size": 1,
        "font_size": 48,
        "tab_width": 4,
        "slant": NORMAL,
        "weight": NORMAL,
        "t2c": {},
        "t2f": {},
        "t2g": {},
        "t2s": {},
        "t2w": {},
        "disable_ligatures": True,
    }<N><N>
from functools import reduce
import operator as op
import re
import itertools as it<N><N>
from manimlib.constants import *
from manimlib.mobject.geometry import Line
from manimlib.mobject.svg.svg_mobject import SVGMobject
from manimlib.mobject.types.vectorized_mobject import VMobject
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.utils.config_ops import digest_config
from manimlib.utils.tex_file_writing import tex_to_svg_file
from manimlib.utils.tex_file_writing import get_tex_config
from manimlib.utils.tex_file_writing import display_during_execution<N><N>

SCALE_FACTOR_PER_FONT_POINT = 0.001<N><N>

tex_string_to_mob_map = {}<N><N>

class SingleStringTex(VMobject):
    CONFIG = {
        "fill_opacity": 1.0,
        "stroke_width": 0,
        "should_center": True,
        "font_size": 48,
        "height": None,
        "organize_left_to_right": False,
        "alignment": "\\centering",
        "math_mode": True,
    }<N><N>
import numpy as np
import moderngl<N><N>
from manimlib.constants import GREY_C
from manimlib.mobject.types.point_cloud_mobject import PMobject
from manimlib.utils.iterables import resize_preserving_order<N><N>

DEFAULT_DOT_CLOUD_RADIUS = 0.05
DEFAULT_GRID_HEIGHT = 6
DEFAULT_BUFF_RATIO = 0.5<N><N>

class DotCloud(PMobject):
    CONFIG = {
        "color": GREY_C,
        "opacity": 1,
        "radius": DEFAULT_DOT_CLOUD_RADIUS,
        "shader_folder": "true_dot",
        "render_primitive": moderngl.POINTS,
        "shader_dtype": [
            ('point', np.float32, (3,)),
            ('radius', np.float32, (1,)),
            ('color', np.float32, (4,)),
        ],
    }<N><N>
    def __init__(self, points=None, **kwargs):
        super().__init__(**kwargs)
        if points is not None:
            self.set_points(points)<N><N>
    def init_data(self):
        super().init_data()
        self.data["radii"] = np.zeros((1, 1))
        self.set_radius(self.radius)<N><N>
import numpy as np<N><N>
from PIL import Image<N><N>
from manimlib.constants import *
from manimlib.mobject.mobject import Mobject
from manimlib.utils.bezier import inverse_interpolate
from manimlib.utils.images import get_full_raster_image_path
from manimlib.utils.iterables import listify<N><N>

class ImageMobject(Mobject):
    CONFIG = {
        "height": 4,
        "opacity": 1,
        "shader_folder": "image",
        "shader_dtype": [
            ('point', np.float32, (3,)),
            ('im_coords', np.float32, (2,)),
            ('opacity', np.float32, (1,)),
        ]
    }<N><N>
    def __init__(self, filename, **kwargs):
        path = get_full_raster_image_path(filename)
        self.image = Image.open(path)
        self.texture_paths = {"Texture": path}
        super().__init__(**kwargs)<N><N>
    def init_data(self):
        self.data = {
            "points": np.array([UL, DL, UR, DR]),
            "im_coords": np.array([(0, 0), (0, 1), (1, 0), (1, 1)]),
            "opacity": np.array([[self.opacity]], dtype=np.float32),
        }<N><N>
    def init_points(self):
        size = self.image.size
        self.set_width(2 * size[0] / size[1], stretch=True)
        self.set_height(self.height)<N><N>
    def set_opacity(self, opacity, recurse=True):
        for mob in self.get_family(recurse):
            mob.data["opacity"] = np.array([[o] for o in listify(opacity)])
        return self<N><N>
    def point_to_rgb(self, point):
        x0, y0 = self.get_corner(UL)[:2]
        x1, y1 = self.get_corner(DR)[:2]
        x_alpha = inverse_interpolate(x0, x1, point[0])
        y_alpha = inverse_interpolate(y0, y1, point[1])
        if not (0 <= x_alpha <= 1) and (0 <= y_alpha <= 1):
            # TODO, raise smarter exception
            raise Exception("Cannot sample color from outside an image")<N><N>
        pw, ph = self.image.size
        rgb = self.image.getpixel((
            int((pw - 1) * x_alpha),
            int((ph - 1) * y_alpha),
        ))
        return np.array(rgb) / 255<N><N>
    def get_shader_data(self):
        shader_data = super().get_shader_data()
        self.read_data_to_shader(shader_data, "im_coords", "im_coords")
        self.read_data_to_shader(shader_data, "opacity", "opacity")
        return shader_data
<N><N>
from manimlib.constants import *
from manimlib.mobject.mobject import Mobject
from manimlib.utils.color import color_gradient
from manimlib.utils.color import color_to_rgba
from manimlib.utils.iterables import resize_with_interpolation
from manimlib.utils.iterables import resize_array<N><N>

class PMobject(Mobject):
    CONFIG = {
        "opacity": 1.0,
    }<N><N>
    def resize_points(self, size, resize_func=resize_array):
        # TODO
        for key in self.data:
            if len(self.data[key]) != size:
                self.data[key] = resize_array(self.data[key], size)
        return self<N><N>
import numpy as np
import moderngl<N><N>
from manimlib.constants import *
from manimlib.mobject.mobject import Mobject
from manimlib.utils.bezier import integer_interpolate
from manimlib.utils.bezier import interpolate
from manimlib.utils.images import get_full_raster_image_path
from manimlib.utils.iterables import listify
from manimlib.utils.space_ops import normalize_along_axis<N><N>
import itertools as it
import operator as op
import moderngl<N><N>
from functools import reduce, wraps<N><N>
import numpy as np<N><N>
from manimlib.animation.animation import Animation
from manimlib.constants import *
from manimlib.mobject.svg.tex_mobject import Tex
from manimlib.scene.scene import Scene<N><N>
from manimlib.constants import *
from manimlib.mobject.numbers import Integer
from manimlib.mobject.svg.tex_mobject import Tex
from manimlib.mobject.types.vectorized_mobject import VMobject, VGroup
from manimlib.scene.scene import Scene
from manimlib.utils.simple_functions import choose<N><N>

DEFAULT_COUNT_NUM_OFFSET = (FRAME_X_RADIUS - 1, FRAME_Y_RADIUS - 1, 0)
DEFAULT_COUNT_RUN_TIME = 5.0<N><N>

class CountingScene(Scene):
    def count(self, items, item_type="mobject", *args, **kwargs):
        if item_type == "mobject":
            self.count_mobjects(items, *args, **kwargs)
        elif item_type == "region":
            self.count_regions(items, *args, **kwargs)
        else:
            raise Exception("Unknown item_type, should be mobject or region")
        return self<N><N>
    def count_mobjects(
        self, mobjects, mode="highlight",
        color="red",
        display_numbers=True,
        num_offset=DEFAULT_COUNT_NUM_OFFSET,
        run_time=DEFAULT_COUNT_RUN_TIME,
    ):
        """
        Note, leaves final number mobject as "number" attribute<N><N>
from manimlib.animation.animation import Animation
from manimlib.animation.movement import ComplexHomotopy
from manimlib.animation.transform import MoveToTarget
from manimlib.constants import *
from manimlib.mobject.coordinate_systems import ComplexPlane
from manimlib.mobject.types.vectorized_mobject import VGroup
from manimlib.scene.scene import Scene<N><N>
from functools import reduce<N><N>
import itertools as it<N><N>
from functools import reduce
import itertools as it
import operator as op<N><N>
import numpy as np<N><N>
from manimlib.constants import *
from manimlib.scene.scene import Scene
from manimlib.utils.rate_functions import there_and_back
from manimlib.utils.space_ops import center_of_mass<N><N>

class Graph():
    def __init__(self):
        # List of points in R^3
        # vertices = []
        # List of pairs of indices of vertices
        # edges = []
        # List of tuples of indices of vertices.  The last should
        # be a cycle whose interior is the entire graph, and when
        # regions are computed its complement will be taken.
        # region_cycles = []<N><N>
        self.construct()<N><N>
    def construct(self):
        pass<N><N>
    def __str__(self):
        return self.__class__.__name__<N><N>

class CubeGraph(Graph):
    """
     5  7
      12
      03
     4  6
    """<N><N>
from traceback import *<N><N>
from scipy.spatial import ConvexHull<N><N>
import numpy as np<N><N>
from manimlib.animation.transform import Transform
from manimlib.constants import *
from manimlib.mobject.mobject import Mobject
from manimlib.scene.scene import Scene<N><N>

class ReconfigurableScene(Scene):
    """
    Note, this seems to no longer work as intented.
    """
    CONFIG = {
        "allow_recursion": True,
    }<N><N>
    def setup(self):
        self.states = []
        self.num_recursions = 0<N><N>
from copy import deepcopy
import itertools as it<N><N>
from manimlib.constants import *
from manimlib.mobject.mobject import Mobject
from manimlib.utils.iterables import adjacent_pairs<N><N>
# Warning: This is all now pretty depricated, and should not be expected to work<N><N>

class Region(Mobject):
    CONFIG = {
        "display_mode": "region"
    }<N><N>
    def __init__(self, condition=(lambda x, y: True), **kwargs):
        """
        Condition must be a function which takes in two real
        arrays (representing x and y values of space respectively)
        and return a boolean array.  This can essentially look like
        a function from R^2 to {True, False}, but & and | must be
        used in place of "and" and "or"
        """
        Mobject.__init__(self, **kwargs)
        self.condition = condition<N><N>
    def _combine(self, region, op):
        self.condition = lambda x, y: op(
            self.condition(x, y),
            region.condition(x, y)
        )<N><N>
    def union(self, region):
        self._combine(region, lambda bg1, bg2: bg1 | bg2)
        return self<N><N>
    def intersect(self, region):
        self._combine(region, lambda bg1, bg2: bg1 & bg2)
        return self<N><N>
    def complement(self):
        self.bool_grid = ~self.bool_grid
        return self<N><N>

class HalfPlane(Region):
    def __init__(self, point_pair, upper_left=True, *args, **kwargs):
        """
        point_pair of the form [(x_0, y_0,...), (x_1, y_1,...)]<N><N>
        Pf upper_left is True, the side of the region will be
        everything on the upper left side of the line through
        the point pair
        """
        if not upper_left:
            point_pair = list(point_pair)
            point_pair.reverse()
        (x0, y0), (x1, y1) = point_pair[0][:2], point_pair[1][:2]<N><N>
        def condition(x, y):
            return (x1 - x0) * (y - y0) > (y1 - y0) * (x - x0)
        Region.__init__(self, condition, *args, **kwargs)<N><N>

def region_from_line_boundary(*lines, **kwargs):
    reg = Region(**kwargs)
    for line in lines:
        reg.intersect(HalfPlane(line, **kwargs))
    return reg<N><N>

def region_from_polygon_vertices(*vertices, **kwargs):
    return region_from_line_boundary(*adjacent_pairs(vertices), **kwargs)<N><N>

def plane_partition(*lines, **kwargs):
    """
    A 'line' is a pair of points [(x0, y0,...), (x1, y1,...)]<N><N>
from manimlib.animation.animation import Animation
from manimlib.animation.transform import MoveToTarget
from manimlib.animation.transform import Transform
from manimlib.animation.update import UpdateFromFunc
from manimlib.constants import *
from manimlib.scene.scene import Scene
from manimlib.mobject.probability import SampleSpace
from manimlib.mobject.types.vectorized_mobject import VGroup<N><N>

class SampleSpaceScene(Scene):
    def get_sample_space(self, **config):
        self.sample_space = SampleSpace(**config)
        return self.sample_space<N><N>
    def add_sample_space(self, **config):
        self.add(self.get_sample_space(**config))<N><N>
    def get_division_change_animations(
        self, sample_space, parts, p_list,
        dimension=1,
        new_label_kwargs=None,
        **kwargs
    ):
        if new_label_kwargs is None:
            new_label_kwargs = {}
        anims = []
        p_list = sample_space.complete_p_list(p_list)
        space_copy = sample_space.copy()<N><N>
import inspect
import random
import platform
import itertools as it
import logging
from functools import wraps<N><N>
from tqdm import tqdm as ProgressDisplay
import numpy as np
import time<N><N>
import numpy as np
from pydub import AudioSegment
import shutil
import subprocess as sp
import os
import sys
import platform<N><N>
from manimlib.constants import FFMPEG_BIN
from manimlib.utils.config_ops import digest_config
from manimlib.utils.file_ops import guarantee_existence
from manimlib.utils.file_ops import add_extension_if_not_present
from manimlib.utils.file_ops import get_sorted_integer_files
from manimlib.utils.sounds import get_full_sound_file_path<N><N>
from manimlib.scene.scene import Scene<N><N>

class ThreeDScene(Scene):
    CONFIG = {
        "camera_config": {
            "samples": 4,
        }
    }<N><N>
    def begin_ambient_camera_rotation(self, rate=0.02):
        pass  # TODO<N><N>
    def stop_ambient_camera_rotation(self):
        pass  # TODO<N><N>
    def move_camera(self,
                    phi=None,
                    theta=None,
                    distance=None,
                    gamma=None,
                    frame_center=None,
                    **kwargs):
        pass  # TODO
<N><N>
import numpy as np<N><N>
from scipy import linalg
import numpy as np<N><N>
from manimlib.utils.simple_functions import choose
from manimlib.utils.space_ops import find_intersection
from manimlib.utils.space_ops import cross2d<N><N>
CLOSED_THRESHOLD = 0.001<N><N>

def bezier(points):
    n = len(points) - 1<N><N>
    def result(t):
        return sum([
            ((1 - t)**(n - k)) * (t**k) * choose(n, k) * point
            for k, point in enumerate(points)
        ])<N><N>
    return result<N><N>

def partial_bezier_points(points, a, b):
    """
    Given an list of points which define
    a bezier curve, and two numbers 0<=a<b<=1,
    return an list of the same size, which
    describes the portion of the original bezier
    curve on the interval [a, b].<N><N>
    This algorithm is pretty nifty, and pretty dense.
    """
    if a == 1:
        return [points[-1]] * len(points)<N><N>
    a_to_1 = [
        bezier(points[i:])(a)
        for i in range(len(points))
    ]
    end_prop = (b - a) / (1. - a)
    return [
        bezier(a_to_1[:i + 1])(end_prop)
        for i in range(len(points))
    ]<N><N>

# Shortened version of partial_bezier_points just for quadratics,
# since this is called a fair amount
def partial_quadratic_bezier_points(points, a, b):
    if a == 1:
        return 3 * [points[-1]]<N><N>
    def curve(t):
        return points[0] * (1 - t) * (1 - t) + 2 * points[1] * t * (1 - t) + points[2] * t * t
    # bezier(points)
    h0 = curve(a) if a > 0 else points[0]
    h2 = curve(b) if b < 1 else points[2]
    h1_prime = (1 - a) * points[1] + a * points[2]
    end_prop = (b - a) / (1. - a)
    h1 = (1 - end_prop) * h0 + end_prop * h1_prime
    return [h0, h1, h2]<N><N>

# Linear interpolation variants<N><N>
def interpolate(start, end, alpha):
    try:
        return (1 - alpha) * start + alpha * end
    except TypeError:
        print(type(start), start.dtype)
        print(type(end), start.dtype)
        print(alpha)
        import sys
        sys.exit(2)<N><N>

def set_array_by_interpolation(arr, arr1, arr2, alpha, interp_func=interpolate):
    arr[:] = interp_func(arr1, arr2, alpha)
    return arr<N><N>

def integer_interpolate(start, end, alpha):
    """
    alpha is a float between 0 and 1.  This returns
    an integer between start and end (inclusive) representing
    appropriate interpolation between them, along with a
    "residue" representing a new proportion between the
    returned integer and the next one of the
    list.<N><N>
    For example, if start=0, end=10, alpha=0.46, This
    would return (4, 0.6).
    """
    if alpha >= 1:
        return (end - 1, 1.0)
    if alpha <= 0:
        return (start, 0)
    value = int(interpolate(start, end, alpha))
    residue = ((end - start) * alpha) % 1
    return (value, residue)<N><N>

def mid(start, end):
    return (start + end) / 2.0<N><N>

def inverse_interpolate(start, end, value):
    return np.true_divide(value - start, end - start)<N><N>

def match_interpolate(new_start, new_end, old_start, old_end, old_value):
    return interpolate(
        new_start, new_end,
        inverse_interpolate(old_start, old_end, old_value)
    )<N><N>

def get_smooth_quadratic_bezier_handle_points(points):
    """
    Figuring out which bezier curves most smoothly connect a sequence of points.<N><N>
    Given three successive points, P0, P1 and P2, you can compute that by defining
    h = (1/4) P0 + P1 - (1/4)P2, the bezier curve defined by (P0, h, P1) will pass
    through the point P2.<N><N>
import random<N><N>
from colour import Color
import numpy as np<N><N>
from manimlib.constants import WHITE
from manimlib.constants import COLORMAP_3B1B
from manimlib.utils.bezier import interpolate
from manimlib.utils.iterables import resize_with_interpolation
from manimlib.utils.simple_functions import clip_in_place
from manimlib.utils.space_ops import normalize<N><N>

def color_to_rgb(color):
    if isinstance(color, str):
        return hex_to_rgb(color)
    elif isinstance(color, Color):
        return np.array(color.get_rgb())
    else:
        raise Exception("Invalid color type")<N><N>

def color_to_rgba(color, alpha=1):
    return np.array([*color_to_rgb(color), alpha])<N><N>

def rgb_to_color(rgb):
    try:
        return Color(rgb=rgb)
    except ValueError:
        return Color(WHITE)<N><N>

def rgba_to_color(rgba):
    return rgb_to_color(rgba[:3])<N><N>

def rgb_to_hex(rgb):
    return "#" + "".join(
        hex(int_x // 16)[2] + hex(int_x % 16)[2]
        for x in rgb
        for int_x in [int(255 * x)]
    )<N><N>

def hex_to_rgb(hex_code):
    hex_part = hex_code[1:]
    if len(hex_part) == 3:
        hex_part = "".join([2 * c for c in hex_part])
    return np.array([
        int(hex_part[i:i + 2], 16) / 255
        for i in range(0, 6, 2)
    ])<N><N>

def invert_color(color):
    return rgb_to_color(1.0 - color_to_rgb(color))<N><N>

def color_to_int_rgb(color):
    return (255 * color_to_rgb(color)).astype('uint8')<N><N>

def color_to_int_rgba(color, opacity=1.0):
    alpha = int(255 * opacity)
    return np.array([*color_to_int_rgb(color), alpha])<N><N>

def color_gradient(reference_colors, length_of_output):
    if length_of_output == 0:
        return reference_colors[0]
    rgbs = list(map(color_to_rgb, reference_colors))
    alphas = np.linspace(0, (len(rgbs) - 1), length_of_output)
    floors = alphas.astype('int')
    alphas_mod1 = alphas % 1
    # End edge case
    alphas_mod1[-1] = 1
    floors[-1] = len(rgbs) - 2
    return [
        rgb_to_color(interpolate(rgbs[i], rgbs[i + 1], alpha))
        for i, alpha in zip(floors, alphas_mod1)
    ]<N><N>

def interpolate_color(color1, color2, alpha):
    rgb = interpolate(color_to_rgb(color1), color_to_rgb(color2), alpha)
    return rgb_to_color(rgb)<N><N>

def average_color(*colors):
    rgbs = np.array(list(map(color_to_rgb, colors)))
    return rgb_to_color(rgbs.mean(0))<N><N>

def random_bright_color():
    color = random_color()
    curr_rgb = color_to_rgb(color)
    new_rgb = interpolate(
        curr_rgb, np.ones(len(curr_rgb)), 0.5
    )
    return Color(rgb=new_rgb)<N><N>

def random_color():
    return Color(rgb=(random.random() for i in range(3)))<N><N>

def get_shaded_rgb(rgb, point, unit_normal_vect, light_source):
    to_sun = normalize(light_source - point)
    factor = 0.5 * np.dot(unit_normal_vect, to_sun)**3
    if factor < 0:
        factor *= 0.5
    result = rgb + factor
    clip_in_place(rgb + factor, 0, 1)
    return result<N><N>

def get_colormap_list(map_name="viridis", n_colors=9):
    """
    Options for map_name:
    3b1b_colormap
    magma
    inferno
    plasma
    viridis
    cividis
    twilight
    twilight_shifted
    turbo
    """
    from matplotlib.cm import get_cmap<N><N>
    if map_name == "3b1b_colormap":
        rgbs = [color_to_rgb(color) for color in COLORMAP_3B1B]
    else:
        rgbs = get_cmap(map_name).colors  # Make more general?
    return resize_with_interpolation(np.array(rgbs), n_colors)
<N><N>
import inspect
import itertools as it<N><N>

def get_all_descendent_classes(Class):
    awaiting_review = [Class]
    result = []
    while awaiting_review:
        Child = awaiting_review.pop()
        awaiting_review += Child.__subclasses__()
        result.append(Child)
    return result<N><N>

def filtered_locals(caller_locals):
    result = caller_locals.copy()
    ignored_local_args = ["self", "kwargs"]
    for arg in ignored_local_args:
        result.pop(arg, caller_locals)
    return result<N><N>

def digest_config(obj, kwargs, caller_locals={}):
    """
    Sets init args and CONFIG values as local variables<N><N>
    The purpose of this function is to ensure that all
    configuration of any object is inheritable, able to
    be easily passed into instantiation, and is attached
    as an attribute of the object.
    """<N><N>
    # Assemble list of CONFIGs from all super classes
    classes_in_hierarchy = [obj.__class__]
    static_configs = []
    while len(classes_in_hierarchy) > 0:
        Class = classes_in_hierarchy.pop()
        classes_in_hierarchy += Class.__bases__
        if hasattr(Class, "CONFIG"):
            static_configs.append(Class.CONFIG)<N><N>
    # Order matters a lot here, first dicts have higher priority
    caller_locals = filtered_locals(caller_locals)
    all_dicts = [kwargs, caller_locals, obj.__dict__]
    all_dicts += static_configs
    obj.__dict__ = merge_dicts_recursively(*reversed(all_dicts))<N><N>

def merge_dicts_recursively(*dicts):
    """
    Creates a dict whose keyset is the union of all the
    input dictionaries.  The value for each key is based
    on the first dict in the list with that key.<N><N>
    dicts later in the list have higher priority<N><N>
    When values are dictionaries, it is applied recursively
    """
    result = dict()
    all_items = it.chain(*[d.items() for d in dicts])
    for key, value in all_items:
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = merge_dicts_recursively(result[key], value)
        else:
            result[key] = value
    return result<N><N>

def soft_dict_update(d1, d2):
    """
    Adds key values pairs of d2 to d1 only when d1 doesn't
    already have that key
    """
    for key, value in list(d2.items()):
        if key not in d1:
            d1[key] = value<N><N>

def digest_locals(obj, keys=None):
    caller_locals = filtered_locals(
        inspect.currentframe().f_back.f_locals
    )
    if keys is None:
        keys = list(caller_locals.keys())
    for key in keys:
        setattr(obj, key, caller_locals[key])<N><N>
# Occasionally convenient in order to write dict.x instead of more laborious
# (and less in keeping with all other attr accesses) dict["x"]<N><N>

class DictAsObject(object):
    def __init__(self, dict):
        self.__dict__ = dict
<N><N>
import os
import tempfile<N><N>
from manimlib.config import get_custom_config
from manimlib.config import get_manim_dir<N><N>
CUSTOMIZATION = {}<N><N>

def get_customization():
    if not CUSTOMIZATION:
        CUSTOMIZATION.update(get_custom_config())
        directories = CUSTOMIZATION["directories"]
        # Unless user has specified otherwise, use the system default temp
        # directory for storing tex files, mobject_data, etc.
        if not directories["temporary_storage"]:
            directories["temporary_storage"] = tempfile.gettempdir()<N><N>
        # Assumes all shaders are written into manimlib/shaders
        directories["shaders"] = os.path.join(
            get_manim_dir(), "manimlib", "shaders"
        )
    return CUSTOMIZATION
<N><N>
import time<N><N>
from manimlib.constants import BLACK
from manimlib.mobject.numbers import Integer
from manimlib.mobject.types.vectorized_mobject import VGroup<N><N>

def print_family(mobject, n_tabs=0):
    """For debugging purposes"""
    print("\t" * n_tabs, mobject, id(mobject))
    for submob in mobject.submobjects:
        print_family(submob, n_tabs + 1)<N><N>

def index_labels(mobject, label_height=0.15):
    labels = VGroup()
    for n, submob in enumerate(mobject):
        label = Integer(n)
        label.set_height(label_height)
        label.move_to(submob)
        label.set_stroke(BLACK, 5, background=True)
        labels.add(label)
    return labels<N><N>

def get_runtime(func):
    now = time.time()
    func()
    return time.time() - now
<N><N>
import os<N><N>
from manimlib.utils.file_ops import guarantee_existence
from manimlib.utils.customization import get_customization<N><N>

def get_directories():
    return get_customization()["directories"]<N><N>

def get_temp_dir():
    return get_directories()["temporary_storage"]<N><N>

def get_tex_dir():
    return guarantee_existence(os.path.join(get_temp_dir(), "Tex"))<N><N>

def get_text_dir():
    return guarantee_existence(os.path.join(get_temp_dir(), "Text"))<N><N>

def get_mobject_data_dir():
    return guarantee_existence(os.path.join(get_temp_dir(), "mobject_data"))<N><N>

def get_downloads_dir():
    return guarantee_existence(os.path.join(get_temp_dir(), "manim_downloads"))<N><N>

def get_output_dir():
    return guarantee_existence(get_directories()["output"])<N><N>

def get_raster_image_dir():
    return get_directories()["raster_images"]<N><N>

def get_vector_image_dir():
    return get_directories()["vector_images"]<N><N>

def get_sound_dir():
    return get_directories()["sounds"]<N><N>

def get_shader_dir():
    return get_directories()["shaders"]
<N><N>
import itertools as it<N><N>

def extract_mobject_family_members(mobject_list, only_those_with_points=False):
    result = list(it.chain(*[
        mob.get_family()
        for mob in mobject_list
    ]))
    if only_those_with_points:
        result = [mob for mob in result if mob.has_points()]
    return result<N><N>

def restructure_list_to_exclude_certain_family_members(mobject_list, to_remove):
    """
    Removes anything in to_remove from mobject_list, but in the event that one of
    the items to be removed is a member of the family of an item in mobject_list,
    the other family members are added back into the list.<N><N>
    This is useful in cases where a scene contains a group, e.g. Group(m1, m2, m3),
    but one of its submobjects is removed, e.g. scene.remove(m1), it's useful
    for the list of mobject_list to be edited to contain other submobjects, but not m1.
    """
    new_list = []
    to_remove = extract_mobject_family_members(to_remove)<N><N>
    def add_safe_mobjects_from_list(list_to_examine, set_to_remove):
        for mob in list_to_examine:
            if mob in set_to_remove:
                continue
            intersect = set_to_remove.intersection(mob.get_family())
            if intersect:
                add_safe_mobjects_from_list(mob.submobjects, intersect)
            else:
                new_list.append(mob)
    add_safe_mobjects_from_list(mobject_list, set(to_remove))
    return new_list
<N><N>
import os
import numpy as np
import validators<N><N>

def add_extension_if_not_present(file_name, extension):
    # This could conceivably be smarter about handling existing differing extensions
    if(file_name[-len(extension):] != extension):
        return file_name + extension
    else:
        return file_name<N><N>

def guarantee_existence(path):
    if not os.path.exists(path):
        os.makedirs(path)
    return os.path.abspath(path)<N><N>

def find_file(file_name, directories=None, extensions=None):
    # Check if this is a file online first, and if so, download
    # it to a temporary directory
    if validators.url(file_name):
        import urllib.request
        from manimlib.utils.directories import get_downloads_dir
        stem, name = os.path.split(file_name)
        folder = get_downloads_dir()
        path = os.path.join(folder, name)
        urllib.request.urlretrieve(file_name, path)
        return path<N><N>
    # Check if what was passed in is already a valid path to a file
    if os.path.exists(file_name):
        return file_name<N><N>
    # Otherwise look in local file system
    directories = directories or [""]
    extensions = extensions or [""]
    possible_paths = (
        os.path.join(directory, file_name + extension)
        for directory in directories
        for extension in extensions
    )
    for path in possible_paths:
        if os.path.exists(path):
            return path
    raise IOError(f"{file_name} not Found")<N><N>

def get_sorted_integer_files(directory,
                             min_index=0,
                             max_index=np.inf,
                             remove_non_integer_files=False,
                             remove_indices_greater_than=None,
                             extension=None,
                             ):
    indexed_files = []
    for file in os.listdir(directory):
        if '.' in file:
            index_str = file[:file.index('.')]
        else:
            index_str = file<N><N>
import numpy as np
from PIL import Image<N><N>
from manimlib.utils.file_ops import find_file
from manimlib.utils.directories import get_raster_image_dir
from manimlib.utils.directories import get_vector_image_dir<N><N>

def get_full_raster_image_path(image_file_name):
    return find_file(
        image_file_name,
        directories=[get_raster_image_dir()],
        extensions=[".jpg", ".jpeg", ".png", ".gif", ""]
    )<N><N>

def get_full_vector_image_path(image_file_name):
    return find_file(
        image_file_name,
        directories=[get_vector_image_dir()],
        extensions=[".svg", ".xdv", ""],
    )<N><N>

def drag_pixels(frames):
    curr = frames[0]
    new_frames = []
    for frame in frames:
        curr += (curr == 0) * np.array(frame)
        new_frames.append(np.array(curr))
    return new_frames<N><N>

def invert_image(image):
    arr = np.array(image)
    arr = (255 * np.ones(arr.shape)).astype(arr.dtype) - arr
    return Image.fromarray(arr)
<N><N>
import yaml
import os<N><N>
import itertools as it
import numpy as np<N><N>

def remove_list_redundancies(l):
    """
    Used instead of list(set(l)) to maintain order
    Keeps the last occurance of each element
    """
    reversed_result = []
    used = set()
    for x in reversed(l):
        if x not in used:
            reversed_result.append(x)
            used.add(x)
    reversed_result.reverse()
    return reversed_result<N><N>

def list_update(l1, l2):
    """
    Used instead of list(set(l1).update(l2)) to maintain order,
    making sure duplicates are removed from l1, not l2.
    """
    return [e for e in l1 if e not in l2] + list(l2)<N><N>

def list_difference_update(l1, l2):
    return [e for e in l1 if e not in l2]<N><N>

def all_elements_are_instances(iterable, Class):
    return all([isinstance(e, Class) for e in iterable])<N><N>

def adjacent_n_tuples(objects, n):
    return zip(*[
        [*objects[k:], *objects[:k]]
        for k in range(n)
    ])<N><N>

def adjacent_pairs(objects):
    return adjacent_n_tuples(objects, 2)<N><N>
import numpy as np
import math<N><N>
from manimlib.constants import OUT
from manimlib.utils.bezier import interpolate
from manimlib.utils.space_ops import get_norm
from manimlib.utils.space_ops import rotation_matrix_transpose<N><N>
STRAIGHT_PATH_THRESHOLD = 0.01<N><N>

def straight_path(start_points, end_points, alpha):
    """
    Same function as interpolate, but renamed to reflect
    intent of being used to determine how a set of points move
    to another set.  For instance, it should be a specific case
    of path_along_arc
    """
    return interpolate(start_points, end_points, alpha)<N><N>

def path_along_arc(arc_angle, axis=OUT):
    """
    If vect is vector from start to end, [vect[:,1], -vect[:,0]] is
    perpendicular to vect in the left direction.
    """
    if abs(arc_angle) < STRAIGHT_PATH_THRESHOLD:
        return straight_path
    if get_norm(axis) == 0:
        axis = OUT
    unit_axis = axis / get_norm(axis)<N><N>
    def path(start_points, end_points, alpha):
        vects = end_points - start_points
        centers = start_points + 0.5 * vects
        if arc_angle != np.pi:
            centers += np.cross(unit_axis, vects / 2.0) / math.tan(arc_angle / 2)
        rot_matrix_T = rotation_matrix_transpose(alpha * arc_angle, unit_axis)
        return centers + np.dot(start_points - centers, rot_matrix_T)<N><N>
    return path<N><N>

def clockwise_path():
    return path_along_arc(-np.pi)<N><N>

def counterclockwise_path():
    return path_along_arc(np.pi)
<N><N>
import numpy as np<N><N>
from manimlib.utils.bezier import bezier<N><N>

def linear(t):
    return t<N><N>

def smooth(t):
    # Zero first and second derivatives at t=0 and t=1.
    # Equivalent to bezier([0, 0, 0, 1, 1, 1])
    s = 1 - t
    return (t**3) * (10 * s * s + 5 * s * t + t * t)<N><N>

def rush_into(t):
    return 2 * smooth(0.5 * t)<N><N>

def rush_from(t):
    return 2 * smooth(0.5 * (t + 1)) - 1<N><N>

def slow_into(t):
    return np.sqrt(1 - (1 - t) * (1 - t))<N><N>

def double_smooth(t):
    if t < 0.5:
        return 0.5 * smooth(2 * t)
    else:
        return 0.5 * (1 + smooth(2 * t - 1))<N><N>

def there_and_back(t):
    new_t = 2 * t if t < 0.5 else 2 * (1 - t)
    return smooth(new_t)<N><N>

def there_and_back_with_pause(t, pause_ratio=1. / 3):
    a = 1. / pause_ratio
    if t < 0.5 - pause_ratio / 2:
        return smooth(a * t)
    elif t < 0.5 + pause_ratio / 2:
        return 1
    else:
        return smooth(a - a * t)<N><N>

def running_start(t, pull_factor=-0.5):
    return bezier([0, 0, pull_factor, pull_factor, 1, 1, 1])(t)<N><N>

def not_quite_there(func=smooth, proportion=0.7):
    def result(t):
        return proportion * func(t)
    return result<N><N>

def wiggle(t, wiggles=2):
    return there_and_back(t) * np.sin(wiggles * np.pi * t)<N><N>

def squish_rate_func(func, a=0.4, b=0.6):
    def result(t):
        if a == b:
            return a
        elif t < a:
            return func(0)
        elif t > b:
            return func(1)
        else:
            return func((t - a) / (b - a))<N><N>
    return result<N><N>
# Stylistically, should this take parameters (with default values)?
# Ultimately, the functionality is entirely subsumed by squish_rate_func,
# but it may be useful to have a nice name for with nice default params for
# "lingering", different from squish_rate_func's default params<N><N>

def lingering(t):
    return squish_rate_func(lambda t: t, 0, 0.8)(t)<N><N>

def exponential_decay(t, half_life=0.1):
    # The half-life should be rather small to minimize
    # the cut-off error at the end
    return 1 - np.exp(-t / half_life)
<N><N>
from functools import reduce
import inspect
import numpy as np
import operator as op<N><N>

def sigmoid(x):
    return 1.0 / (1 + np.exp(-x))<N><N>

CHOOSE_CACHE = {}<N><N>

def choose_using_cache(n, r):
    if n not in CHOOSE_CACHE:
        CHOOSE_CACHE[n] = {}
    if r not in CHOOSE_CACHE[n]:
        CHOOSE_CACHE[n][r] = choose(n, r, use_cache=False)
    return CHOOSE_CACHE[n][r]<N><N>

def choose(n, r, use_cache=True):
    if use_cache:
        return choose_using_cache(n, r)
    if n < r:
        return 0
    if r == 0:
        return 1
    denom = reduce(op.mul, range(1, r + 1), 1)
    numer = reduce(op.mul, range(n, n - r, -1), 1)
    return numer // denom<N><N>

def get_num_args(function):
    return len(get_parameters(function))<N><N>

def get_parameters(function):
    return inspect.signature(function).parameters<N><N>
# Just to have a less heavyweight name for this extremely common operation
#
# We may wish to have more fine-grained control over division by zero behavior
# in the future (separate specifiable values for 0/0 and x/0 with x != 0),
# but for now, we just allow the option to handle indeterminate 0/0.<N><N>

def clip(a, min_a, max_a):
    if a < min_a:
        return min_a
    elif a > max_a:
        return max_a
    return a<N><N>

def clip_in_place(array, min_val=None, max_val=None):
    if max_val is not None:
        array[array > max_val] = max_val
    if min_val is not None:
        array[array < min_val] = min_val
    return array<N><N>

def fdiv(a, b, zero_over_zero_value=None):
    if zero_over_zero_value is not None:
        out = np.full_like(a, zero_over_zero_value)
        where = np.logical_or(a != 0, b != 0)
    else:
        out = None
        where = True<N><N>
    return np.true_divide(a, b, out=out, where=where)<N><N>

def binary_search(function,
                  target,
                  lower_bound,
                  upper_bound,
                  tolerance=1e-4):
    lh = lower_bound
    rh = upper_bound
    while abs(rh - lh) > tolerance:
        mh = np.mean([lh, rh])
        lx, mx, rx = [function(h) for h in (lh, mh, rh)]
        if lx == target:
            return lx
        if rx == target:
            return rx<N><N>
        if lx <= target and rx >= target:
            if mx > target:
                rh = mh
            else:
                lh = mh
        elif lx > target and rx < target:
            lh, rh = rh, lh
        else:
            return None
    return mh
<N><N>
from manimlib.utils.file_ops import find_file
from manimlib.utils.directories import get_sound_dir<N><N>
def get_full_sound_file_path(sound_file_name):
    return find_file(
        sound_file_name,
        directories=[get_sound_dir()],
        extensions=[".wav", ".mp3"]
    )

import numpy as np
import itertools as it
import math
from mapbox_earcut import triangulate_float32 as earcut<N><N>
from manimlib.constants import RIGHT
from manimlib.constants import DOWN
from manimlib.constants import OUT
from manimlib.constants import PI
from manimlib.constants import TAU
from manimlib.utils.iterables import adjacent_pairs<N><N>

def get_norm(vect):
    return sum([x**2 for x in vect])**0.5<N><N>

# Quaternions
# TODO, implement quaternion type<N><N>

def quaternion_mult(*quats):
    if len(quats) == 0:
        return [1, 0, 0, 0]
    result = quats[0]
    for next_quat in quats[1:]:
        w1, x1, y1, z1 = result
        w2, x2, y2, z2 = next_quat
        result = [
            w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,
            w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,
            w1 * y2 + y1 * w2 + z1 * x2 - x1 * z2,
            w1 * z2 + z1 * w2 + x1 * y2 - y1 * x2,
        ]
    return result<N><N>

def quaternion_from_angle_axis(angle, axis, axis_normalized=False):
    if not axis_normalized:
        axis = normalize(axis)
    return [math.cos(angle / 2), *(math.sin(angle / 2) * axis)]<N><N>

def angle_axis_from_quaternion(quaternion):
    axis = normalize(
        quaternion[1:],
        fall_back=[1, 0, 0]
    )
    angle = 2 * np.arccos(quaternion[0])
    if angle > TAU / 2:
        angle = TAU - angle
    return angle, axis<N><N>

def quaternion_conjugate(quaternion):
    result = list(quaternion)
    for i in range(1, len(result)):
        result[i] *= -1
    return result<N><N>
import re
import string<N><N>

def to_camel_case(name):
    return "".join([
        [c for c in part if c not in string.punctuation + string.whitespace].capitalize()
        for part in name.split("_")
    ])<N><N>

def initials(name, sep_values=[" ", "_"]):
    return "".join([
        (s[0] if s else "")
        for s in re.split("|".join(sep_values), name)
    ])<N><N>

def camel_case_initials(name):
    return [c for c in name if c.isupper()]<N><N>

def complex_string(complex_num):
    return [c for c in str(complex_num) if c not in "()"]<N><N>

def split_string_to_isolate_substrings(full_string, *isolate):
    """
    Given a string, and an arbitrary number of possible substrings,
    to isolate, this returns a list of strings which would concatenate
    to make the full string, and in which these special substrings
    appear as their own elements.<N><N>
    For example,split_string_to_isolate_substrings("to be or not to be", "to", "be")
    would return ["to", " ", "be", " or not ", "to", " ", "be"]
    """
    pattern = "|".join(*(
        "({})".format(re.escape(ss))
        for ss in isolate
    ))
    pieces = re.split(pattern, full_string)
    return list(filter(lambda s: s, pieces))
<N><N>
import logging
import sys
import os
import hashlib
from contextlib import contextmanager<N><N>
from manimlib.utils.directories import get_tex_dir
from manimlib.config import get_manim_dir
from manimlib.config import get_custom_config<N><N>

SAVED_TEX_CONFIG = {}<N><N>
import cv2
import mediapipe as mp
import time<N><N>
mp_objectron = mp.solutions.objectron
mp_drawing = mp.solutions.drawing_utils<N><N>

cap = cv2.VideoCapture(0)<N><N>
with mp_objectron.Objectron(static_image_mode=False,max_num_objects=2,min_detection_confidence=0.5,min_tracking_confidence=0.8,model_name='Cup') as objectron:<N><N>
 while cap.isOpened():<N><N>
      success,image = cap.read()<N><N>
      start = time.time()<N><N>
      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<N><N>

      image.flags.writeable = False
      results = objectron.process(image)<N><N>
      image.flags.writable = True<N><N>
      if results.detected_objects:<N><N>
          for detected_objects in results.detected_objects:
              mp_drawing.draw_landmarks(image, detected_object.landmarks_2d, mp_objectron.BOX_CONNECTIONS)
              mp_drawing.draw_axis(image, detected_object.rotation, detected_object.translation)<N><N>
      end = time.time()
      totalTime = end-start
      try:
          fps=1/totalTime
      except ZeroDivisionError:
          print("FPS",fps)<N><N>
      cv2.putText(image,f'FPS:{int(fps)}',(20,70),cv2.FONT_HERSHEY_SIMPLEX,1.5,(255,0,200),2)<N><N>
      cv2.imshow('MediaPipe Objectron',image)<N><N>
      if cv2.waitKey(5) & 0xFF==27:
          break<N><N>
cap.release()<N><N>
import cv2
import mediapipe as mp
import time<N><N>
mp_facedetector = mp.solutions.face_detection
mp_draw = mp.solutions.drawing_utils<N><N>
cap = cv2.VideoCapture(0)<N><N>
with mp_facedetector.FaceDetection(min_detection_confidence=0.7) as face_detection:<N><N>
    while cap.isOpened():<N><N>
        success, image = cap.read()<N><N>
        start = time.time()<N><N>

        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<N><N>

        results = face_detection.process(image)<N><N>

        image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)<N><N>

        if results.detections:
            for id, detection in enumerate(results.detections):
                mp_draw.draw_detection(image, detection)
                print(id, detection)<N><N>
                bBox = detection.location_data.relative_bounding_box<N><N>
                h, w, c = image.shape<N><N>
                boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)<N><N>
                cv2.putText(image, f'{int(detection.score[0]*100)}%',(boundBox[0],boundBox[1] - 20),cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,255,0),2)<N><N>

        end = time.time()
        totalTime = end - start<N><N>
        try:
           fps = 1 / totalTime
           print("total time: ", totalTime)
        except ZeroDivisionError:
            print(fps)<N><N>
        cv2.putText(image, f'Fps:{int(fps)}',(20, 70),cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,255,0),2)
        cv2.imshow('Face Detection',image)<N><N>
        if cv2.waitKey(5) & 0xFF == 27:
            break<N><N>
cap.release()<N><N>
import numpy as np
import cv2 as cv<N><N>
img = cv.imread('./samples/messi5.jpg')
cv.imshow('image', img)
print('image shape: ',img.shape)
print('image size: ', img.size)
print('data type: ', img.dtype)<N><N>
# image roi
ball = img[280:340, 330:390]
img[273:333, 100:160] = ball
while(1):
    cv.imshow('image',img)
    k = cv.waitKey(1) & 0xFF
    if (k==27):
        break<N><N>
# splitting and merging image channels
b, g, r = cv.split(img)
img = cv.merge((b, g, r))<N><N>
while(1):
    cv.imshow('image',g)
    k = cv.waitKey(1) & 0xFF
    if (k==27):
        break<N><N>
img_border = cv.copyMakeBorder(img, 10,20,10,20,cv.BORDER_CONSTANT,value=[255, 0, 0])
while(1):
    cv.imshow('image',img_border)
    k = cv.waitKey(1) & 0xFF
    if (k==27):
        break<N><N>
import numpy as np
import cv2 as cv<N><N>
img1 = cv.imread('./samples/messi5.jpg')
img2 = cv.imread('./samples/opencv-logo-white.png')<N><N>
# create an roi to put the logo on the top left corner
rows, cols, channels = img2.shape
roi = img1[0:rows, 0:cols]<N><N>
# create a mask of the logo and the inverse of the mask as well
img2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)
ret, mask = cv.threshold(img2gray, 10, 255, cv.THRESH_BINARY)
mask_inv = cv.bitwise_not(mask)<N><N>
# black out the logo area in the image
img1_bg = cv.bitwise_and(roi, roi, mask = mask_inv)<N><N>
# take only the region of logo from logo image
img2_fg = cv.bitwise_and(img2, img2, mask=mask)<N><N>
# put logo in ROI and modify the main image
dst = cv.add(img1_bg, img2_fg)
img1[0:rows, 0:cols] = dst<N><N>
cv.imshow('res',img1)
cv.waitKey(0)
cv.destroyAllWindows()
<N><N>
import numpy as np
import cv2 as cv<N><N>img1 = cv.imread('./samples/ml.png')
img2 = cv.imread('./samples/opencv-logo.png')
img2 = img2[0:380,0:308]<N><N>print(img2.shape)<N><N>dst = cv.addWeighted(img1, 0.7, img2, 0.3, 0)<N><N>cv.imshow('dst',dst)
cv.waitKey(0)
cv.destroyAllWindows()
import numpy as np
import cv2 as cv<N><N>img = cv.imread('./Samples/messi5.jpg')
px = img[100, 100]
print(px)
blue = img[100, 100, 0]
print(blue)
img[100, 100] = [255, 255, 255]
print(img[100, 100])<N><N># better single pixel accesing method
print(img.item(10, 10, 2))<N><N>img.itemset((10,10,2),100)
print(img.item(10,10,2))
print(img [10, 10, 2])
while(1):
    cv.imshow('image',img)
    k = cv.waitKey(1) & 0xFF
    if k==27:
        break
import numpy as np
import cv2 as cv
from os import walk
import time
import fnmatch
import math<N><N>
MAX_IMAGE_SIZE = 600
def blend_images(img1, img2, w):
    width = img1.shape[0]
    height = img1.shape[1]<N><N>
    if height>MAX_IMAGE_SIZE:
        aspect_ratio = width/height
        height = MAX_IMAGE_SIZE
        width = round(MAX_IMAGE_SIZE*aspect_ratio)
        dim = (height, width)
        img1 = cv.resize(img1,dim)
    dim = (height, width)
    img2 = cv.resize(img2, dim)
    blend_image = cv.addWeighted(img1,1-w,img2,w,0)
    return blend_image<N><N>

mypath='./samples/'
f = []
print('------------------')
for (dirpath, dirnames, filenames) in walk(mypath):
    for name in filenames:
        if name.endswith('.png'):
            f.append(name)
print('------------------')
print(f)
print('++++++++++++++++')<N><N>
import numpy as np
import cv2 as cv<N><N>cap = cv.VideoCapture(0)<N><N>fourcc = cv.VideoWriter_fourcc(*'XVID')
out = cv.VideoWriter('output.avi',fourcc,20.0,(640,480))<N><N>while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("can't recieve frame")
        break<N><N>    frame = cv.flip(frame,0)<N><N>    out.write(frame)<N><N>    cv.imshow('frame',frame)
    if cv.waitKey(1) == ord("q"):
        break<N><N>cap.release()
out.release()
cv.destroyAllWindows()
import numpy as np
import cv2 as cv<N><N>cap=cv.VideoCapture(0)
if not cap.isOpened():
    print("cannot open camera")
    exit()
while True:
    ret, frame = cap.read()<N><N>    if not ret:
        print("Can't receive frame (stream end?. Exiting ...")
        break<N><N>    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cv.imshow('frame',gray)
    if cv.waitKey(1)==ord("q"):
        break<N><N>cap.realease()
cv.destroyAllWindows()
from typing import no_type_check<N><N>
import numpy as np
import cv2 as cv<N><N>def draw_circle(event,x,y,flags,param):
    if event == cv.EVENT_LBUTTONDOWN:
        print("clicked!")
        cv.circle(img, (x, y), 100, (255,0,0), 1)<N><N>img = np.zeros((512,512,3),np.uint8)
cv.namedWindow('image')
cv.setMouseCallback('image',draw_circle)<N><N>while(1):
    cv.imshow('image',img)
    if cv.waitKey(20) & 0xFF == 27:
        break<N><N>cv.destroyAllWindows()
import numpy as np
import cv2 as cv<N><N>
drawing = False # true if mouse is pressed
mode = True # if True, draw a rectangle. Press 'm' to toggle to curve
ix, iy = -1, -1<N><N>
def draw_circle(event,x,y,flags,param):
    global ix, iy, drawing, mode<N><N>
import numpy as np
import cv2 as cv<N><N>
def draw_image(img):
    while True:
        cv.imshow("image",img)
        if cv.waitKey(1) == ord("q"):
            break<N><N>
img = np.zeros((512,512,3),np.uint8)<N><N>
cv.line(img, (0,0), (511,511), (255,0,0), 5)
draw_image(img)<N><N>
cv.rectangle(img, (384,0), (510,128), (0,255,0),3)
draw_image(img)<N><N>
cv.circle(img, (447,63), 63, (0,0,255),1)
draw_image(img)<N><N>
cv.ellipse(img, (256,256), (100,50), 0, 0, 180, 255, -1)
draw_image(img)<N><N>
font = cv.FONT_HERSHEY_COMPLEX
cv.putText(img, "OPENCV", (10,500), font, 4, (255,255,255), 2, cv.LINE_AA)
draw_image(img)<N><N>
<N><N><N><N>
import numpy as np
import cv2 as cv<N><N>cap=cv.VideoCapture("C:\Personal\ComputerVision\image_data\horse.mp4")
if not cap.isOpened():
    print("cannot open camera")
    exit()
while True:
    ret, frame = cap.read()<N><N>    if not ret:
        print("Can't receive frame (stream end?. Exiting ...")
        break<N><N>    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cv.imshow('frame',gray)
    if cv.waitKey(1)==ord("q"):
        break<N><N>cap.realease()
cv.destroyAllWindows()
import cv2 as cv
import sys<N><N>img=cv.imread(cv.samples.findFile("starry_night.jpg"))<N><N>if img is None:
    sys.exit('Could not read the image.')<N><N>cv.imshow("Display windows",img)
k=cv.waitKey(0)<N><N>if k==ord("s"):
    cv.imwrite("starry_night.png",img)
import numpy as np
import cv2 as cv<N><N>
def nothing(x):
    pass<N><N>
img = np.zeros((300,512,3), np.uint8)
cv.namedWindow('image')
cv.createTrackbar('R','image',0,255,nothing)
cv.createTrackbar('G','image',0,255,nothing)
cv.createTrackbar('B','image',0,255,nothing)<N><N>
switch = '0 : OFF \n1 : ON'
cv.createTrackbar(switch,'image',0,1,nothing)<N><N>
while(1):
    cv.imshow('image',img)
    k=cv.waitKey(1) & 0xFF
    if k == 27:
        break
    
    r = cv.getTrackbarPos('R','image')
    g = cv.getTrackbarPos('G','image')
    b = cv.getTrackbarPos('B','image')
    s = cv.getTrackbarPos(switch,'image')<N><N>
    if s == 0:
        img[:] = 0
    else:
        img[:] = [b, g, r]<N><N>
cv.destroyAllWindows<N><N>
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt<N><N>
img = cv.imread('./samples/sudoku.png',0)
img = cv.medianBlur(img, 5)<N><N>
ret,th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)
th2 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_MEAN_C,
                        cv.THRESH_BINARY,11,2)
th3 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C,
                        cv.THRESH_BINARY,11,2)<N><N>

titles = ['Original Image', 'Global Thresholding (v=127)', 'Adaptive mean thresholding',
 'Adaptive Gaussian Thresholding']<N><N>
images = [img, th1, th2, th3]<N><N>
for i in range(4):
    plt.subplot(2,2,i+1), plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([]), plt.yticks([])<N><N>
plt.show()
<N><N>
import numpy as np
import cv2 as cv<N><N>
blue=np.uint8([[[0, 0, 255]]])
hsv_blue = cv.cvtColor(blue,cv.COLOR_BGR2HSV)
print ('------------------------')
print(hsv_blue)
print ('------------------------')<N><N>
cap = cv.VideoCapture(0)<N><N>
<N><N>while(1):
    _, frame = cap.read()
    hsv = cv.cvtColor(frame,cv.COLOR_BGR2HSV)<N><N>
    lower_blue = np.array([100, 50, 50])
    upper_blue = np.array([130, 255, 255])<N><N>
    mask = cv.inRange(hsv,lower_blue,upper_blue)<N><N>
    res = cv.bitwise_and(frame,frame,mask)
    cv.imshow('frame',frame)
    cv.imshow('mask',mask)
    cv.imshow('res',res)<N><N>
    k=cv.waitKey(5) & 0xFF
    if k==27:
        break<N><N>
cv.destroyAllWindows()
<N><N>
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt<N><N>
img = cv.imread('./samples/sudoku.png',0)<N><N>
laplacian = cv.Laplacian(img, cv.CV_64F)
sobelx = cv.Sobel(img, cv.CV_64F, 1, 0, ksize=5)
sobely = cv.Sobel(img, cv.CV_64F, 0, 1, ksize=5)<N><N>
plt.subplot(2, 2, 1),plt.imshow(img, cmap='gray')
plt.title('Original'), plt.xticks([]),plt.yticks([])
plt.subplot(2, 2, 2),plt.imshow(laplacian, cmap='gray')
plt.title('laplacian'), plt.xticks([]), plt.yticks([])<N><N>
plt.subplot(2, 2, 3),plt.imshow(sobelx, cmap='gray')
plt.title('sobel x'), plt.xticks([]), plt.yticks([])
plt.subplot(2, 2, 4),plt.imshow(sobely, cmap='gray')
plt.title('sobel y'), plt.xticks([]), plt.yticks([])<N><N>
plt.show()<N><N>
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt<N><N>
img = cv.imread('./samples/noisy 2.png',0)<N><N>
ret1, th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)<N><N>
ret2, th2 = cv.threshold(img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)<N><N>
blur = cv.GaussianBlur(img,(5,5),0)<N><N>
ret3, th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY + cv.THRESH_OTSU)<N><N>
images = [img, 0, th1,
        img, 0, th2, 
        blur, 0, th3]
titles = ['Original noisy image', 'Histogram', 'Global threhsholding',
        ' Original noisy image', 'Histogram', 'Otsus thresholding',
        'Gaussian filtered image','Histogram','Otsus thresholding']<N><N>

for i in range(3):
    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')
    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)
    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')
    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])
plt.show()<N><N>
import numpy as np
import cv2 as cv<N><N>from matplotlib import pyplot as plt<N><N>img = cv.imread('./samples/opencv-logo.png')<N><N>kernel = np.ones((5,5), np.float32)/25
# dst = cv.filter2D(img,-1,kernel)
dst = cv.blur(img,(5,5))
dst = cv.GaussianBlur(img,(5,5),0)<N><N>plt.subplot(121),plt.imshow(img),plt.title('original')
plt.xticks([]), plt.yticks([])
plt.subplot(122), plt.imshow(dst), plt.title('Averaging')
plt.xticks([]), plt.yticks([])
plt.show()<N><N>
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt<N><N>
img = cv.imread('./samples/gradient.png',0)
res, thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
res, thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)
res, thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
res, thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
res, thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)<N><N>
titles = ['Origianl Image', 'binary', 'binary inv', 'trunc', 'ToZero',' tozero_inv']
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]<N><N>
for i in range(6):
    plt.subplot(2,3,i+1), plt.imshow(images[i],'gray',vmin=0,vmax=255)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()<N><N>
import numpy as np
import cv2 as cv<N><N>img = cv.imread('./samples/messi5.jpg',0)
rows, cols = img.shape<N><N># Shift
M = np.float32([[1,0,100], [0,1,50]]) # shift (100,50)
dst = cv.warpAffine(img, M, (cols,rows))<N><N>
cv.imshow('image',img)
cv.waitKey(0)
cv.imshow('image', dst)
cv.waitKey(0)<N><N># rotate
M = cv.getRotationMatrix2D((cols/2, rows/2),90,1)
dst = cv.warpAffine(img,M,(cols,rows))
print(M)<N><N>cv.imshow('image',dst)
cv.waitKey(0)<N><N>cv.destroyAllWindows()

import cv2
import numpy as np
from glob import glob
import matplotlib.pyplot as plt<N><N>
from tkinter import *
import tkinter as tk
from tkinter import filedialog, Text
from PIL import Image,ImageTk
import pytesseract as pt
from pytesseract import Output
import cv2
import numpy as np
import random as rng<N><N>
from math import floor<N><N>

counter = 0
   <N><N>
#!/usr/bin/env python<N><N>
'''
Floodfill sample.<N><N>
Usage:
  floodfill.py [<image>]<N><N>
  Click on the image to set seed point<N><N>
Keys:
  f     - toggle floating range
  c     - toggle 4/8 connectivity
  ESC   - exit
'''<N><N>
# Python 2/3 compatibility
from __future__ import print_function<N><N>
import numpy as np
import cv2
from math import floor<N><N>
if __name__ == '__main__':
    import sys
    try:
        fn = sys.argv[1]
    except:
        fn = './fruits.jpg'
    print(__doc__)<N><N>
    img = cv2.imread(fn)
    if img is None:
        print('Failed to load image file:', fn)
        sys.exit(1)<N><N>
    h, w = img.shape[:2]
    mask = np.zeros((h+2, w+2), np.uint8)
    seed_pt = None
    fixed_range = True
    connectivity = 4<N><N>
import pygame
import serial
import time
from pygame.locals import *
from pygame import mixer<N><N>

import numpy as np
import cv2
import mediapipe as mp<N><N>
<N><N>
serial_port = serial.Serial('COM3', 9600)<N><N>
def get_offsets():
	
	pitch = list()
	roll = list()<N><N>
	while len(pitch) < 500:
		
		while serial_port.inWaiting() == 0:
			pass
		
		data = serial_port.readline()<N><N>
		if len(data) > 1:<N><N>
			data = data.decode()
			if len(data.split(",")) == 2:
				pitch_measurement, roll_measurement = data.split(",")<N><N>
				pitch.append(float(pitch_measurement))
				roll.append(float(roll_measurement))<N><N>
	pitch_offset = sum(pitch) / len(pitch)
	roll_offset = sum(roll) / len(roll)<N><N>

	return (pitch_offset, roll_offset)<N><N>

print("Encontrando offsets, Wait...")
pitch_offset, roll_offset = get_offsets()<N><N>
print(f"pitch offset: {pitch_offset}")
print(f"roll offset: {roll_offset}")<N><N>
<N><N><N><N>
pygame.mixer.pre_init(44100, -16, 2, 512)
mixer.init()
pygame.init()<N><N>
clock = pygame.time.Clock()
fps = 30<N><N>
screen_width = 500
screen_height = 500<N><N>
screen = pygame.display.set_mode((screen_width, screen_height))
pygame.display.set_caption('Platformer')<N><N>

#define font
font = pygame.font.SysFont('Bauhaus 93', 70)
font_score = pygame.font.SysFont('Bauhaus 93', 30)<N><N>
#define game variables
tile_size = 25
game_over = 0
main_menu = True
score = 0<N><N>
#define colours
white = (255, 255, 255)
blue = (0, 0, 255)
red = (255, 0, 0)<N><N>

#load images
bg_img = pygame.image.load('img/bgrb.png')
restart_img = pygame.image.load('img/restart.png')
start_img = pygame.image.load('img/start.png')
exit_img = pygame.image.load('img/exit.png')<N><N>

#load sounds
coin_fx = pygame.mixer.Sound('img/coin.wav')
coin_fx.set_volume(0.5)
jump_fx = pygame.mixer.Sound('img/jump.wav')
jump_fx.set_volume(0.5)
game_over_fx = pygame.mixer.Sound('img/game_over.wav')
game_over_fx.set_volume(0.5)
restart_fx = pygame.mixer.Sound('img/restart.wav')
restart_fx.set_volume(0.5)<N><N>

def draw_text(text, font, text_col, x, y):
	img = font.render(text, True, text_col)
	screen.blit(img, (x, y))<N><N>
class Button():
	def __init__(self, x, y, image):
		self.image = image
		self.rect = self.image.get_rect()
		self.rect.x = x
		self.rect.y = y
		self.clicked = False<N><N>
	def draw(self):
		action = False<N><N>
		#get mouse position
		pos = pygame.mouse.get_pos()<N><N>
		#check mouseover and clicked conditions
		if self.rect.collidepoint(pos):
			if pygame.mouse.get_pressed()[0] == 1 and self.clicked == False:
				action = True
				self.clicked = True<N><N>
		if pygame.mouse.get_pressed()[0] == 0:
			self.clicked = False<N><N>

		#draw button
		screen.blit(self.image, self.rect)<N><N>
		return action<N><N>

class Player():
		
	def __init__(self, x, y):
		self.reset(x, y)
		<N><N>
	def reset(self, x, y):<N><N>
import numpy as np
import cv2
import mediapipe as mp
import time
import matplotlib.pyplot as plotter
import csv<N><N>
t_0 = time.time()<N><N>
video_capture = cv2.VideoCapture(0)
mphands = mp.solutions.hands
hands = mphands.Hands()
mpdraw = mp.solutions.drawing_utils<N><N>
previous_time = 0
current_time = 0<N><N>
wx = []
wy = []
wz = []<N><N>
	<N><N>def find_normal_vector(v1, v2):
    
    normal_vector = np.cross(v1, v2)
    return normal_vector<N><N>

def get_inclination_angle(vector, axis_vector):<N><N>
	dot_product = np.dot(vector, axis_vector)
	magnitude_product = np.linalg.norm(vector) * np.linalg.norm(axis_vector)<N><N>
	theta = np.arccos(dot_product / magnitude_product)<N><N>
	return theta<N><N>
def draw_normal_vector(image, initial_point, final_point):<N><N>
	width, height, channels = image.shape<N><N>
	frame_size = np.array([width, height])<N><N>
	P0 = initial_point[:2] * frame_size
	P1 = final_point[:2] * frame_size<N><N>
	P0 = np.array([int(p) for p in P0])
	P1 = np.array([int(p) for p in P1])<N><N>
	cv2.line(image, P0, P1, (255, 0, 255), 15)<N><N>
<N><N>while (time.time() - t_0) < 30:<N><N>
	success, image = video_capture.read()
	RGB_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
	results = hands.process(RGB_image)<N><N>
	
	if results.multi_hand_landmarks:
		for hand_landmarks in results.multi_hand_landmarks:
			
			wrist = hand_landmarks.landmark[0]
			index_finger = hand_landmarks.landmark[5]
			pinky = hand_landmarks.landmark[17]<N><N>
			wrist = np.array([wrist.x, wrist.y, wrist.z])
			index_finger = np.array([index_finger.x, index_finger.y, index_finger.z])
			pinky = np.array([pinky.x, pinky.y, pinky.z])<N><N>

			v1 = index_finger - wrist
			v2 = pinky - wrist<N><N>
			normal_vector = find_normal_vector(v1, v2)
			normalized_normal_vector = normal_vector / np.linalg.norm(normal_vector)<N><N>
			<N><N>			P0 = wrist[:2]
			P1 = normalized_normal_vector[:2] + wrist[:2]<N><N>
			inclination_angle = np.arctan((P1 - P0)[0] / (P1 - P0)[1]) * 180 / np.pi<N><N>
			cv2.putText(image, str(inclination_angle), (150, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)<N><N>
			if wrist[1] < 0.5:
				cv2.putText(image, "JUMP!", (500, 300), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)<N><N>
			draw_normal_vector(image, wrist, normalized_normal_vector + wrist)<N><N>

			"""
			for id, landmark in enumerate(hand_landmarks.landmark):
				
				print(id, landmark)
				if id == 0:
					wx.append(landmark.x)
					wy.append(landmark.y)
					wz.append(landmark.z)<N><N>
				#width, height, channels = image.shape<N><N>
				#x, y = int(landmark.x * width), int(landmark.y*height)<N><N>
				#print(id, x, y)
			"""
			mpdraw.draw_landmarks(image, hand_landmarks, mphands.HAND_CONNECTIONS)<N><N>
	current_time = time.time()
	fps = 1 / (current_time - previous_time)
	previous_time = current_time<N><N>
	cv2.putText(image, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)<N><N>

	cv2.imshow("Camera", image)
	cv2.waitKey(1)<N><N>
<N><N>"""
plotter.plot([i for i in range(len(wx))], wx, label="x")
plotter.plot([i for i in range(len(wy))], wy, label="y")
plotter.plot([i for i in range(len(wz))], wz, label="z")
plotter.legend()
plotter.show()<N><N>

with open("mapeo.csv", 'w') as file:<N><N>
	writer = csv.writer(file)<N><N>
	for x, y, z in zip(wx, wy, wz):
		row = [x, y, z]
		writer.writerow(row)
"""<N><N>
import telegram
import datetime
import time
import cv2
import os
import queue 
import threading
import numpy as np<N><N>
from utils import Utils<N><N>
class HomeCamBot():
    def __init__(self):
        self.botToken = os.environ['BOT_TOKEN']
        self.chat_id = os.environ['CHAT_ID']
        self.bot = telegram.Bot(token=self.botToken)
        self.HeartBeatSent = False<N><N>
    def SendPhoto(self, img, msg):
        message = self.bot.sendPhoto(photo=img, caption=msg, chat_id=self.chat_id)
        return message<N><N>
    def SendMessage(self, message):
        self.bot.sendMessage(chat_id=self.chat_id, text=message)
        self.HeartBeatSent = True<N><N>
class Detector():
    def __init__(self):
        self.modelConfiguration = "yolo/coco_yolov3-tiny.cfg"
        self.modelWeights = "yolo/coco_yolov3-tiny.weights"
        self.net = cv2.dnn.readNetFromDarknet(self.modelConfiguration, self.modelWeights)
        self.net.getLayerNames()
        self.layerOutput = self.net.getUnconnectedOutLayersNames()<N><N>
        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)<N><N>
        self.target_w = 416
        self.target_h = 416<N><N>
        self.classes = None
        self.classesFile = "yolo/coco.names"
        with open(self.classesFile, 'rt') as f:
            self.classes = f.read().rstrip('\n').split('\n')      <N><N>
        self.include_objects = ["person", "laptop", "keyboard", "cell phone", "tvmonitor", "knife"]
        self.util_lib = Utils()<N><N>
    def detect(self, frame):
        blob = cv2.dnn.blobFromImage(frame, 
                            1.0/255, 
                            (self.target_w, self.target_h), 
                            (0, 0, 0), swapRB=True, crop=False)<N><N>
        # predict classess & box
        self.net.setInput(blob)
        output = self.net.forward(self.layerOutput)
        
        t, _ = self.net.getPerfProfile()
        print('inference time: %.2f s' % (t / cv2.getTickFrequency()))<N><N>
        return self.util_lib.postprocess(output, frame, self.classes, self.include_objects)<N><N>
class CustomVideoCapture():
    def __init__(self, name):
        self.name = name
        self.cap = cv2.VideoCapture(self.name)
        self.q = queue.Queue()
        t = threading.Thread(target=self._reader)
        t.daemon = True
        t.start()<N><N>
import os
import cv2 
import numpy as np <N><N>
class Utils():
    def draw_ped(self, img, label, x0, y0, xt, yt, color=(255,127,0), text_color=(255,255,255)):<N><N>
        y0, yt = max(y0 - 15, 0) , min(yt + 15, img.shape[0])
        x0, xt = max(x0 - 15, 0) , min(xt + 15, img.shape[1])<N><N>
import cv2<N><N># Load the cascade
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N># Read the input image
img = cv2.imread('test.jpg')<N><N># Convert into grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N># Detect faces
faces = face_cascade.detectMultiScale(gray, 1.1, 4)<N><N># Draw rectangle around the faces
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)<N><N># Display the output
cv2.imshow('img', img)
cv2.waitKey()

import cv2<N><N>
# Load the cascade
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N>
# To capture video from webcam. 
cap = cv2.VideoCapture(0)
# To use a video file as input 
# cap = cv2.VideoCapture('filename.mp4')<N><N>
<N><N><N><N>

while True:
    
    # Read the frame
    _, img = cap.read()
    font = cv2.FONT_HERSHEY_SIMPLEX<N><N>
    
    cv2.putText(img,'Drakor Start-Up PROJECT ',(30,30), font, 1,(255,0,0),2)
    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>
    # Detect the faces
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)<N><N>
    # Draw the rectangle around each face
    for (x, y, w, h) in faces:
        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
        
        cv2.putText(img,'face '+chr(x), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)<N><N>

    # Display
    cv2.imshow('img', img)<N><N>
    # Stop if escape key is pressed
    k = cv2.waitKey(30) & 0xff
    if k==27:
        break
        
# Release the VideoCapture object
cap.release()<N><N>
import cv2<N><N>
# Load the cascade
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')
smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')
# To capture video from webcam. 
cap = cv2.VideoCapture(0)
# To use a video file as input <N><N>
# cap = cv2.VideoCapture('filename.mp4')<N><N>
<N><N><N><N>

while True:
    
    # Read the frame
    _, img = cap.read()
    font = cv2.FONT_HERSHEY_SIMPLEX<N><N>
# Train multiple images per person
# Find and recognize faces in an image using a SVC with scikit-learn<N><N>
import face_recognition
import cv2
import numpy as np
from PIL import Image, ImageDraw<N><N>

# Get a reference to webcam #0 (the default one)
video_capture = cv2.VideoCapture(0)<N><N>
# Loading picture
name_image = face_recognition.load_image_file('''Image Name''')
name_face_encoding = face_recognition.face_encodings(name_image)[0]<N><N>
<N><N># Create arrays of known face encodings and their names
known_face_encodings = [
    name_face_encoding,
]
known_face_names = [
    '''Give Names Here'''
]<N><N>
# Initialize some variables
face_locations = []
face_encodings = [500000]
face_names = []
process_this_frame = True<N><N>
while True:
    # Grab a single frame of video
    ret, frame = video_capture.read()<N><N>
    # Resize frame of video to 1/4 size for faster face recognition processing
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)<N><N>
    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)
    rgb_small_frame = small_frame[:, :, ::-1]<N><N>
    # Only process every other frame of video to save time
    if process_this_frame:
        # Find all the faces and face encodings in the current frame of video
        face_locations = face_recognition.face_locations(rgb_small_frame, model="cnn")
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)
        face_landmarks_list = face_recognition.face_landmarks(rgb_small_frame)<N><N>
        face_names = []
        for face_encoding in face_encodings:
            # See if the face is a match for the known face(s)
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"<N><N>
            # # If a match was found in known_face_encodings, just use the first one.
            # if True in matches:
            #     first_match_index = matches.index(True)
            #     name = known_face_names[first_match_index]<N><N>
            # Or instead, use the known face with the smallest distance to the new face
            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
            best_match_index = np.argmin(face_distances)
            if matches[best_match_index]:
                name = known_face_names[best_match_index]<N><N>
            face_names.append(name)
            # Create a PIL imagedraw object so we can draw on the picture
            pil_image = Image.fromarray(rgb_small_frame)
            d = ImageDraw.Draw(pil_image)<N><N>
            for face_landmarks in face_landmarks_list:<N><N>
                # Print the location of each facial feature in this image
                for facial_feature in face_landmarks.keys():
                    print("The {} in this face has the following points: {}".format(facial_feature,
                                                                                    face_landmarks[facial_feature]))<N><N>
                # Let's trace out each facial feature in the image with a line!
                for facial_feature in face_landmarks.keys():
                    d.line(face_landmarks[facial_feature], width=1)<N><N>
<N><N>    process_this_frame = not process_this_frame<N><N>
    # Display the results
    for (top, right, bottom, left), name in zip(face_locations, face_names):
        # Scale back up face locations since the frame we detected in was scaled to 1/4 size
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4
        # Draw a box around the face
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)<N><N>
        # Draw a label with a name below the face
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)<N><N>
    # Display the resulting image
    cv2.imshow('Video Console', frame)
    <N><N>
    # Hit 'q' on the keyboard to quit!
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>
# Release handle to the webcam
video_capture.release()
cv2.destroyAllWindows()
<N><N>
import cv2
import mediapipe as mp
import time<N><N>

mpDraw=mp.solutions.drawing_utils
mpPose=mp.solutions.pose
pose=mpPose.Pose()<N><N>

cap = cv2.VideoCapture("4_Trim.mp4")
pTime=0<N><N>
while True:
    success,img=cap.read()
    imgRGB=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    results=pose.process(imgRGB)
    #print(results.pose_landmarks)
    if results.pose_landmarks:
        mpDraw.draw_landmarks(img,results.pose_landmarks,mpPose.POSE_CONNECTIONS)<N><N>
        for id,lm in enumerate(results.pose_landmarks.landmark):
            h,w,c=img.shape
            print(id,lm)
            cx,cy= int(lm.x * w), int(lm.y * h)
            cv2.circle(img,(cx,cy),5,(255,0,0),cv2.FILLE)<N><N>

    cTime= time.time()
    fps=1/(cTime-pTime)
    pTime=cTime<N><N>
    cv2.putText(img,str(int(fps)), (70,50), cv2.FONT_HERSHEY_PLAIN, 3, (255,0,0), 3)<N><N>
    cv2.imshow("Display Screen",img)<N><N>
    cv2.waitKey(1)<N><N>
import cv2
import mediapipe as mp
import time<N><N>

class poseDetector():
    def __init__(self,mode=False, upBody = False, smooth = True,
                 detectionCon = 0.5, trackCon = 0.5):<N><N>
        self.mode = mode
        self.upBody = upBody
        self.smooth = smooth
        self.detectionCon = detectionCon
        self.trackCon = trackCon<N><N>
        self.mpDraw = mp.solutions.drawing_utils
        self.mpPose = mp.solutions.pose
        self.pose = self.mpPose.Pose(self.mode,self.upBody,self.smooth,self.detectionCon,self.trackCon)<N><N>

    def findPose(self,img,draw=True):
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        self.results = self.pose.process(imgRGB)<N><N>
        if self.results.pose_landmarks:
            if draw:
                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)<N><N>

        return img<N><N>
    def findposition(self,img,draw=True):
        lmlist = []
        if self.results.pose_landmarks:
            for id,lm in enumerate(self.results.pose_landmarks.landmark):
                h,w,c=img.shape
                #print(id,lm)
                cx,cy= int(lm.x * w), int(lm.y * h)
                lmlist.append([id,cx,cy])
                if draw:
                    cv2.circle(img,(cx,cy),5,(255,0,0),cv2.FILLED)<N><N>
        return lmlist<N><N>

def main():
    cap = cv2.VideoCapture("4_Trim.mp4")
    pTime = 0<N><N>
    detector =  poseDetector()<N><N>
    while True:
        success, img = cap.read()
        img = detector.findPose(img)
        lmlist = detector.findposition(img)
        print(lmlist)<N><N>
        cTime = time.time()
        fps = 1 / (cTime - pTime)
        pTime = cTime<N><N>
        cv2.putText(img, str(int(fps)), (70, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)<N><N>
        cv2.imshow("Display Screen", img)<N><N>
        cv2.waitKey(1)<N><N>
if __name__ == "__main__":
    main()<N><N>
import datetime<N><N>
from PIL import ImageGrab
import numpy as np
import cv2
#which captures windows API, which is for screen resolution
from win32api import GetSystemMetrics<N><N>
# importing libraries
import cv2
import numpy as np
import datetime<N><N>

cap = cv2.VideoCapture(0)  # VideoCapture is used to capture video from webcam
# here '0' indicates that the video will be captured from the first webcam on your device<N><N>

# taking input point
# this is used to capture very first frame of the video and we will use it later on 
# in calcOpticalFlowPyrLK() function
# for this we have created gray input image with blur of (4,4) kernel<N><N>

_, inp_img = cap.read()
inp_img = cv2.flip(inp_img, 1)
inp_img = cv2.blur(inp_img, (4, 4))
gray_inp_img = cv2.cvtColor(inp_img, cv2.COLOR_BGR2GRAY)<N><N>
# tracking starts here<N><N>
# the old_pts is the initialized window which we see when the code is run
old_pts = np.array([[350, 180], [350, 350]], dtype=np.float32).reshape(-1, 1, 2)<N><N>
# creating backup of old points and storing it in backup_img
backup = old_pts.copy()
backup_img = gray_inp_img.copy()<N><N>
# Output windows which is blank and black in color which will show text
outp = np.zeros((480, 640, 3))<N><N>
# variable
ytest_pos = 40  # -----> to print the text in various positions in output window
###############<N><N>

# creating an infinite loop so that user can exit whenever he/she wants
while True:
    _, new_inp_img = cap.read()  # reading the frame
    new_inp_img = cv2.flip(new_inp_img, 1)  # flipping the frame
    new_inp_img = cv2.blur(new_inp_img, (4, 4))
    new_gray = cv2.cvtColor(new_inp_img, cv2.COLOR_BGR2GRAY)<N><N>
import cv2
import numpy as np
from playsound import playsound<N><N>
<N><N>cap = cv2.VideoCapture(0)<N><N>
import cv2
import numpy as np
import sys
import argparse
parser = argparse.ArgumentParser('parser')
parser.add_argument('-cam',default=0)
parser.add_argument('-speed',default=1)<N><N>
args = parser.parse_args()
cap = cv2.VideoCapture(int(args.cam))
speed = int(args.speed)
counter = 0
_, f = cap.read()
hist_frame = f[:, 0:counter+1]<N><N>
# -*- coding: utf-8 -*-<N><N>
import serial
from serial.tools import list_ports
import sys
import cv2
import numpy as np<N><N>
minimum_radius = int(10);<N><N>
def getCircleDualThreshould(hsv_frame, lower_color1, upper_color1,lower_color2, upper_color2):
    global minimum_radius;<N><N>
    # 指定した色範囲のみを抽出する
    bin_image1 = cv2.inRange(hsv_frame, lower_color1, upper_color1);
    bin_image2 = cv2.inRange(hsv_frame, lower_color2, upper_color2);
    bin_image = cv2.bitwise_or(bin_image1, bin_image2);<N><N>
    # オープニング・クロージングによるノイズ除去
    element8 = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], np.uint8);
    oc = cv2.morphologyEx(bin_image, cv2.MORPH_OPEN, element8);
    oc = cv2.morphologyEx(oc, cv2.MORPH_CLOSE, element8);<N><N>
    # 輪郭抽出
    contours, hierarchy = cv2.findContours(oc, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE);
    if len(contours) > 0:
        # 一番大きい領域を指定する
        contours.sort(key=cv2.contourArea, reverse=True);
        cnt = contours[0];<N><N>
        # 最小外接円を用いて円を検出する
        (x, y), radius = cv2.minEnclosingCircle(cnt);
        center = (int(x), int(y));
        radius = int(radius);<N><N>
        # 円が小さすぎたら円を検出していないとみなす
        if radius < minimum_radius:
            return None, None, None;
        else:
            return oc, center, radius;
    else:
        return None, None, None;<N><N>
def getCircle(hsv_frame, lower_color, upper_color):
    global minimum_radius;
    # 指定した色範囲のみを抽出する
    bin_image = cv2.inRange(hsv_frame, lower_color, upper_color);<N><N>
    # オープニング・クロージングによるノイズ除去
    element8 = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], np.uint8);
    oc = cv2.morphologyEx(bin_image, cv2.MORPH_OPEN, element8);
    oc = cv2.morphologyEx(oc, cv2.MORPH_CLOSE, element8);<N><N>
    # 輪郭抽出
    contours, hierarchy = cv2.findContours(oc, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE);
    if len(contours) > 0:
        # 一番大きい領域を指定する
        contours.sort(key=cv2.contourArea, reverse=True);
        cnt = contours[0];<N><N>
        # 最小外接円を用いて円を検出する
        (x, y), radius = cv2.minEnclosingCircle(cnt);
        center = (int(x), int(y));
        radius = int(radius);<N><N>
# -*- coding: utf-8 -*-<N><N>
import serial
from serial.tools import list_ports
import sys
import cv2
import numpy as np
import time
import math
import csv<N><N>
minimum_radius = int(10)
length_of_memory = int(100)<N><N>
# -*- coding: utf-8 -*-<N><N>
import serial
from serial.tools import list_ports
import sys
import cv2
import numpy as np
import csv
import time
import math<N><N>

length_of_memory = int(100)<N><N>
# -*- coding: utf-8 -*-<N><N>
import cv2
import numpy as np<N><N>
src = cv2.imread('sample2.png')<N><N>
# Convert image to gray and blur it
src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
cv2.namedWindow('src_grey')
keep_while_loop = True
bin_thresh1 = 100<N><N>
while keep_while_loop:
    kret = cv2.waitKey(1000) & 0xFF  # 1000ミリ秒の間キー入力を受け付ける
    if kret == ord('q'):  # qを押したら実行
        keep_while_loop = False  # while loopを脱出
    elif kret == ord('o'):
        bin_thresh1 = bin_thresh1+5  # 二値化の閾値をあげる
        print('bin_thresh1=', bin_thresh1)
    elif kret == ord('p'):
        bin_thresh1 = bin_thresh1-5  # 二値化の閾値をさげる
        print('bin_thresh1=', bin_thresh1)<N><N>
    cv2.imshow('src_grey', src_gray)  # グレースケール画像を表示<N><N>
    ret, bin_img = cv2.threshold(src_gray, bin_thresh1, 255,
                                 cv2.THRESH_BINARY)  # 二値化画像生成
    ret, bin_imgRev = cv2.threshold(src_gray, bin_thresh1, 255,
                                    cv2.THRESH_BINARY_INV)  # 反転した二値化画像生成<N><N>
    cv2.imshow('bin_img', bin_img)  # 二値化画像生成
    cv2.imshow('reverse bin_img', bin_imgRev)  # 反転した二値化画像生成<N><N>
    contours, hierarchy = cv2.findContours(bin_img,
                                           cv2.RETR_EXTERNAL,
                                           cv2.CHAIN_APPROX_NONE)  # 輪郭抽出<N><N>
    # Find the rotated rectangles
    minRect = [None]*len(contours)  # 四角囲みの配列確保
    for i, c in enumerate(contours):
        minRect[i] = cv2.minAreaRect(c)  # 囲んでいる四角を取得，代入<N><N>
    drawing = np.zeros(
        (bin_img.shape[0], bin_img.shape[1], 3), dtype=np.uint8)  # 輪郭度を描画する配列
    for i, c in enumerate(contours):
        color1 = (0, 255, 255)
        color2 = (255, 128, 0)
        # contour
        cv2.drawContours(drawing, contours, i, color1)  # 輪郭を描画<N><N>
        # rotated rectangle
        box = cv2.boxPoints(minRect[i])  # 囲んでいる四角の頂点を計算
        box = np.intp(box)  # 整数化
        cv2.drawContours(drawing, [box], 0, color2)  # 四角を描画<N><N>
    # Get the moments
    mu = [None]*len(contours)  # モーメントのデータを代入する配列を確保
    for i in range(len(contours)):
        mu[i] = cv2.moments(contours[i])<N><N>
# -*- coding: utf-8 -*-<N><N>
import cv2
import numpy as np<N><N>
src = cv2.imread('sample2.png')<N><N>
# Convert image to gray and blur it
src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
cv2.namedWindow('src_grey')
keep_while_loop = True
bin_thresh1 = 100<N><N>
while keep_while_loop:
    kret = cv2.waitKey(1000) & 0xFF  # 1000ミリ秒の間キー入力を受け付ける
    if kret == ord('q'):  # qを押したら実行
        keep_while_loop = False  # while loopを脱出
    elif kret == ord('o'):
        bin_thresh1 = bin_thresh1+5  # 二値化の閾値をあげる
        print('bin_thresh1=', bin_thresh1)
    elif kret == ord('p'):
        bin_thresh1 = bin_thresh1-5  # 二値化の閾値をさげる
        print('bin_thresh1=', bin_thresh1)<N><N>
    cv2.imshow('src_grey', src_gray)  # グレースケール画像を表示<N><N>
    ret, bin_img = cv2.threshold(src_gray, bin_thresh1, 255,
                                 cv2.THRESH_BINARY)  # 二値化画像生成
    ret, bin_imgRev = cv2.threshold(src_gray, bin_thresh1, 255,
                                    cv2.THRESH_BINARY_INV)  # 反転した二値化画像生成<N><N>
    cv2.imshow('bin_img', bin_img)  # 二値化画像生成
    cv2.imshow('reverse bin_img', bin_imgRev)  # 反転した二値化画像生成<N><N>
    contours, hierarchy = cv2.findContours(bin_img,
                                           cv2.RETR_EXTERNAL,
                                           cv2.CHAIN_APPROX_NONE)  # 輪郭抽出<N><N>
    # Find the rotated rectangles
    minRect = [None]*len(contours)  # 四角囲みの配列確保
    for i, c in enumerate(contours):
        minRect[i] = cv2.minAreaRect(c)  # 囲んでいる四角を取得，代入<N><N>
    drawing = np.zeros(
        (bin_img.shape[0], bin_img.shape[1], 3), dtype=np.uint8)  # 輪郭度を描画する配列
    for i, c in enumerate(contours):
        color1 = (0, 255, 255)
        color2 = (255, 128, 0)
        # contour
        cv2.drawContours(drawing, contours, i, color1)  # 輪郭を描画<N><N>
        # rotated rectangle
        box = cv2.boxPoints(minRect[i])  # 囲んでいる四角の頂点を計算
        box = np.intp(box)  # 整数化
        cv2.drawContours(drawing, [box], 0, color2)  # 四角を描画<N><N>
    # Get the moments
    mu = [None]*len(contours)  # モーメントのデータを代入する配列を確保
    for i in range(len(contours)):
        mu[i] = cv2.moments(contours[i])<N><N>
#view the pixel intensity of the image while display<N><N>
import os
import cv2
import numpy as np
from PIL import Image
from matplotlib import pyplot as plt<N><N>
prediction_images_nd_mask = "/segmentation_results/"<N><N>
list_img_msk = os.listdir(prediction_images_nd_mask)<N><N>
for j in list_img_msk:
	if j.endswith("_image.png"):
		image_path = prediction_images_nd_mask + j
		mask_path = prediction_images_nd_mask + j.split("_")[0]+"_prediction.png"
		img = cv2.imread(image_path)
		plt.figure(1)
		plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
		plt.show()
<N><N>
import argparse
import cv2 as cv
import imutils
import numpy as np
from imutils.object_detection import non_max_suppression<N><N>
subject_label = 1
font = cv.FONT_HERSHEY_SIMPLEX
list_of_videos = []
hog = cv.HOGDescriptor()
hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())<N><N>
output_image_width = 500<N><N>
import numpy as np
import tensorflow as tf<N><N># Load TFLite model and allocate tensors.
interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class//model.tflite")<N><N># Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()<N><N>interpreter.allocate_tensors()<N><N># input details
print(input_details)
# output details
print(output_details)
import tensorflow as tf
import numpy as np
import cv2
import pathlib<N><N>
interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class//model.tflite")<N><N>
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()<N><N>
print(input_details)
print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>
for file in pathlib.Path('images').iterdir():
    img = cv2.imread(r"{}".format(file.resolve()))
    new_img = cv2.resize(img, (320, 320))<N><N>
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>
    print("For file {}".format(file.stem))
    print("Rectangles are: {}".format(rects))
    print("Scores are: {}".format(scores))<N><N>
import tensorflow as tf
import numpy as np
import cv2
import pathlib<N><N>
interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class//model.tflite")<N><N>
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()<N><N>
print(input_details)
print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>
for file in pathlib.Path('images').iterdir():<N><N>
    img = cv2.imread(r"{}".format(file.resolve()))
    new_img = cv2.resize(img, (320, 320))<N><N>
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>
    for index, score in enumerate(scores[0]):
        if score > 0.7:
            print("For file {}".format(file.stem))
            print("Rectangles are: {}".format(rects[index]))<N><N>
import tensorflow as tf
import numpy as np
import cv2
import pathlib<N><N>
interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class_2//model.tflite")<N><N>
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()<N><N>
print(input_details)
print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>
def draw_rect(image, box):
    height, width, c = image.shape
    y_min = int(max(1, (box[0] * height)))
    x_min = int(max(1, (box[1] * width)))
    y_max = int(min(height, (box[2] * height)))
    x_max = int(min(width, (box[3] * width)))<N><N>
    # draw a rectangle on the image
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 255, 255), 2)<N><N>

for file in pathlib.Path('images').iterdir():<N><N>
    if file.suffix != '.jpg' and file.suffix != '.png':
        continue<N><N>
    img = cv2.imread(r"{}".format(file.resolve()))
    new_img = cv2.resize(img, (320, 320))
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])<N><N>
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>
    for index, score in enumerate(scores[0]):
        if score > 0.5:
            draw_rect(new_img, rects[0][index])<N><N>
    cv2.imshow("image", new_img)
    cv2.waitKey(0)<N><N>
import numpy as np
import cv2<N><N>
xml = 'model//haarcascade_frontalface_default.xml'
face_cascade = cv2.CascadeClassifier(xml)<N><N>
cap = cv2.VideoCapture(0)  # 노트북 웹캠을 카메라로 사용
cap.set(3, 640)  # 너비
cap.set(4, 480)  # 높이<N><N>
while (True):
    ret, frame = cap.read()
    frame = cv2.flip(frame, 1)  # 좌우 대칭
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<N><N>
    faces = face_cascade.detectMultiScale(gray, 1.05, 5)
    print("Number of faces detected: " + str(len(faces)))<N><N>
    if len(faces):
        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)<N><N>
    cv2.imshow('result', frame)<N><N>
    k = cv2.waitKey(30) & 0xff
    if k == 27:  # Esc 키를 누르면 종료
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import cv2, pathlib
import tensorflow as tf<N><N>
cap = cv2.VideoCapture(0)  # 노트북 웹캠을 카메라로 사용
cap.set(3, 320)  # 너비
cap.set(4, 320)  # 높이<N><N>
# DHT11, HC-SR04
# interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__dht_hc//model.tflite")<N><N>
# SW_420, DHT11, HC_SR04, SZH_EK086
# interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class//model.tflite")<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04
# interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class_2//model.tflite")<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04, RFID
interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection_class_5//model.tflite")<N><N>
# COCO SSD MobileNet v1 Model
# interpreter = tf.contrib.lite.Interpreter(model_path="model//COCO SSD MobileNet v1 Model/model.tflite")<N><N>
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
# print(input_details)
# print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>

def draw_rect(image, box):
    height, width, c = image.shape
    y_min = int(max(1, (box[0] * height)))
    x_min = int(max(1, (box[1] * width)))
    y_max = int(min(height, (box[2] * height)))
    x_max = int(min(width, (box[3] * width)))<N><N>
    # draw a rectangle on the image
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), 1)<N><N>
while True:
    ret, frame = cap.read()
    new_img = cv2.resize(frame, (320, 320))
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    # ret, frame = cap.read()
    # cv2.imshow('frame', frame)
    # new_img = cv2.resize(frame, (320, 320))
    # cv2.imshow('img', new_img)<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>
    for index, score in enumerate(scores[0]):
        if score > 0.7:
            draw_rect(new_img, rects[0][index])<N><N>
    cv2.imshow("image", new_img)
    k = cv2.waitKey(30) & 0xff
    if k == 27:  # Esc 키를 누르면 종료
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import cv2, pathlib
import tensorflow as tf<N><N>
cap = cv2.VideoCapture(0)  # 노트북 웹캠을 카메라로 사용
cap.set(3, 320)  # 너비
cap.set(4, 320)  # 높이<N><N>
# DHT11, HC-SR04
# model_path="model//tflite-object_detection__dht_hc//model.tflite"<N><N>
# SW_420, DHT11, HC_SR04, SZH_EK086
# model_path="model//tflite-object_detection__4_class//model.tflite"<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04
# model_path="model//tflite-object_detection__4_class_2//model.tflite"<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04, RFID
model_path="model//tflite-object_detection_class_5//model.tflite"<N><N>
# COCO SSD MobileNet v1 Model
# model_path="model//COCO SSD MobileNet v1 Model/model.tflite"<N><N>

label_path = f'{model_path}/../dict.txt'<N><N>
interpreter = tf.contrib.lite.Interpreter(model_path=model_path)
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
# print(input_details)
# print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>

def draw_rect(image, box, detect_name):
    height, width, c = image.shape
    y_min = int(max(1, (box[0] * height)))
    x_min = int(max(1, (box[1] * width)))
    y_max = int(min(height, (box[2] * height)))
    x_max = int(min(width, (box[3] * width)))
    # draw a rectangle on the image
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), 1)
    cv2.putText(image, detect_name, (x_min, y_min), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)<N><N>

def create_category_index(label_path=label_path):
    f = open(label_path)
    category_index = {}
    for i, val in enumerate(f):
        if i != 0:
            val = val[:-1]
            if val != '???':
                category_index.update({(i - 1): {'id': (i - 1), 'name': val}})<N><N>
    f.close()
    return category_index<N><N>

def write_text(image, x, y, detect_name):
    pass<N><N>
category_index = create_category_index(label_path)
print(category_index)<N><N>
while True:
    ret, frame = cap.read()
    new_img = cv2.resize(frame, (320, 320))
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])<N><N>
    classes = interpreter.get_tensor(
        output_details[1]['index'])<N><N>
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>

    for index, score in enumerate(scores[0]):
        if score > 0.8:
            detect_name = category_index.get(int(classes[0][index])).get('name')
            draw_rect(new_img, rects[0][index], detect_name)<N><N>
    cv2.imshow("image", new_img)
    k = cv2.waitKey(30) & 0xff
    if k == 27:  # Esc 키를 누르면 종료
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import cv2, pathlib
import tensorflow as tf<N><N>
cap = cv2.VideoCapture(0)  # 노트북 웹캠을 카메라로 사용
cap.set(3, 320)  # 너비
cap.set(4, 320)  # 높이<N><N>
# DHT11, HC-SR04
interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__dht_hc//model.tflite")<N><N>
# SW_420, DHT11, HC_SR04, SZH_EK086
# interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class//model.tflite")<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04
# interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection__4_class_2//model.tflite")<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04, RFID
# interpreter = tf.contrib.lite.Interpreter(model_path="model//tflite-object_detection_class_5//model.tflite")<N><N>
# COCO SSD MobileNet v1 Model
# interpreter = tf.contrib.lite.Interpreter(model_path="model//COCO SSD MobileNet v1 Model/model.tflite")<N><N>
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
# print(input_details)
# print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>

def draw_rect(image, box):
    height, width, c = image.shape
    y_min = int(max(1, (box[0] * height)))
    x_min = int(max(1, (box[1] * width)))
    y_max = int(min(height, (box[2] * height)))
    x_max = int(min(width, (box[3] * width)))
    # draw a rectangle on the image
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), 1)
    write_text(image, x_min, y_min)<N><N>

def write_text(image, x, y):
    cv2.putText(image, "detect", (x, y), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)<N><N>

while True:
    ret, frame = cap.read()
    new_img = cv2.resize(frame, (320, 320))
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    # ret, frame = cap.read()
    # cv2.imshow('frame', frame)
    # new_img = cv2.resize(frame, (320, 320))
    # cv2.imshow('img', new_img)<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])<N><N>
    classes = interpreter.get_tensor(
        output_details[1]['index'])<N><N>
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>
    for index, score in enumerate(scores[0]):
        if score > 0.8:
            draw_rect(new_img, rects[0][index])
            print(f"classes : {classes[0][index]}")<N><N>

    cv2.imshow("image", new_img)
    k = cv2.waitKey(30) & 0xff
    if k == 27:  # Esc 키를 누르면 종료
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import cv2, pathlib
import tensorflow as tf<N><N>
cap = cv2.VideoCapture(0)  # 노트북 웹캠을 카메라로 사용
cap.set(3, 320)  # 너비
cap.set(4, 320)  # 높이<N><N>
# DHT11, HC-SR04
model_path="model//tflite-object_detection__dht_hc//model.tflite"<N><N>
# SW_420, DHT11, HC_SR04, SZH_EK086
# model_path="model//tflite-object_detection__4_class//model.tflite"<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04
# model_path="model//tflite-object_detection__4_class_2//model.tflite"<N><N>
# 1602_LCD, SW_420, DHT11, HC_SR04, RFID
# model_path="model//tflite-object_detection_class_5//model.tflite"<N><N>
# COCO SSD MobileNet v1 Model
# model_path="model//COCO SSD MobileNet v1 Model/model.tflite"<N><N>

label_path = f'{model_path}/../dict.txt'<N><N>
interpreter = tf.contrib.lite.Interpreter(model_path=model_path)
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
# print(input_details)
# print(output_details)<N><N>
interpreter.allocate_tensors()<N><N>

def draw_rect(image, box, detect_name):
    height, width, c = image.shape
    y_min = int(max(1, (box[0] * height)))
    x_min = int(max(1, (box[1] * width)))
    y_max = int(min(height, (box[2] * height)))
    x_max = int(min(width, (box[3] * width)))
    # draw a rectangle on the image
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), 1)
    cv2.putText(image, detect_name, (x_min, y_min), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)<N><N>

def create_category_index(label_path=label_path):
    f = open(label_path)
    category_index = {}
    for i, val in enumerate(f):
        if i != 0:
            val = val[:-1]
            if val != '???':
                category_index.update({(i - 1): {'id': (i - 1), 'name': val}})<N><N>
    f.close()
    return category_index<N><N>

category_index = create_category_index(label_path)<N><N>
while True:
    ret, frame = cap.read()
    new_img = cv2.resize(frame, (320, 320))
    interpreter.set_tensor(input_details[0]['index'], [new_img])<N><N>
    interpreter.invoke()
    rects = interpreter.get_tensor(
        output_details[0]['index'])<N><N>
    classes = interpreter.get_tensor(
        output_details[1]['index'])<N><N>
    scores = interpreter.get_tensor(
        output_details[2]['index'])<N><N>

    for index, score in enumerate(scores[0]):
        if score > 0.6:
            detect_name = category_index.get(int(classes[0][index])).get('name')
            draw_rect(new_img, rects[0][index], detect_name)<N><N>
    cv2.imshow("image", new_img)
    k = cv2.waitKey(30) & 0xff
    if k == 27:  # Esc 키를 누르면 종료
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import sys
import glob
import cv2
import numpy as np
import matplotlib.pyplot as plt<N><N>
data_path = sys.argv[1]  # Data directory path. Should contain camera intrinsics in `K.txt`.
fnames = glob.glob(f'{data_path}/*.jpg')
fnames.sort()  # Order of capture is important for incremental SFM!<N><N>
K = np.loadtxt(f'{data_path}/K.txt', dtype=np.float)  # Load intrinsics
kp_prev, desc_prev = None, None
Rt_prev = np.append(np.eye(3), np.zeros((3, 1)), axis=-1)  # Init extrinsics for current frame
Rt = np.zeros((3, 4))  # Init extrinsics for next frame
P_prev = K @ Rt_prev  # Init projection matrix for current frame
P = np.zeros((3, 4))  # Init projection matrix for next frame
pt_cld = np.empty((3, 1))<N><N>
for i in range(len(fnames)):
    img = cv2.imread(fnames[i])
    det = cv2.xfeatures2d.SIFT_create()  # Init SIFT detector
    kp, desc = det.detectAndCompute(img, None)  # Extract keypoints & their descriptors
    if i == 0:  # If first frame, update and skip
        kp_prev, desc_prev = kp, desc
        continue<N><N>
import numpy as np
import cv2<N><N>
cap = cv2.VideoCapture(0)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')<N><N>
while True:
    ret, frame = cap.read()<N><N>
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 5)
        roi_gray = gray[y:y+w, x:x+w]
        roi_color = frame[y:y+h, x:x+w]
        eyes = eye_cascade.detectMultiScale(roi_gray, 1.3, 5)
        for (ex, ey, ew, eh) in eyes:
            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 5)<N><N>
    cv2.imshow('frame', frame)<N><N>
    if cv2.waitKey(1) == ord('q'):
        break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import cv2
import mediapipe as mp
import time<N><N>

class handDetector():
    def __init__(self, mode=False, maxHands=2, detectCon=0.5, trackCon=0.5):   #maxhands can be changed according to requirements
        self.mode = mode
        self.maxHands = maxHands
        self.detectCon = detectCon
        self.trackCon = trackCon<N><N>
        self.mpHands = mp.solutions.hands
        self.hands = self.mpHands.Hands(self.mode, self.maxHands, self.detectCon, self.trackCon)
        self.mpDraw = mp.solutions.drawing_utils  # method in mp to draw lines between each point on hand<N><N>
    def findHands(self, img, draw=True):
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        self.results = self.hands.process(imgRGB)<N><N>
        if self.results.multi_hand_landmarks:
            for handLms in self.results.multi_hand_landmarks:  # for each hand detected this will run the loop
                if draw:
                    self.mpDraw.draw_landmarks(img, handLms,
                                               self.mpHands.HAND_CONNECTIONS)  # not taking the RGB we will be displaying these lines on BGR (original)
        return img<N><N>
    def findPosition(self, img, handNo=0, draw=True):
        lmList = []<N><N>
        if self.results.multi_hand_landmarks:
            myHand = self.results.multi_hand_landmarks[handNo]
            for id, lm in enumerate(myHand.landmark):  # will give id and location of the hand
                h, w, c = img.shape
                cx, cy = int(lm.x * w), int(lm.y * h)  # positon of center , multiplying with the value to find pixel location<N><N>
                lmList.append([id, cx, cy])          #each position getting stored in list
                if draw and id == 4:                                      #for the 4th point i.e the tip of thumb
                    cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)     #drawing circles on the ID mentioned (4)
        return lmList<N><N>
#python drowsiness_yawn.py --webcam 0 --alarm alarm.wav<N><N>
from scipy.spatial import distance as dist
from imutils.video import VideoStream
from imutils import face_utils
from threading import Thread
import numpy as np
import argparse
import imutils
import time
import dlib
import cv2
import os
import playsound
from twilio.rest import Client<N><N>
import geocoder
g = geocoder.ip('me')
print(str(g.latlng))<N><N>
account_sid = 'GET FROM YOUR TWILIO ACCOUNT'
auth_token = 'GET FROM YOUR TWILIO ACCOUNT'
client = Client(account_sid, auth_token)<N><N>
def sound_alarm(path):
	# play an alarm sound
	playsound.playsound('alarm.wav', False)<N><N>
def eye_aspect_ratio(eye):
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    C = dist.euclidean(eye[0], eye[3])<N><N>
    ear = (A + B) / (2.0 * C)
    return ear<N><N>
def final_ear(shape):
    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]<N><N>
    leftEye = shape[lStart:lEnd]
    rightEye = shape[rStart:rEnd]<N><N>
    leftEAR = eye_aspect_ratio(leftEye)
    rightEAR = eye_aspect_ratio(rightEye)<N><N>
    ear = (leftEAR + rightEAR) / 2.0
    return (ear, leftEye, rightEye)<N><N>
def lip_distance(shape):
    top_lip = shape[50:53]
    top_lip = np.concatenate((top_lip, shape[61:64]))<N><N>
    low_lip = shape[56:59]
    low_lip = np.concatenate((low_lip, shape[65:68]))<N><N>
    top_mean = np.mean(top_lip, axis=0)
    low_mean = np.mean(low_lip, axis=0)<N><N>
    distance = abs(top_mean[1] - low_mean[1])
    return distance<N><N>

ap = argparse.ArgumentParser()
##ap.add_argument("-p", "--shape-predictor", required=True, help="path to facial landmark predictor")
ap.add_argument("-a", "--alarm", type=str, default="", help="path alarm .WAV file")
ap.add_argument("-w", "--webcam", type=int, default=0, help="index of webcam on system")
args = vars(ap.parse_args())
 
EYE_AR_THRESH = 0.3
EYE_AR_CONSEC_FRAMES = 30
YAWN_THRESH = 20
sms_eye = 1
sms_yawn = 1<N><N>
# initialize the frame counter as well as a boolean used to
# indicate if the alarm is going off
COUNTER = 0
ALARM_ON = False<N><N>
print("-> Loading the predictor and detector...")
#detector = dlib.get_frontal_face_detector()
detector = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")    #Faster but less accurate
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')<N><N>

print("-> Starting Video Stream")
vs = VideoStream(src=args["webcam"]).start()
#vs= VideoStream(usePiCamera=True).start()       //For Raspberry Pi
time.sleep(1.0)<N><N>
while True:<N><N>
    frame = vs.read()
    frame = imutils.resize(frame, width=450)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<N><N>
    #rects = detector(gray, 0)
    rects = detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)<N><N>
    #for rect in rects:
    for (x, y, w, h) in rects:
        rect = dlib.rectangle(int(x), int(y), int(x + w),int(y + h))
        
        shape = predictor(gray, rect)
        shape = face_utils.shape_to_np(shape)<N><N>
        eye = final_ear(shape)
        ear = eye[0]
        leftEye = eye [1]
        rightEye = eye[2]<N><N>
        distance = lip_distance(shape)<N><N>
        leftEyeHull = cv2.convexHull(leftEye)
        rightEyeHull = cv2.convexHull(rightEye)
        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)
        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)<N><N>
        lip = shape[48:60]
        cv2.drawContours(frame, [lip], -1, (0, 255, 0), 1)<N><N>
        if ear < EYE_AR_THRESH:
            COUNTER += 1<N><N>
"""
Simulate a quadrotor following a 3D trajectory<N><N>
Author: Daniel Ingram (daniel-s-ingram)
"""<N><N>
from math import cos, sin
import numpy as np
from Quadrotor import Quadrotor
from TrajectoryGenerator import TrajectoryGenerator
from mpl_toolkits.mplot3d import Axes3D<N><N>
show_animation = True<N><N>
# Simulation parameters
g = 9.81
m = 0.2
Ixx = 1
Iyy = 1
Izz = 1
T = 5<N><N>
# Proportional coefficients
Kp_x = 1
Kp_y = 1
Kp_z = 1
Kp_roll = 25
Kp_pitch = 25
Kp_yaw = 25<N><N>
# Derivative coefficients
Kd_x = 10
Kd_y = 10
Kd_z = 1<N><N>

def quad_sim(x_c, y_c, z_c):
    """
    Calculates the necessary thrust and torques for the quadrotor to
    follow the trajectory described by the sets of coefficients
    x_c, y_c, and z_c.
    """
    x_pos = -5
    y_pos = -5
    z_pos = 5
    x_vel = 0
    y_vel = 0
    z_vel = 0
    x_acc = 0
    y_acc = 0
    z_acc = 0
    roll = 0
    pitch = 0
    yaw = 0
    roll_vel = 0
    pitch_vel = 0
    yaw_vel = 0<N><N>
    des_yaw = 0<N><N>
    dt = 0.1
    t = 0<N><N>
    q = Quadrotor(x=x_pos, y=y_pos, z=z_pos, roll=roll,
                  pitch=pitch, yaw=yaw, size=1, show_animation=show_animation)<N><N>
    i = 0
    n_run = 8
    irun = 0<N><N>
"""
Class for plotting a quadrotor<N><N>
Author: Daniel Ingram (daniel-s-ingram)
"""<N><N>
from math import cos, sin
import numpy as np
import matplotlib.pyplot as plt<N><N>
class Quadrotor():
    def __init__(self, x=0, y=0, z=0, roll=0, pitch=0, yaw=0, size=0.25, show_animation=True):
        self.p1 = np.array([size / 2, 0, 0, 1]).T
        self.p2 = np.array([-size / 2, 0, 0, 1]).T
        self.p3 = np.array([0, size / 2, 0, 1]).T
        self.p4 = np.array([0, -size / 2, 0, 1]).T<N><N>
        self.x_data = []
        self.y_data = []
        self.z_data = []
        self.show_animation = show_animation<N><N>
        if self.show_animation:
            plt.ion()
            fig = plt.figure()
            # for stopping simulation with the esc key.
            fig.canvas.mpl_connect('key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])<N><N>
            self.ax = fig.add_subplot(111, projection='3d')<N><N>
        self.update_pose(x, y, z, roll, pitch, yaw)<N><N>
    def update_pose(self, x, y, z, roll, pitch, yaw):
        self.x = x
        self.y = y
        self.z = z
        self.roll = roll
        self.pitch = pitch
        self.yaw = yaw
        self.x_data.append(x)
        self.y_data.append(y)
        self.z_data.append(z)<N><N>
        if self.show_animation:
            self.plot()<N><N>
"""
Generates a quintic polynomial trajectory.<N><N>
Author: Daniel Ingram (daniel-s-ingram)
"""<N><N>
import numpy as np<N><N>
class TrajectoryGenerator():
    def __init__(self, start_pos, des_pos, T, start_vel=[0,0,0], des_vel=[0,0,0], start_acc=[0,0,0], des_acc=[0,0,0]):
        self.start_x = start_pos[0]
        self.start_y = start_pos[1]
        self.start_z = start_pos[2]<N><N>
        self.des_x = des_pos[0]
        self.des_y = des_pos[1]
        self.des_z = des_pos[2]<N><N>
        self.start_x_vel = start_vel[0]
        self.start_y_vel = start_vel[1]
        self.start_z_vel = start_vel[2]<N><N>
        self.des_x_vel = des_vel[0]
        self.des_y_vel = des_vel[1]
        self.des_z_vel = des_vel[2]<N><N>
        self.start_x_acc = start_acc[0]
        self.start_y_acc = start_acc[1]
        self.start_z_acc = start_acc[2]<N><N>
        self.des_x_acc = des_acc[0]
        self.des_y_acc = des_acc[1]
        self.des_z_acc = des_acc[2]<N><N>
        self.T = T<N><N>
    def solve(self):
        A = np.array(
            [[0, 0, 0, 0, 0, 1],
             [self.T**5, self.T**4, self.T**3, self.T**2, self.T, 1],
             [0, 0, 0, 0, 1, 0],
             [5*self.T**4, 4*self.T**3, 3*self.T**2, 2*self.T, 1, 0],
             [0, 0, 0, 2, 0, 0],
             [20*self.T**3, 12*self.T**2, 6*self.T, 2, 0, 0]
            ])<N><N>
        b_x = np.array(
            [[self.start_x],
             [self.des_x],
             [self.start_x_vel],
             [self.des_x_vel],
             [self.start_x_acc],
             [self.des_x_acc]
            ])<N><N>
        b_y = np.array(
            [[self.start_y],
             [self.des_y],
             [self.start_y_vel],
             [self.des_y_vel],
             [self.start_y_acc],
             [self.des_y_acc]
            ])<N><N>
        b_z = np.array(
            [[self.start_z],
             [self.des_z],
             [self.start_z_vel],
             [self.des_z_vel],
             [self.start_z_acc],
             [self.des_z_acc]
            ])<N><N>
        self.x_c = np.linalg.solve(A, b_x)
        self.y_c = np.linalg.solve(A, b_y)
        self.z_c = np.linalg.solve(A, b_z)<N><N>
import os
import sys<N><N>sys.path.append(os.path.dirname(os.path.abspath(__file__)))

"""<N><N>A rocket powered landing with successive convexification<N><N>
author: Sven Niederberger
        Atsushi Sakai<N><N>
Ref:
- Python implementation of 'Successive Convexification for 6-DoF Mars Rocket Powered Landing with Free-Final-Time' paper
by Michael Szmuk and Behcet Acıkmese.<N><N>
- EmbersArc/SuccessiveConvexificationFreeFinalTime: Implementation of "Successive Convexification for 6-DoF Mars Rocket Powered Landing with Free-Final-Time" https://github.com/EmbersArc/SuccessiveConvexificationFreeFinalTime<N><N>
"""<N><N>from time import time
import numpy as np
from scipy.integrate import odeint
import cvxpy
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d<N><N>
# Trajectory points
K = 50<N><N>
# Max solver iterations
iterations = 30<N><N>
# Weight constants
W_SIGMA = 1  # flight time
W_DELTA = 1e-3  # difference in state/input
W_DELTA_SIGMA = 1e-1  # difference in flight time
W_NU = 1e5  # virtual control<N><N>
solver = 'ECOS'
verbose_solver = False<N><N>
show_animation = True<N><N>

class Rocket_Model_6DoF:
    """
    A 6 degree of freedom rocket landing problem.
    """<N><N>
    def __init__(self):
        """
        A large r_scale for a small scale problem will
        ead to numerical problems as parameters become excessively small
        and (it seems) precision is lost in the dynamics.
        """
        self.n_x = 14
        self.n_u = 3<N><N>
        # Mass
        self.m_wet = 3.0  # 30000 kg
        self.m_dry = 2.2  # 22000 kg<N><N>
        # Flight time guess
        self.t_f_guess = 10.0  # 10 s<N><N>
        # State constraints
        self.r_I_final = np.array((0., 0., 0.))
        self.v_I_final = np.array((-1e-1, 0., 0.))
        self.q_B_I_final = self.euler_to_quat((0, 0, 0))
        self.w_B_final = np.deg2rad(np.array((0., 0., 0.)))<N><N>
        self.w_B_max = np.deg2rad(60)<N><N>
        # Angles
        max_gimbal = 20
        max_angle = 90
        glidelslope_angle = 20<N><N>
        self.tan_delta_max = np.tan(np.deg2rad(max_gimbal))
        self.cos_theta_max = np.cos(np.deg2rad(max_angle))
        self.tan_gamma_gs = np.tan(np.deg2rad(glidelslope_angle))<N><N>
        # Thrust limits
        self.T_max = 5.0
        self.T_min = 0.3<N><N>
        # Angular moment of inertia
        self.J_B = 1e-2 * np.diag([1., 1., 1.])<N><N>
        # Gravity
        self.g_I = np.array((-1, 0., 0.))<N><N>
        # Fuel consumption
        self.alpha_m = 0.01<N><N>
        # Vector from thrust point to CoM
        self.r_T_B = np.array([-1e-2, 0., 0.])<N><N>
        self.set_random_initial_state()<N><N>
        self.x_init = np.concatenate(
            ((self.m_wet,), self.r_I_init, self.v_I_init, self.q_B_I_init, self.w_B_init))
        self.x_final = np.concatenate(
            ((self.m_dry,), self.r_I_final, self.v_I_final, self.q_B_I_final, self.w_B_final))<N><N>
        self.r_scale = np.linalg.norm(self.r_I_init)
        self.m_scale = self.m_wet<N><N>
    def set_random_initial_state(self):
        self.r_I_init = np.array((0., 0., 0.))
        self.r_I_init[0] = np.random.uniform(3, 4)
        self.r_I_init[1:3] = np.random.uniform(-2, 2, size=2)<N><N>
        self.v_I_init = np.array((0., 0., 0.))
        self.v_I_init[0] = np.random.uniform(-1, -0.5)
        self.v_I_init[1:3] = np.random.uniform(
            -0.5, -0.2, size=2) * self.r_I_init[1:3]<N><N>
        self.q_B_I_init = self.euler_to_quat((0,
                                              np.random.uniform(-30, 30),
                                              np.random.uniform(-30, 30)))
        self.w_B_init = np.deg2rad((0,
                                    np.random.uniform(-20, 20),
                                    np.random.uniform(-20, 20)))<N><N>
    def f_func(self, x, u):
        m, rx, ry, rz, vx, vy, vz, q0, q1, q2, q3, wx, wy, wz = x[0], x[1], x[
            2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], x[11], x[12], x[13]
        ux, uy, uz = u[0], u[1], u[2]<N><N>
"""
Obstacle navigation using A* on a toroidal grid<N><N>
Author: Daniel Ingram (daniel-s-ingram)
"""
from math import pi
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import from_levels_and_colors<N><N>
plt.ion()<N><N>
# Simulation parameters
M = 100
obstacles = [[1.75, 0.75, 0.6], [0.55, 1.5, 0.5], [0, -1, 0.25]]<N><N>

def main():
    arm = NLinkArm([1, 1], [0, 0])
    start = (10, 50)
    goal = (58, 56)
    grid = get_occupancy_grid(arm, obstacles)
    plt.imshow(grid)
    plt.show()
    route = astar_torus(grid, start, goal)
    for node in route:
        theta1 = 2 * pi * node[0] / M - pi
        theta2 = 2 * pi * node[1] / M - pi
        arm.update_joints([theta1, theta2])
        arm.plot(obstacles=obstacles)<N><N>

def detect_collision(line_seg, circle):
    """
    Determines whether a line segment (arm link) is in contact
    with a circle (obstacle).
    Credit to: http://doswa.com/2009/07/13/circle-segment-intersectioncollision.html
    Args:
        line_seg: List of coordinates of line segment endpoints e.g. [[1, 1], [2, 2]]
        circle: List of circle coordinates and radius e.g. [0, 0, 0.5] is a circle centered
                at the origin with radius 0.5<N><N>
"""
Obstacle navigation using A* on a toroidal grid<N><N>
Author: Daniel Ingram (daniel-s-ingram)
        Tullio Facchinetti (tullio.facchinetti@unipv.it)
"""
from math import pi
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import from_levels_and_colors
import sys<N><N>
plt.ion()<N><N>
# Simulation parameters
M = 100
obstacles = [[1.75, 0.75, 0.6], [0.55, 1.5, 0.5], [0, -1, 0.7]]<N><N>

def press(event):
    """Exit from the simulation."""
    if event.key == 'q' or event.key == 'Q':
        print('Quitting upon request.')
        sys.exit(0)<N><N>

def main():
    # Arm geometry in the working space
    link_length = [0.5, 1.5]
    initial_link_angle = [0, 0]
    arm = NLinkArm(link_length, initial_link_angle)
    # (x, y) co-ordinates in the joint space [cell]
    start = (10, 50)
    goal = (58, 56)
    grid = get_occupancy_grid(arm, obstacles)
    route = astar_torus(grid, start, goal)
    if len(route) >= 0:
        animate(grid, arm, route)<N><N>
"""
Class of n-link arm in 3D
Author: Takayuki Murooka (takayuki5168)
"""
import numpy as np
import math
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt<N><N>

class Link:
    def __init__(self, dh_params):
        self.dh_params_ = dh_params<N><N>
    def transformation_matrix(self):
        theta = self.dh_params_[0]
        alpha = self.dh_params_[1]
        a = self.dh_params_[2]
        d = self.dh_params_[3]<N><N>
        st = math.sin(theta)
        ct = math.cos(theta)
        sa = math.sin(alpha)
        ca = math.cos(alpha)
        trans = np.array([[ct, -st * ca, st * sa, a * ct],
                          [st, ct * ca, -ct * sa, a * st],
                          [0, sa, ca, d],
                          [0, 0, 0, 1]])<N><N>
        return trans<N><N>
    @staticmethod
    def basic_jacobian(trans_prev, ee_pos):
        pos_prev = np.array(
            [trans_prev[0, 3], trans_prev[1, 3], trans_prev[2, 3]])
        z_axis_prev = np.array(
            [trans_prev[0, 2], trans_prev[1, 2], trans_prev[2, 2]])<N><N>
        basic_jacobian = np.hstack(
            (np.cross(z_axis_prev, ee_pos - pos_prev), z_axis_prev))
        return basic_jacobian<N><N>

class NLinkArm:
    def __init__(self, dh_params_list):
        self.link_list = []
        for i in range(len(dh_params_list)):
            self.link_list.append(Link(dh_params_list[i]))<N><N>
    def transformation_matrix(self):
        trans = np.identity(4)
        for i in range(len(self.link_list)):
            trans = np.dot(trans, self.link_list[i].transformation_matrix())
        return trans<N><N>
    def forward_kinematics(self, plot=False):
        trans = self.transformation_matrix()<N><N>
        x = trans[0, 3]
        y = trans[1, 3]
        z = trans[2, 3]
        alpha, beta, gamma = self.euler_angle()<N><N>
        if plot:
            self.fig = plt.figure()
            self.ax = Axes3D(self.fig)<N><N>
            x_list = []
            y_list = []
            z_list = []<N><N>
            trans = np.identity(4)<N><N>
            x_list.append(trans[0, 3])
            y_list.append(trans[1, 3])
            z_list.append(trans[2, 3])
            for i in range(len(self.link_list)):
                trans = np.dot(trans, self.link_list[i].transformation_matrix())
                x_list.append(trans[0, 3])
                y_list.append(trans[1, 3])
                z_list.append(trans[2, 3])<N><N>
            self.ax.plot(x_list, y_list, z_list, "o-", color="#00aa00", ms=4,
                         mew=0.5)
            self.ax.plot([0], [0], [0], "o")<N><N>
            self.ax.set_xlim(-1, 1)
            self.ax.set_ylim(-1, 1)
            self.ax.set_zlim(-1, 1)<N><N>
            plt.show()<N><N>
        return [x, y, z, alpha, beta, gamma]<N><N>
    def basic_jacobian(self):
        ee_pos = self.forward_kinematics()[0:3]
        basic_jacobian_mat = []<N><N>
        trans = np.identity(4)
        for i in range(len(self.link_list)):
            basic_jacobian_mat.append(
                self.link_list[i].basic_jacobian(trans, ee_pos))
            trans = np.dot(trans, self.link_list[i].transformation_matrix())<N><N>
        return np.array(basic_jacobian_mat).T<N><N>
    def inverse_kinematics(self, ref_ee_pose, plot=False):
        for cnt in range(500):
            ee_pose = self.forward_kinematics()
            diff_pose = np.array(ref_ee_pose) - ee_pose<N><N>
            basic_jacobian_mat = self.basic_jacobian()
            alpha, beta, gamma = self.euler_angle()<N><N>
            K_zyz = np.array(
                [[0, -math.sin(alpha), math.cos(alpha) * math.sin(beta)],
                 [0, math.cos(alpha), math.sin(alpha) * math.sin(beta)],
                 [1, 0, math.cos(beta)]])
            K_alpha = np.identity(6)
            K_alpha[3:, 3:] = K_zyz<N><N>
            theta_dot = np.dot(
                np.dot(np.linalg.pinv(basic_jacobian_mat), K_alpha),
                np.array(diff_pose))
            self.update_joint_angles(theta_dot / 100.)<N><N>
        if plot:
            self.fig = plt.figure()
            self.ax = Axes3D(self.fig)<N><N>
            x_list = []
            y_list = []
            z_list = []<N><N>
            trans = np.identity(4)<N><N>
            x_list.append(trans[0, 3])
            y_list.append(trans[1, 3])
            z_list.append(trans[2, 3])
            for i in range(len(self.link_list)):
                trans = np.dot(trans, self.link_list[i].transformation_matrix())
                x_list.append(trans[0, 3])
                y_list.append(trans[1, 3])
                z_list.append(trans[2, 3])<N><N>
            self.ax.plot(x_list, y_list, z_list, "o-", color="#00aa00", ms=4,
                         mew=0.5)
            self.ax.plot([0], [0], [0], "o")<N><N>
            self.ax.set_xlim(-1, 1)
            self.ax.set_ylim(-1, 1)
            self.ax.set_zlim(-1, 1)<N><N>
            self.ax.plot([ref_ee_pose[0]], [ref_ee_pose[1]], [ref_ee_pose[2]],
                         "o")
            plt.show()<N><N>
    def euler_angle(self):
        trans = self.transformation_matrix()<N><N>
"""
Forward Kinematics for an n-link arm in 3D
Author: Takayuki Murooka (takayuki5168)
"""
import math
from NLinkArm3d import NLinkArm
import random<N><N>

def random_val(min_val, max_val):
    return min_val + random.random() * (max_val - min_val)<N><N>
"""
Inverse Kinematics for an n-link arm in 3D
Author: Takayuki Murooka (takayuki5168)
"""
import math
from NLinkArm3d import NLinkArm
import random<N><N>

def random_val(min_val, max_val):
    return min_val + random.random() * (max_val - min_val)<N><N>
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

"""
Class for controlling and plotting an arm with an arbitrary number of links.<N><N>
Author: Daniel Ingram
"""<N><N>
import numpy as np
import matplotlib.pyplot as plt<N><N>

class NLinkArm(object):
    def __init__(self, link_lengths, joint_angles, goal, show_animation):
        self.show_animation = show_animation
        self.n_links = len(link_lengths)
        if self.n_links != len(joint_angles):
            raise ValueError()<N><N>
        self.link_lengths = np.array(link_lengths)
        self.joint_angles = np.array(joint_angles)
        self.points = [[0, 0] for _ in range(self.n_links + 1)]<N><N>
        self.lim = sum(link_lengths)
        self.goal = np.array(goal).T<N><N>
        if show_animation:  # pragma: no cover
            self.fig = plt.figure()
            self.fig.canvas.mpl_connect('button_press_event', self.click)<N><N>
            plt.ion()
            plt.show()<N><N>
        self.update_points()<N><N>
    def update_joints(self, joint_angles):
        self.joint_angles = joint_angles<N><N>
        self.update_points()<N><N>
    def update_points(self):
        for i in range(1, self.n_links + 1):
            self.points[i][0] = self.points[i - 1][0] + \
                self.link_lengths[i - 1] * \
                np.cos(np.sum(self.joint_angles[:i]))
            self.points[i][1] = self.points[i - 1][1] + \
                self.link_lengths[i - 1] * \
                np.sin(np.sum(self.joint_angles[:i]))<N><N>
        self.end_effector = np.array(self.points[self.n_links]).T
        if self.show_animation:  # pragma: no cover
            self.plot()<N><N>
    def plot(self):  # pragma: no cover
        plt.cla()
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect('key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])<N><N>
        for i in range(self.n_links + 1):
            if i is not self.n_links:
                plt.plot([self.points[i][0], self.points[i + 1][0]],
                         [self.points[i][1], self.points[i + 1][1]], 'r-')
            plt.plot(self.points[i][0], self.points[i][1], 'ko')<N><N>
        plt.plot(self.goal[0], self.goal[1], 'gx')<N><N>
        plt.plot([self.end_effector[0], self.goal[0]], [
                 self.end_effector[1], self.goal[1]], 'g--')<N><N>
        plt.xlim([-self.lim, self.lim])
        plt.ylim([-self.lim, self.lim])
        plt.draw()
        plt.pause(0.0001)<N><N>
    def click(self, event):
        self.goal = np.array([event.xdata, event.ydata]).T
        self.plot()
<N><N>
"""
Inverse kinematics for an n-link arm using the Jacobian inverse method<N><N>
Author: Daniel Ingram (daniel-s-ingram)
        Atsushi Sakai (@Atsushi_twi)
"""
import numpy as np<N><N>
from ArmNavigation.n_joint_arm_to_point_control.NLinkArm import NLinkArm<N><N>
# Simulation parameters
Kp = 2
dt = 0.1
N_LINKS = 10
N_ITERATIONS = 10000<N><N>
# States
WAIT_FOR_NEW_GOAL = 1
MOVING_TO_GOAL = 2<N><N>
show_animation = True<N><N>
"""
RRT* path planner for a seven joint arm
Author: Mahyar Abdeetedal (mahyaret)
"""
import math
import os
import sys
import random
import numpy as np
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../n_joint_arm_3d/")
try:
    from NLinkArm3d import NLinkArm
except ImportError:
    raise<N><N>
show_animation = True
verbose = False<N><N>

class RobotArm(NLinkArm):
    def get_points(self, joint_angle_list):
        self.set_joint_angles(joint_angle_list)<N><N>
        x_list = []
        y_list = []
        z_list = []<N><N>
        trans = np.identity(4)<N><N>
        x_list.append(trans[0, 3])
        y_list.append(trans[1, 3])
        z_list.append(trans[2, 3])
        for i in range(len(self.link_list)):
            trans = np.dot(trans, self.link_list[i].transformation_matrix())
            x_list.append(trans[0, 3])
            y_list.append(trans[1, 3])
            z_list.append(trans[2, 3])<N><N>
        return x_list, y_list, z_list<N><N>

class RRTStar:
    """
    Class for RRT Star planning
    """<N><N>
    class Node:
        def __init__(self, x):
            self.x = x
            self.parent = None
            self.cost = 0.0<N><N>
    def __init__(self, start, goal, robot, obstacle_list, rand_area,
                 expand_dis=.30,
                 path_resolution=.1,
                 goal_sample_rate=20,
                 max_iter=300,
                 connect_circle_dist=50.0
                 ):
        """
        Setting Parameter<N><N>
        start:Start Position [q1,...,qn]
        goal:Goal Position [q1,...,qn]
        obstacleList:obstacle Positions [[x,y,z,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
"""
Inverse kinematics of a two-joint arm
Left-click the plot to set the goal position of the end effector<N><N>
Author: Daniel Ingram (daniel-s-ingram)
        Atsushi Sakai (@Atsushi_twi)<N><N>
Ref: P. I. Corke, "Robotics, Vision & Control", Springer 2017,
 ISBN 978-3-319-54413-7 p102
- [Robotics, Vision and Control]
(https://link.springer.com/book/10.1007/978-3-642-20144-8)<N><N>
"""<N><N>import matplotlib.pyplot as plt
import numpy as np<N><N>

# Similation parameters
Kp = 15
dt = 0.01<N><N>
# Link lengths
l1 = l2 = 1<N><N>
# Set initial goal position to the initial end-effector position
x = 2
y = 0<N><N>
show_animation = True<N><N>
if show_animation:
    plt.ion()<N><N>
"""
Bipedal Walking with modifying designated footsteps
author: Takayuki Murooka (takayuki5168)
"""
import numpy as np
import math
from matplotlib import pyplot as plt
import matplotlib.patches as pat
from mpl_toolkits.mplot3d import Axes3D
import mpl_toolkits.mplot3d.art3d as art3d<N><N>

class BipedalPlanner(object):
    def __init__(self):
        self.act_p = []  # actual footstep positions
        self.ref_p = []  # reference footstep positions
        self.com_trajectory = []
        self.ref_footsteps = None
        self.g = 9.8<N><N>
    def set_ref_footsteps(self, ref_footsteps):
        self.ref_footsteps = ref_footsteps<N><N>
    def inverted_pendulum(self, x, x_dot, px_star, y, y_dot, py_star, z_c,
                          time_width):
        time_split = 100<N><N>
        for i in range(time_split):
            delta_time = time_width / time_split<N><N>
            x_dot2 = self.g / z_c * (x - px_star)
            x += x_dot * delta_time
            x_dot += x_dot2 * delta_time<N><N>
            y_dot2 = self.g / z_c * (y - py_star)
            y += y_dot * delta_time
            y_dot += y_dot2 * delta_time<N><N>
            if i % 10 == 0:
                self.com_trajectory.append([x, y])<N><N>
        return x, x_dot, y, y_dot<N><N>
    def walk(self, t_sup=0.8, z_c=0.8, a=10, b=1, plot=False):
        if self.ref_footsteps is None:
            print("No footsteps")
            return<N><N>
        # set up plotter
        com_trajectory_for_plot, ax = None, None
        if plot:
            fig = plt.figure()
            ax = Axes3D(fig)
            com_trajectory_for_plot = []<N><N>
# -*- coding: utf-8 -*-
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config<N><N>
# -- Path setup --------------------------------------------------------------<N><N>
# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))<N><N>

# -- Project information -----------------------------------------------------<N><N>
project = 'PythonRobotics'
copyright = '2018, Atsushi Sakai'
author = 'Atsushi Sakai'<N><N>
# The short X.Y version
version = ''
# The full version, including alpha/beta/rc tags
release = ''<N><N>

# -- General configuration ---------------------------------------------------<N><N>
# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'<N><N>
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.mathjax',
    'sphinx.ext.viewcode',
    'sphinx.ext.viewcode',
]<N><N>
# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']<N><N>
# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'<N><N>
# The master toctree document.
master_doc = 'index'<N><N>
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = None<N><N>
# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path .
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']<N><N>
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'<N><N>

# -- Options for HTML output -------------------------------------------------<N><N>
# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
# Fix for read the docs
on_rtd = os.environ.get('READTHEDOCS') == 'True'
if on_rtd:
    html_theme = 'default'
else:
    html_theme = 'sphinx_rtd_theme'<N><N>
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
html_logo = '../icon.png'
html_theme_options = {
    'display_version': False,
}<N><N>
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']<N><N>
# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}<N><N>

# -- Options for HTMLHelp output ---------------------------------------------<N><N>
# Output file base name for HTML help builder.
htmlhelp_basename = 'PythonRoboticsdoc'<N><N>

# -- Options for LaTeX output ------------------------------------------------<N><N>
latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',<N><N>
    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',<N><N>
    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',<N><N>
    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}<N><N>
# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'PythonRobotics.tex', 'PythonRobotics Documentation',
     'Atsushi Sakai', 'manual'),
]<N><N>

# -- Options for manual page output ------------------------------------------<N><N>
# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    (master_doc, 'pythonrobotics', 'PythonRobotics Documentation',
     [author], 1)
]<N><N>

# -- Options for Texinfo output ----------------------------------------------<N><N>
# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'PythonRobotics', 'PythonRobotics Documentation',
     author, 'PythonRobotics', 'One line description of project.',
     'Miscellaneous'),
]<N><N>

# -- Extension configuration -------------------------------------------------
<N><N>
"""<N><N>Jupyter notebook converter to rst file<N><N>
author: Atsushi Sakai<N><N>
"""
import subprocess
import os.path
import os
import glob<N><N>

NOTEBOOK_DIR = "../"<N><N>

def get_notebook_path_list(ndir):
    path = glob.glob(ndir + "**/*.ipynb", recursive=True)
    return path<N><N>

def convert_rst(rstpath):<N><N>
    with open(rstpath, "r") as bfile:
        filedata = bfile.read()<N><N>
        # convert from code directive to code-block
        # because showing code in Sphinx
        before = ".. code:: ipython3"
        after = ".. code-block:: ipython3"
        filedata = filedata.replace(before, after)<N><N>
    with open(rstpath, "w") as afile:
        afile.write(filedata)<N><N>

def generate_rst(npath):
    print("====Start generating rst======")<N><N>
    # generate dir
    dirpath = os.path.dirname(npath)
    # print(dirpath)<N><N>
    rstpath = os.path.abspath("./modules/" + npath[3:-5] + "rst")
    # print(rstpath)<N><N>
    basename = os.path.basename(rstpath)<N><N>
    cmd = "jupyter nbconvert --to rst "
    cmd += npath
    print(cmd)
    subprocess.call(cmd, shell=True)<N><N>
    rstpath = dirpath + "/" + basename
    convert_rst(rstpath)<N><N>
    # clean up old files
    cmd = "rm -rf "
    cmd += "./modules/"
    cmd += basename[:-4]
    cmd += "*"
    # print(cmd)
    subprocess.call(cmd, shell=True)<N><N>
    # move files to module dir
    cmd = "mv "
    cmd += dirpath
    cmd += "/*.rst ./modules/"
    print(cmd)
    subprocess.call(cmd, shell=True)<N><N>
    cmd = "mv "
    cmd += dirpath
    cmd += "/*_files ./modules/"
    print(cmd)
    subprocess.call(cmd, shell=True)<N><N>

def main():
    print("start!!")<N><N>
    notebook_path_list = get_notebook_path_list(NOTEBOOK_DIR)
    # print(notebook_path_list)<N><N>
    for npath in notebook_path_list:
        if "template" not in npath:
            generate_rst(npath)<N><N>
    print("done!!")<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""
Inverted Pendulum MPC control
author: Atsushi Sakai
"""<N><N>
import math
import time<N><N>
import cvxpy
import matplotlib.pyplot as plt
import numpy as np<N><N>
# Model parameters<N><N>
l_bar = 2.0  # length of bar
M = 1.0  # [kg]
m = 0.3  # [kg]
g = 9.8  # [m/s^2]<N><N>
Q = np.diag([0.0, 1.0, 1.0, 0.0])
R = np.diag([0.01])
nx = 4  # number of state
nu = 1  # number of input
T = 30  # Horizon length
delta_t = 0.1  # time tick<N><N>
animation = True<N><N>

def main():
    x0 = np.array([
        [0.0],
        [0.0],
        [0.3],
        [0.0]
    ])<N><N>
    x = np.copy(x0)<N><N>
    for i in range(50):<N><N>
        # calc control input
        opt_x, opt_delta_x, opt_theta, opt_delta_theta, opt_input = \
            mpc_control(x)<N><N>
        # get input
        u = opt_input[0]<N><N>
        # simulate inverted pendulum cart
        x = simulation(x, u)<N><N>
        if animation:
            plt.clf()
            px = float(x[0])
            theta = float(x[2])
            plot_cart(px, theta)
            plt.xlim([-5.0, 2.0])
            plt.pause(0.001)<N><N>

def simulation(x, u):
    A, B = get_model_matrix()<N><N>
    x = np.dot(A, x) + np.dot(B, u)<N><N>
    return x<N><N>

def mpc_control(x0):
    x = cvxpy.Variable((nx, T + 1))
    u = cvxpy.Variable((nu, T))<N><N>
    A, B = get_model_matrix()<N><N>
    cost = 0.0
    constr = []
    for t in range(T):
        cost += cvxpy.quad_form(x[:, t + 1], Q)
        cost += cvxpy.quad_form(u[:, t], R)
        constr += [x[:, t + 1] == A * x[:, t] + B * u[:, t]]<N><N>
    constr += [x[:, 0] == x0[:, 0]]
    prob = cvxpy.Problem(cvxpy.Minimize(cost), constr)<N><N>
    start = time.time()
    prob.solve(verbose=False)
    elapsed_time = time.time() - start
    print("calc time:{0} [sec]".format(elapsed_time))<N><N>
    if prob.status == cvxpy.OPTIMAL:
        ox = get_numpy_array_from_matrix(x.value[0, :])
        dx = get_numpy_array_from_matrix(x.value[1, :])
        theta = get_numpy_array_from_matrix(x.value[2, :])
        d_theta = get_numpy_array_from_matrix(x.value[3, :])<N><N>
        ou = get_numpy_array_from_matrix(u.value[0, :])
    else:
        ox, dx, theta, d_theta, ou = None, None, None, None, None<N><N>
    return ox, dx, theta, d_theta, ou<N><N>

def get_numpy_array_from_matrix(x):
    """
    get build-in list from matrix
    """
    return np.array(x).flatten()<N><N>

def get_model_matrix():
    A = np.array([
        [0.0, 1.0, 0.0, 0.0],
        [0.0, 0.0, m * g / M, 0.0],
        [0.0, 0.0, 0.0, 1.0],
        [0.0, 0.0, g * (M + m) / (l_bar * M), 0.0]
    ])
    A = np.eye(nx) + delta_t * A<N><N>
    B = np.array([
        [0.0],
        [1.0 / M],
        [0.0],
        [1.0 / (l_bar * M)]
    ])
    B = delta_t * B<N><N>
    return A, B<N><N>

def flatten(a):
    return np.array(a).flatten()<N><N>

def plot_cart(xt, theta):
    cart_w = 1.0
    cart_h = 0.5
    radius = 0.1<N><N>
    cx = np.array([-cart_w / 2.0, cart_w / 2.0, cart_w /
                   2.0, -cart_w / 2.0, -cart_w / 2.0])
    cy = np.array([0.0, 0.0, cart_h, cart_h, 0.0])
    cy += radius * 2.0<N><N>
    cx = cx + xt<N><N>
    bx = np.array([0.0, l_bar * math.sin(-theta)])
    bx += xt
    by = np.array([cart_h, l_bar * math.cos(-theta) + cart_h])
    by += radius * 2.0<N><N>
    angles = np.arange(0.0, math.pi * 2.0, math.radians(3.0))
    ox = np.array([radius * math.cos(a) for a in angles])
    oy = np.array([radius * math.sin(a) for a in angles])<N><N>
    rwx = np.copy(ox) + cart_w / 4.0 + xt
    rwy = np.copy(oy) + radius
    lwx = np.copy(ox) - cart_w / 4.0 + xt
    lwy = np.copy(oy) + radius<N><N>
    wx = np.copy(ox) + bx[-1]
    wy = np.copy(oy) + by[-1]<N><N>
    plt.plot(flatten(cx), flatten(cy), "-b")
    plt.plot(flatten(bx), flatten(by), "-k")
    plt.plot(flatten(rwx), flatten(rwy), "-k")
    plt.plot(flatten(lwx), flatten(lwy), "-k")
    plt.plot(flatten(wx), flatten(wy), "-k")
    plt.title("x:" + str(round(xt, 2)) + ",theta:" +
              str(round(math.degrees(theta), 2)))<N><N>
    plt.axis("equal")<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""
Cubature Kalman filter using Constant Turn Rate and Velocity (CTRV) model
Fuse sensor data from IMU and GPS to obtain accurate position<N><N>
https://ieeexplore.ieee.org/document/4982682<N><N>
Author: Raghuram Shankar<N><N>
state matrix:                       2D x-y position, yaw, velocity and yaw rate
measurement matrix:                 2D x-y position, velocity and yaw rate<N><N>
"""<N><N>Ensemble Kalman Filter(EnKF) localization sample<N><N>
author: Ryohei Sasaki(rsasaki0109)<N><N>
Ref:
Ensemble Kalman filtering
(https://rmets.onlinelibrary.wiley.com/doi/10.1256/qj.05.135)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot<N><N>
#  Simulation parameter
Q_sim = np.diag([0.2, np.deg2rad(1.0)]) ** 2
R_sim = np.diag([1.0, np.deg2rad(30.0)]) ** 2<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]
MAX_RANGE = 20.0  # maximum observation range<N><N>
# Ensemble Kalman filter parameter
NP = 20  # Number of Particle<N><N>
show_animation = True<N><N>

def calc_input():
    v = 1.0  # [m/s]
    yaw_rate = 0.1  # [rad/s]
    u = np.array([[v, yaw_rate]]).T
    return u<N><N>

def observation(xTrue, xd, u, RFID):
    xTrue = motion_model(xTrue, u)<N><N>
    z = np.zeros((0, 4))<N><N>
    for i in range(len(RFID[:, 0])):<N><N>
        dx = RFID[i, 0] - xTrue[0, 0]
        dy = RFID[i, 1] - xTrue[1, 0]
        d = math.hypot(dx, dy)
        angle = pi_2_pi(math.atan2(dy, dx) - xTrue[2, 0])
        if d <= MAX_RANGE:
            dn = d + np.random.randn() * Q_sim[0, 0] ** 0.5  # add noise
            angle_with_noise = angle + np.random.randn() * Q_sim[1, 1] ** 0.5
            zi = np.array([dn, angle_with_noise, RFID[i, 0], RFID[i, 1]])
            z = np.vstack((z, zi))<N><N>
    # add noise to input
    ud = np.array([[
        u[0, 0] + np.random.randn() * R_sim[0, 0] ** 0.5,
        u[1, 0] + np.random.randn() * R_sim[1, 1] ** 0.5]]).T<N><N>
    xd = motion_model(xd, ud)
    return xTrue, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0, 0],
                  [0, 1.0, 0, 0],
                  [0, 0, 1.0, 0],
                  [0, 0, 0, 0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT],
                  [1.0, 0.0]])
    x = F.dot(x) + B.dot(u)<N><N>
    return x<N><N>

def observe_landmark_position(x, landmarks):
    landmarks_pos = np.zeros((2 * landmarks.shape[0], 1))
    for (i, lm) in enumerate(landmarks):
        index = 2 * i
        q = Q_sim[0, 0] ** 0.5
        landmarks_pos[index] = x[0, 0] + lm[0] * math.cos(
            x[2, 0] + lm[1]) + np.random.randn() * q / np.sqrt(2)
        landmarks_pos[index + 1] = x[1, 0] + lm[0] * math.sin(
            x[2, 0] + lm[1]) + np.random.randn() * q / np.sqrt(2)
    return landmarks_pos<N><N>

def calc_covariance(xEst, px):
    cov = np.zeros((3, 3))<N><N>
    for i in range(px.shape[1]):
        dx = (px[:, i] - xEst)[0:3]
        cov += dx.dot(dx.T)
    cov /= NP<N><N>
    return cov<N><N>

def enkf_localization(px, z, u):
    """
    Localization with Ensemble Kalman filter
    """
    pz = np.zeros((z.shape[0] * 2, NP))  # Particle store of z
    for ip in range(NP):
        x = np.array([px[:, ip]]).T<N><N>
        #  Predict with random input sampling
        ud1 = u[0, 0] + np.random.randn() * R_sim[0, 0] ** 0.5
        ud2 = u[1, 0] + np.random.randn() * R_sim[1, 1] ** 0.5
        ud = np.array([[ud1, ud2]]).T
        x = motion_model(x, ud)
        px[:, ip] = x[:, 0]
        z_pos = observe_landmark_position(x, z)
        pz[:, ip] = z_pos[:, 0]<N><N>
    x_ave = np.mean(px, axis=1)
    x_dif = px - np.tile(x_ave, (NP, 1)).T<N><N>
    z_ave = np.mean(pz, axis=1)
    z_dif = pz - np.tile(z_ave, (NP, 1)).T<N><N>
    U = 1 / (NP - 1) * x_dif @ z_dif.T
    V = 1 / (NP - 1) * z_dif @ z_dif.T<N><N>
    K = U @ np.linalg.inv(V)  # Kalman Gain<N><N>
    z_lm_pos = z[:, [2, 3]].reshape(-1, )<N><N>
    px_hat = px + K @ (np.tile(z_lm_pos, (NP, 1)).T - pz)<N><N>
    xEst = np.average(px_hat, axis=1).reshape(4, 1)
    PEst = calc_covariance(xEst, px_hat)<N><N>
    return xEst, PEst, px_hat<N><N>

def plot_covariance_ellipse(xEst, PEst):  # pragma: no cover
    Pxy = PEst[0:2, 0:2]
    eig_val, eig_vec = np.linalg.eig(Pxy)<N><N>
    if eig_val[0] >= eig_val[1]:
        big_ind = 0
        small_ind = 1
    else:
        big_ind = 1
        small_ind = 0<N><N>
    t = np.arange(0, 2 * math.pi + 0.1, 0.1)<N><N>
    # eig_val[big_ind] or eiq_val[small_ind] were occasionally negative
    # numbers extremely close to 0 (~10^-20), catch these cases and set
    # the respective variable to 0
    try:
        a = math.sqrt(eig_val[big_ind])
    except ValueError:
        a = 0<N><N>
    try:
        b = math.sqrt(eig_val[small_ind])
    except ValueError:
        b = 0<N><N>
    x = [a * math.cos(it) for it in t]
    y = [b * math.sin(it) for it in t]
    angle = math.atan2(eig_vec[1, big_ind], eig_vec[0, big_ind])
    rot = Rot.from_euler('z', angle).as_matrix()[0:2, 0:2]
    fx = np.stack([x, y]).T @ rot<N><N>
    px = np.array(fx[:, 0] + xEst[0, 0]).flatten()
    py = np.array(fx[:, 1] + xEst[1, 0]).flatten()
    plt.plot(px, py, "--r")<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    time = 0.0<N><N>
    # RF_ID positions [x, y]
    RF_ID = np.array([[10.0, 0.0],
                      [10.0, 10.0],
                      [0.0, 15.0],
                      [-5.0, 20.0]])<N><N>
    # State Vector [x y yaw v]'
    xEst = np.zeros((4, 1))
    xTrue = np.zeros((4, 1))
    px = np.zeros((4, NP))  # Particle store of x<N><N>
    xDR = np.zeros((4, 1))  # Dead reckoning<N><N>
    # history
    hxEst = xEst
    hxTrue = xTrue
    hxDR = xTrue<N><N>
    while SIM_TIME >= time:
        time += DT
        u = calc_input()<N><N>
        xTrue, z, xDR, ud = observation(xTrue, xDR, u, RF_ID)<N><N>
        xEst, PEst, px = enkf_localization(px, z, ud)<N><N>
        # store data history
        hxEst = np.hstack((hxEst, xEst))
        hxDR = np.hstack((hxDR, xDR))
        hxTrue = np.hstack((hxTrue, xTrue))<N><N>
        if show_animation:
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])<N><N>
"""<N><N>Extended kalman filter (EKF) localization sample<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot<N><N>
# Covariance for EKF simulation
Q = np.diag([
    0.1,  # variance of location on x-axis
    0.1,  # variance of location on y-axis
    np.deg2rad(1.0),  # variance of yaw angle
    1.0  # variance of velocity
]) ** 2  # predict state covariance
R = np.diag([1.0, 1.0]) ** 2  # Observation x,y position covariance<N><N>
#  Simulation parameter
INPUT_NOISE = np.diag([1.0, np.deg2rad(30.0)]) ** 2
GPS_NOISE = np.diag([0.5, 0.5]) ** 2<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]<N><N>
show_animation = True<N><N>

def calc_input():
    v = 1.0  # [m/s]
    yawrate = 0.1  # [rad/s]
    u = np.array([[v], [yawrate]])
    return u<N><N>

def observation(xTrue, xd, u):
    xTrue = motion_model(xTrue, u)<N><N>
    # add noise to gps x-y
    z = observation_model(xTrue) + GPS_NOISE @ np.random.randn(2, 1)<N><N>
    # add noise to input
    ud = u + INPUT_NOISE @ np.random.randn(2, 1)<N><N>
    xd = motion_model(xd, ud)<N><N>
    return xTrue, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0, 0],
                  [0, 1.0, 0, 0],
                  [0, 0, 1.0, 0],
                  [0, 0, 0, 0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT],
                  [1.0, 0.0]])<N><N>
    x = F @ x + B @ u<N><N>
    return x<N><N>

def observation_model(x):
    H = np.array([
        [1, 0, 0, 0],
        [0, 1, 0, 0]
    ])<N><N>
    z = H @ x<N><N>
    return z<N><N>

def jacob_f(x, u):
    """
    Jacobian of Motion Model<N><N>
    motion model
    x_{t+1} = x_t+v*dt*cos(yaw)
    y_{t+1} = y_t+v*dt*sin(yaw)
    yaw_{t+1} = yaw_t+omega*dt
    v_{t+1} = v{t}
    so
    dx/dyaw = -v*dt*sin(yaw)
    dx/dv = dt*cos(yaw)
    dy/dyaw = v*dt*cos(yaw)
    dy/dv = dt*sin(yaw)
    """
    yaw = x[2, 0]
    v = u[0, 0]
    jF = np.array([
        [1.0, 0.0, -DT * v * math.sin(yaw), DT * math.cos(yaw)],
        [0.0, 1.0, DT * v * math.cos(yaw), DT * math.sin(yaw)],
        [0.0, 0.0, 1.0, 0.0],
        [0.0, 0.0, 0.0, 1.0]])<N><N>
    return jF<N><N>

def jacob_h():
    # Jacobian of Observation Model
    jH = np.array([
        [1, 0, 0, 0],
        [0, 1, 0, 0]
    ])<N><N>
    return jH<N><N>

def ekf_estimation(xEst, PEst, z, u):
    #  Predict
    xPred = motion_model(xEst, u)
    jF = jacob_f(xEst, u)
    PPred = jF @ PEst @ jF.T + Q<N><N>
    #  Update
    jH = jacob_h()
    zPred = observation_model(xPred)
    y = z - zPred
    S = jH @ PPred @ jH.T + R
    K = PPred @ jH.T @ np.linalg.inv(S)
    xEst = xPred + K @ y
    PEst = (np.eye(len(xEst)) - K @ jH) @ PPred
    return xEst, PEst<N><N>

def plot_covariance_ellipse(xEst, PEst):  # pragma: no cover
    Pxy = PEst[0:2, 0:2]
    eigval, eigvec = np.linalg.eig(Pxy)<N><N>
    if eigval[0] >= eigval[1]:
        bigind = 0
        smallind = 1
    else:
        bigind = 1
        smallind = 0<N><N>
    t = np.arange(0, 2 * math.pi + 0.1, 0.1)
    a = math.sqrt(eigval[bigind])
    b = math.sqrt(eigval[smallind])
    x = [a * math.cos(it) for it in t]
    y = [b * math.sin(it) for it in t]
    angle = math.atan2(eigvec[1, bigind], eigvec[0, bigind])
    rot = Rot.from_euler('z', angle).as_matrix()[0:2, 0:2]
    fx = rot @ (np.array([x, y]))
    px = np.array(fx[0, :] + xEst[0, 0]).flatten()
    py = np.array(fx[1, :] + xEst[1, 0]).flatten()
    plt.plot(px, py, "--r")<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    time = 0.0<N><N>
    # State Vector [x y yaw v]'
    xEst = np.zeros((4, 1))
    xTrue = np.zeros((4, 1))
    PEst = np.eye(4)<N><N>
    xDR = np.zeros((4, 1))  # Dead reckoning<N><N>
    # history
    hxEst = xEst
    hxTrue = xTrue
    hxDR = xTrue
    hz = np.zeros((2, 1))<N><N>
    while SIM_TIME >= time:
        time += DT
        u = calc_input()<N><N>
        xTrue, z, xDR, ud = observation(xTrue, xDR, u)<N><N>
        xEst, PEst = ekf_estimation(xEst, PEst, z, ud)<N><N>
        # store data history
        hxEst = np.hstack((hxEst, xEst))
        hxDR = np.hstack((hxDR, xDR))
        hxTrue = np.hstack((hxTrue, xTrue))
        hz = np.hstack((hz, z))<N><N>
"""<N><N>Histogram Filter 2D localization example<N><N>

In this simulation, x,y are unknown, yaw is known.<N><N>
Initial position is not needed.<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import copy
import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter
from scipy.stats import norm<N><N>
# Parameters
EXTEND_AREA = 10.0  # [m] grid map extended length
SIM_TIME = 50.0  # simulation time [s]
DT = 0.1  # time tick [s]
MAX_RANGE = 10.0  # maximum observation range
MOTION_STD = 1.0  # standard deviation for motion gaussian distribution
RANGE_STD = 3.0  # standard deviation for observation gaussian distribution<N><N>
# grid map param
XY_RESOLUTION = 0.5  # xy grid resolution
MIN_X = -15.0
MIN_Y = -5.0
MAX_X = 15.0
MAX_Y = 25.0<N><N>
# simulation parameters
NOISE_RANGE = 2.0  # [m] 1σ range noise parameter
NOISE_SPEED = 0.5  # [m/s] 1σ speed noise parameter<N><N>
show_animation = True<N><N>

class GridMap:<N><N>
    def __init__(self):
        self.data = None
        self.xy_resolution = None
        self.min_x = None
        self.min_y = None
        self.max_x = None
        self.max_y = None
        self.x_w = None
        self.y_w = None
        self.dx = 0.0  # movement distance
        self.dy = 0.0  # movement distance<N><N>

def histogram_filter_localization(grid_map, u, z, yaw):
    grid_map = motion_update(grid_map, u, yaw)<N><N>
    grid_map = observation_update(grid_map, z, RANGE_STD)<N><N>
    return grid_map<N><N>

def calc_gaussian_observation_pdf(grid_map, z, iz, ix, iy, std):
    # predicted range
    x = ix * grid_map.xy_resolution + grid_map.min_x
    y = iy * grid_map.xy_resolution + grid_map.min_y
    d = math.hypot(x - z[iz, 1], y - z[iz, 2])<N><N>
    # likelihood
    pdf = (1.0 - norm.cdf(abs(d - z[iz, 0]), 0.0, std))<N><N>
    return pdf<N><N>

def observation_update(grid_map, z, std):
    for iz in range(z.shape[0]):
        for ix in range(grid_map.x_w):
            for iy in range(grid_map.y_w):
                grid_map.data[ix][iy] *= calc_gaussian_observation_pdf(
                    grid_map, z, iz, ix, iy, std)<N><N>
    grid_map = normalize_probability(grid_map)<N><N>
    return grid_map<N><N>

def calc_input():
    v = 1.0  # [m/s]
    yaw_rate = 0.1  # [rad/s]
    u = np.array([v, yaw_rate]).reshape(2, 1)
    return u<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0, 0],
                  [0, 1.0, 0, 0],
                  [0, 0, 1.0, 0],
                  [0, 0, 0, 0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT],
                  [1.0, 0.0]])<N><N>
    x = F @ x + B @ u<N><N>
    return x<N><N>

def draw_heat_map(data, mx, my):
    max_value = max([max(i_data) for i_data in data])
    plt.pcolor(mx, my, data, vmax=max_value, cmap=plt.cm.get_cmap("Blues"))
    plt.axis("equal")<N><N>

def observation(xTrue, u, RFID):
    xTrue = motion_model(xTrue, u)<N><N>
    z = np.zeros((0, 3))<N><N>
    for i in range(len(RFID[:, 0])):<N><N>
        dx = xTrue[0, 0] - RFID[i, 0]
        dy = xTrue[1, 0] - RFID[i, 1]
        d = math.hypot(dx, dy)
        if d <= MAX_RANGE:
            # add noise to range observation
            dn = d + np.random.randn() * NOISE_RANGE
            zi = np.array([dn, RFID[i, 0], RFID[i, 1]])
            z = np.vstack((z, zi))<N><N>
    # add noise to speed
    ud = u[:, :]
    ud[0] += np.random.randn() * NOISE_SPEED<N><N>
    return xTrue, z, ud<N><N>

def normalize_probability(grid_map):
    sump = sum([sum(i_data) for i_data in grid_map.data])<N><N>
    for ix in range(grid_map.x_w):
        for iy in range(grid_map.y_w):
            grid_map.data[ix][iy] /= sump<N><N>
    return grid_map<N><N>

def init_grid_map(xy_resolution, min_x, min_y, max_x, max_y):
    grid_map = GridMap()<N><N>
    grid_map.xy_resolution = xy_resolution
    grid_map.min_x = min_x
    grid_map.min_y = min_y
    grid_map.max_x = max_x
    grid_map.max_y = max_y
    grid_map.x_w = int(round((grid_map.max_x - grid_map.min_x)
                             / grid_map.xy_resolution))
    grid_map.y_w = int(round((grid_map.max_y - grid_map.min_y)
                             / grid_map.xy_resolution))<N><N>
    grid_map.data = [[1.0 for _ in range(grid_map.y_w)]
                     for _ in range(grid_map.x_w)]
    grid_map = normalize_probability(grid_map)<N><N>
    return grid_map<N><N>

def map_shift(grid_map, x_shift, y_shift):
    tmp_grid_map = copy.deepcopy(grid_map.data)<N><N>
    for ix in range(grid_map.x_w):
        for iy in range(grid_map.y_w):
            nix = ix + x_shift
            niy = iy + y_shift<N><N>
            if 0 <= nix < grid_map.x_w and 0 <= niy < grid_map.y_w:
                grid_map.data[ix + x_shift][iy + y_shift] =\
                    tmp_grid_map[ix][iy]<N><N>
    return grid_map<N><N>

def motion_update(grid_map, u, yaw):
    grid_map.dx += DT * math.cos(yaw) * u[0]
    grid_map.dy += DT * math.sin(yaw) * u[0]<N><N>
    x_shift = grid_map.dx // grid_map.xy_resolution
    y_shift = grid_map.dy // grid_map.xy_resolution<N><N>
    if abs(x_shift) >= 1.0 or abs(y_shift) >= 1.0:  # map should be shifted
        grid_map = map_shift(grid_map, int(x_shift), int(y_shift))
        grid_map.dx -= x_shift * grid_map.xy_resolution
        grid_map.dy -= y_shift * grid_map.xy_resolution<N><N>
    grid_map.data = gaussian_filter(grid_map.data, sigma=MOTION_STD)<N><N>
    return grid_map<N><N>

def calc_grid_index(grid_map):
    mx, my = np.mgrid[slice(grid_map.min_x - grid_map.xy_resolution / 2.0,
                            grid_map.max_x + grid_map.xy_resolution / 2.0,
                            grid_map.xy_resolution),
                      slice(grid_map.min_y - grid_map.xy_resolution / 2.0,
                            grid_map.max_y + grid_map.xy_resolution / 2.0,
                            grid_map.xy_resolution)]<N><N>
    return mx, my<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    # RF_ID positions [x, y]
    RF_ID = np.array([[10.0, 0.0],
                      [10.0, 10.0],
                      [0.0, 15.0],
                      [-5.0, 20.0]])<N><N>
    time = 0.0<N><N>
    xTrue = np.zeros((4, 1))
    grid_map = init_grid_map(XY_RESOLUTION, MIN_X, MIN_Y, MAX_X, MAX_Y)
    mx, my = calc_grid_index(grid_map)  # for grid map visualization<N><N>
    while SIM_TIME >= time:
        time += DT
        print("Time:", time)<N><N>
        u = calc_input()<N><N>
        yaw = xTrue[2, 0]  # Orientation is known
        xTrue, z, ud = observation(xTrue, u, RF_ID)<N><N>
        grid_map = histogram_filter_localization(grid_map, u, z, yaw)<N><N>
"""<N><N>Particle Filter localization sample<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot<N><N>
# Estimation parameter of PF
Q = np.diag([0.2]) ** 2  # range error
R = np.diag([2.0, np.deg2rad(40.0)]) ** 2  # input error<N><N>
#  Simulation parameter
Q_sim = np.diag([0.2]) ** 2
R_sim = np.diag([1.0, np.deg2rad(30.0)]) ** 2<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]
MAX_RANGE = 20.0  # maximum observation range<N><N>
# Particle filter parameter
NP = 100  # Number of Particle
NTh = NP / 2.0  # Number of particle for re-sampling<N><N>
show_animation = True<N><N>

def calc_input():
    v = 1.0  # [m/s]
    yaw_rate = 0.1  # [rad/s]
    u = np.array([[v, yaw_rate]]).T
    return u<N><N>

def observation(x_true, xd, u, rf_id):
    x_true = motion_model(x_true, u)<N><N>
    # add noise to gps x-y
    z = np.zeros((0, 3))<N><N>
    for i in range(len(rf_id[:, 0])):<N><N>
        dx = x_true[0, 0] - rf_id[i, 0]
        dy = x_true[1, 0] - rf_id[i, 1]
        d = math.hypot(dx, dy)
        if d <= MAX_RANGE:
            dn = d + np.random.randn() * Q_sim[0, 0] ** 0.5  # add noise
            zi = np.array([[dn, rf_id[i, 0], rf_id[i, 1]]])
            z = np.vstack((z, zi))<N><N>
    # add noise to input
    ud1 = u[0, 0] + np.random.randn() * R_sim[0, 0] ** 0.5
    ud2 = u[1, 0] + np.random.randn() * R_sim[1, 1] ** 0.5
    ud = np.array([[ud1, ud2]]).T<N><N>
    xd = motion_model(xd, ud)<N><N>
    return x_true, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0, 0],
                  [0, 1.0, 0, 0],
                  [0, 0, 1.0, 0],
                  [0, 0, 0, 0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT],
                  [1.0, 0.0]])<N><N>
    x = F.dot(x) + B.dot(u)<N><N>
    return x<N><N>

def gauss_likelihood(x, sigma):
    p = 1.0 / math.sqrt(2.0 * math.pi * sigma ** 2) * \
        math.exp(-x ** 2 / (2 * sigma ** 2))<N><N>
    return p<N><N>

def calc_covariance(x_est, px, pw):
    """
    calculate covariance matrix
    see ipynb doc
    """
    cov = np.zeros((3, 3))
    n_particle = px.shape[1]
    for i in range(n_particle):
        dx = (px[:, i:i + 1] - x_est)[0:3]
        cov += pw[0, i] * dx @ dx.T
    cov *= 1.0 / (1.0 - pw @ pw.T)<N><N>
    return cov<N><N>

def pf_localization(px, pw, z, u):
    """
    Localization with Particle filter
    """<N><N>
    for ip in range(NP):
        x = np.array([px[:, ip]]).T
        w = pw[0, ip]<N><N>
        #  Predict with random input sampling
        ud1 = u[0, 0] + np.random.randn() * R[0, 0] ** 0.5
        ud2 = u[1, 0] + np.random.randn() * R[1, 1] ** 0.5
        ud = np.array([[ud1, ud2]]).T
        x = motion_model(x, ud)<N><N>
        #  Calc Importance Weight
        for i in range(len(z[:, 0])):
            dx = x[0, 0] - z[i, 1]
            dy = x[1, 0] - z[i, 2]
            pre_z = math.hypot(dx, dy)
            dz = pre_z - z[i, 0]
            w = w * gauss_likelihood(dz, math.sqrt(Q[0, 0]))<N><N>
        px[:, ip] = x[:, 0]
        pw[0, ip] = w<N><N>
    pw = pw / pw.sum()  # normalize<N><N>
    x_est = px.dot(pw.T)
    p_est = calc_covariance(x_est, px, pw)<N><N>
    N_eff = 1.0 / (pw.dot(pw.T))[0, 0]  # Effective particle number
    if N_eff < NTh:
        px, pw = re_sampling(px, pw)
    return x_est, p_est, px, pw<N><N>

def re_sampling(px, pw):
    """
    low variance re-sampling
    """<N><N>
    w_cum = np.cumsum(pw)
    base = np.arange(0.0, 1.0, 1 / NP)
    re_sample_id = base + np.random.uniform(0, 1 / NP)
    indexes = []
    ind = 0
    for ip in range(NP):
        while re_sample_id[ip] > w_cum[ind]:
            ind += 1
        indexes.append(ind)<N><N>
    px = px[:, indexes]
    pw = np.zeros((1, NP)) + 1.0 / NP  # init weight<N><N>
    return px, pw<N><N>

def plot_covariance_ellipse(x_est, p_est):  # pragma: no cover
    p_xy = p_est[0:2, 0:2]
    eig_val, eig_vec = np.linalg.eig(p_xy)<N><N>
    if eig_val[0] >= eig_val[1]:
        big_ind = 0
        small_ind = 1
    else:
        big_ind = 1
        small_ind = 0<N><N>
    t = np.arange(0, 2 * math.pi + 0.1, 0.1)<N><N>
    # eig_val[big_ind] or eiq_val[small_ind] were occasionally negative
    # numbers extremely close to 0 (~10^-20), catch these cases and set the
    # respective variable to 0
    try:
        a = math.sqrt(eig_val[big_ind])
    except ValueError:
        a = 0<N><N>
    try:
        b = math.sqrt(eig_val[small_ind])
    except ValueError:
        b = 0<N><N>
    x = [a * math.cos(it) for it in t]
    y = [b * math.sin(it) for it in t]
    angle = math.atan2(eig_vec[1, big_ind], eig_vec[0, big_ind])
    rot = Rot.from_euler('z', angle).as_matrix()[0:2, 0:2]
    fx = rot.dot(np.array([[x, y]]))
    px = np.array(fx[0, :] + x_est[0, 0]).flatten()
    py = np.array(fx[1, :] + x_est[1, 0]).flatten()
    plt.plot(px, py, "--r")<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    time = 0.0<N><N>
    # RF_ID positions [x, y]
    rf_id = np.array([[10.0, 0.0],
                      [10.0, 10.0],
                      [0.0, 15.0],
                      [-5.0, 20.0]])<N><N>
    # State Vector [x y yaw v]'
    x_est = np.zeros((4, 1))
    x_true = np.zeros((4, 1))<N><N>
    px = np.zeros((4, NP))  # Particle store
    pw = np.zeros((1, NP)) + 1.0 / NP  # Particle weight
    x_dr = np.zeros((4, 1))  # Dead reckoning<N><N>
    # history
    h_x_est = x_est
    h_x_true = x_true
    h_x_dr = x_true<N><N>
    while SIM_TIME >= time:
        time += DT
        u = calc_input()<N><N>
        x_true, z, x_dr, ud = observation(x_true, x_dr, u, rf_id)<N><N>
        x_est, PEst, px, pw = pf_localization(px, pw, z, ud)<N><N>
        # store data history
        h_x_est = np.hstack((h_x_est, x_est))
        h_x_dr = np.hstack((h_x_dr, x_dr))
        h_x_true = np.hstack((h_x_true, x_true))<N><N>
        if show_animation:
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])<N><N>
"""<N><N>Unscented kalman filter (UKF) localization sample<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot
import scipy.linalg<N><N>
# Covariance for UKF simulation
Q = np.diag([
    0.1,  # variance of location on x-axis
    0.1,  # variance of location on y-axis
    np.deg2rad(1.0),  # variance of yaw angle
    1.0  # variance of velocity
]) ** 2  # predict state covariance
R = np.diag([1.0, 1.0]) ** 2  # Observation x,y position covariance<N><N>
#  Simulation parameter
INPUT_NOISE = np.diag([1.0, np.deg2rad(30.0)]) ** 2
GPS_NOISE = np.diag([0.5, 0.5]) ** 2<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]<N><N>
#  UKF Parameter
ALPHA = 0.001
BETA = 2
KAPPA = 0<N><N>
show_animation = True<N><N>

def calc_input():
    v = 1.0  # [m/s]
    yawRate = 0.1  # [rad/s]
    u = np.array([[v, yawRate]]).T
    return u<N><N>

def observation(xTrue, xd, u):
    xTrue = motion_model(xTrue, u)<N><N>
    # add noise to gps x-y
    z = observation_model(xTrue) + GPS_NOISE @ np.random.randn(2, 1)<N><N>
    # add noise to input
    ud = u + INPUT_NOISE @ np.random.randn(2, 1)<N><N>
    xd = motion_model(xd, ud)<N><N>
    return xTrue, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0, 0],
                  [0, 1.0, 0, 0],
                  [0, 0, 1.0, 0],
                  [0, 0, 0, 0]])<N><N>
    B = np.array([[DT * math.cos(x[2]), 0],
                  [DT * math.sin(x[2]), 0],
                  [0.0, DT],
                  [1.0, 0.0]])<N><N>
    x = F @ x + B @ u<N><N>
    return x<N><N>

def observation_model(x):
    H = np.array([
        [1, 0, 0, 0],
        [0, 1, 0, 0]
    ])<N><N>
    z = H @ x<N><N>
    return z<N><N>

def generate_sigma_points(xEst, PEst, gamma):
    sigma = xEst
    Psqrt = scipy.linalg.sqrtm(PEst)
    n = len(xEst[:, 0])
    # Positive direction
    for i in range(n):
        sigma = np.hstack((sigma, xEst + gamma * Psqrt[:, i:i + 1]))<N><N>
    # Negative direction
    for i in range(n):
        sigma = np.hstack((sigma, xEst - gamma * Psqrt[:, i:i + 1]))<N><N>
    return sigma<N><N>

def predict_sigma_motion(sigma, u):
    """
        Sigma Points prediction with motion model
    """
    for i in range(sigma.shape[1]):
        sigma[:, i:i + 1] = motion_model(sigma[:, i:i + 1], u)<N><N>
    return sigma<N><N>

def predict_sigma_observation(sigma):
    """
        Sigma Points prediction with observation model
    """
    for i in range(sigma.shape[1]):
        sigma[0:2, i] = observation_model(sigma[:, i])<N><N>
    sigma = sigma[0:2, :]<N><N>
    return sigma<N><N>

def calc_sigma_covariance(x, sigma, wc, Pi):
    nSigma = sigma.shape[1]
    d = sigma - x[0:sigma.shape[0]]
    P = Pi
    for i in range(nSigma):
        P = P + wc[0, i] * d[:, i:i + 1] @ d[:, i:i + 1].T
    return P<N><N>

def calc_pxz(sigma, x, z_sigma, zb, wc):
    nSigma = sigma.shape[1]
    dx = sigma - x
    dz = z_sigma - zb[0:2]
    P = np.zeros((dx.shape[0], dz.shape[0]))<N><N>
    for i in range(nSigma):
        P = P + wc[0, i] * dx[:, i:i + 1] @ dz[:, i:i + 1].T<N><N>
    return P<N><N>

def ukf_estimation(xEst, PEst, z, u, wm, wc, gamma):
    #  Predict
    sigma = generate_sigma_points(xEst, PEst, gamma)
    sigma = predict_sigma_motion(sigma, u)
    xPred = (wm @ sigma.T).T
    PPred = calc_sigma_covariance(xPred, sigma, wc, Q)<N><N>
    #  Update
    zPred = observation_model(xPred)
    y = z - zPred
    sigma = generate_sigma_points(xPred, PPred, gamma)
    zb = (wm @ sigma.T).T
    z_sigma = predict_sigma_observation(sigma)
    st = calc_sigma_covariance(zb, z_sigma, wc, R)
    Pxz = calc_pxz(sigma, xPred, z_sigma, zb, wc)
    K = Pxz @ np.linalg.inv(st)
    xEst = xPred + K @ y
    PEst = PPred - K @ st @ K.T<N><N>
    return xEst, PEst<N><N>

def plot_covariance_ellipse(xEst, PEst):  # pragma: no cover
    Pxy = PEst[0:2, 0:2]
    eigval, eigvec = np.linalg.eig(Pxy)<N><N>
    if eigval[0] >= eigval[1]:
        bigind = 0
        smallind = 1
    else:
        bigind = 1
        smallind = 0<N><N>
    t = np.arange(0, 2 * math.pi + 0.1, 0.1)
    a = math.sqrt(eigval[bigind])
    b = math.sqrt(eigval[smallind])
    x = [a * math.cos(it) for it in t]
    y = [b * math.sin(it) for it in t]
    angle = math.atan2(eigvec[1, bigind], eigvec[0, bigind])
    rot = Rot.from_euler('z', angle).as_matrix()[0:2, 0:2]
    fx = rot @ np.array([x, y])
    px = np.array(fx[0, :] + xEst[0, 0]).flatten()
    py = np.array(fx[1, :] + xEst[1, 0]).flatten()
    plt.plot(px, py, "--r")<N><N>

def setup_ukf(nx):
    lamb = ALPHA ** 2 * (nx + KAPPA) - nx
    # calculate weights
    wm = [lamb / (lamb + nx)]
    wc = [(lamb / (lamb + nx)) + (1 - ALPHA ** 2 + BETA)]
    for i in range(2 * nx):
        wm.append(1.0 / (2 * (nx + lamb)))
        wc.append(1.0 / (2 * (nx + lamb)))
    gamma = math.sqrt(nx + lamb)<N><N>
    wm = np.array([wm])
    wc = np.array([wc])<N><N>
    return wm, wc, gamma<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    nx = 4  # State Vector [x y yaw v]'
    xEst = np.zeros((nx, 1))
    xTrue = np.zeros((nx, 1))
    PEst = np.eye(nx)
    xDR = np.zeros((nx, 1))  # Dead reckoning<N><N>
    wm, wc, gamma = setup_ukf(nx)<N><N>
    # history
    hxEst = xEst
    hxTrue = xTrue
    hxDR = xTrue
    hz = np.zeros((2, 1))<N><N>
    time = 0.0<N><N>
    while SIM_TIME >= time:
        time += DT
        u = calc_input()<N><N>
        xTrue, z, xDR, ud = observation(xTrue, xDR, u)<N><N>
        xEst, PEst = ukf_estimation(xEst, PEst, z, ud, wm, wc, gamma)<N><N>
        # store data history
        hxEst = np.hstack((hxEst, xEst))
        hxDR = np.hstack((hxDR, xDR))
        hxTrue = np.hstack((hxTrue, xTrue))
        hz = np.hstack((hz, z))<N><N>
"""<N><N>Object shape recognition with circle fitting<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import matplotlib.pyplot as plt
import math
import random
import numpy as np<N><N>
show_animation = True<N><N>

def circle_fitting(x, y):
    """
    Circle Fitting with least squared
        input: point x-y positions
        output  cxe x center position
                cye y center position
                re  radius of circle
                error: prediction error
    """<N><N>
    sumx = sum(x)
    sumy = sum(y)
    sumx2 = sum([ix ** 2 for ix in x])
    sumy2 = sum([iy ** 2 for iy in y])
    sumxy = sum([ix * iy for (ix, iy) in zip(x, y)])<N><N>
    F = np.array([[sumx2, sumxy, sumx],
                  [sumxy, sumy2, sumy],
                  [sumx, sumy, len(x)]])<N><N>
    G = np.array([[-sum([ix ** 3 + ix * iy ** 2 for (ix, iy) in zip(x, y)])],
                  [-sum([ix ** 2 * iy + iy ** 3 for (ix, iy) in zip(x, y)])],
                  [-sum([ix ** 2 + iy ** 2 for (ix, iy) in zip(x, y)])]])<N><N>
    T = np.linalg.inv(F).dot(G)<N><N>
    cxe = float(T[0] / -2)
    cye = float(T[1] / -2)
    re = math.sqrt(cxe**2 + cye**2 - T[2])<N><N>
    error = sum([np.hypot(cxe - ix, cye - iy) - re for (ix, iy) in zip(x, y)])<N><N>
    return (cxe, cye, re, error)<N><N>

def get_sample_points(cx, cy, cr, angle_reso):
    x, y, angle, r = [], [], [], []<N><N>
    # points sampling
    for theta in np.arange(0.0, 2.0 * math.pi, angle_reso):
        nx = cx + cr * math.cos(theta)
        ny = cy + cr * math.sin(theta)
        nangle = math.atan2(ny, nx)
        nr = math.hypot(nx, ny) * random.uniform(0.95, 1.05)<N><N>
        x.append(nx)
        y.append(ny)
        angle.append(nangle)
        r.append(nr)<N><N>
    # ray casting filter
    rx, ry = ray_casting_filter(x, y, angle, r, angle_reso)<N><N>
    return rx, ry<N><N>

def ray_casting_filter(xl, yl, thetal, rangel, angle_reso):
    rx, ry = [], []
    rangedb = [float("inf") for _ in range(
        int(math.floor((math.pi * 2.0) / angle_reso)) + 1)]<N><N>
    for i, _ in enumerate(thetal):
        angleid = math.floor(thetal[i] / angle_reso)<N><N>
        if rangedb[angleid] > rangel[i]:
            rangedb[angleid] = rangel[i]<N><N>
    for i, _ in enumerate(rangedb):
        t = i * angle_reso
        if rangedb[i] != float("inf"):
            rx.append(rangedb[i] * math.cos(t))
            ry.append(rangedb[i] * math.sin(t))<N><N>
    return rx, ry<N><N>

def plot_circle(x, y, size, color="-b"):  # pragma: no cover
    deg = list(range(0, 360, 5))
    deg.append(0)
    xl = [x + size * math.cos(np.deg2rad(d)) for d in deg]
    yl = [y + size * math.sin(np.deg2rad(d)) for d in deg]
    plt.plot(xl, yl, color)<N><N>

def main():<N><N>
    # simulation parameters
    simtime = 15.0  # simulation time
    dt = 1.0  # time tick<N><N>
    cx = -2.0  # initial x position of obstacle
    cy = -8.0  # initial y position of obstacle
    cr = 1.0  # obstacle radious
    theta = np.deg2rad(30.0)  # obstacle moving direction
    angle_reso = np.deg2rad(3.0)  # sensor angle resolution<N><N>
    time = 0.0
    while time <= simtime:
        time += dt<N><N>
        cx += math.cos(theta)
        cy += math.cos(theta)<N><N>
        x, y = get_sample_points(cx, cy, cr, angle_reso)<N><N>
        ex, ey, er, error = circle_fitting(x, y)
        print("Error:", error)<N><N>
        if show_animation:  # pragma: no cover
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect('key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])
            plt.axis("equal")
            plt.plot(0.0, 0.0, "*r")
            plot_circle(cx, cy, cr)
            plt.plot(x, y, "xr")
            plot_circle(ex, ey, er, "-r")
            plt.pause(dt)<N><N>
    print("Done")<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>2D gaussian grid map sample<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm<N><N>
EXTEND_AREA = 10.0  # [m] grid map extention length<N><N>
show_animation = True<N><N>

def generate_gaussian_grid_map(ox, oy, xyreso, std):<N><N>
    minx, miny, maxx, maxy, xw, yw = calc_grid_map_config(ox, oy, xyreso)<N><N>
    gmap = [[0.0 for i in range(yw)] for i in range(xw)]<N><N>
    for ix in range(xw):
        for iy in range(yw):<N><N>
            x = ix * xyreso + minx
            y = iy * xyreso + miny<N><N>
            # Search minimum distance
            mindis = float("inf")
            for (iox, ioy) in zip(ox, oy):
                d = math.hypot(iox - x, ioy - y)
                if mindis >= d:
                    mindis = d<N><N>
            pdf = (1.0 - norm.cdf(mindis, 0.0, std))
            gmap[ix][iy] = pdf<N><N>
    return gmap, minx, maxx, miny, maxy<N><N>

def calc_grid_map_config(ox, oy, xyreso):
    minx = round(min(ox) - EXTEND_AREA / 2.0)
    miny = round(min(oy) - EXTEND_AREA / 2.0)
    maxx = round(max(ox) + EXTEND_AREA / 2.0)
    maxy = round(max(oy) + EXTEND_AREA / 2.0)
    xw = int(round((maxx - minx) / xyreso))
    yw = int(round((maxy - miny) / xyreso))<N><N>
    return minx, miny, maxx, maxy, xw, yw<N><N>

def draw_heatmap(data, minx, maxx, miny, maxy, xyreso):
    x, y = np.mgrid[slice(minx - xyreso / 2.0, maxx + xyreso / 2.0, xyreso),
                    slice(miny - xyreso / 2.0, maxy + xyreso / 2.0, xyreso)]
    plt.pcolor(x, y, data, vmax=1.0, cmap=plt.cm.Blues)
    plt.axis("equal")<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    xyreso = 0.5  # xy grid resolution
    STD = 5.0  # standard diviation for gaussian distribution<N><N>
    for i in range(5):
        ox = (np.random.rand(4) - 0.5) * 10.0
        oy = (np.random.rand(4) - 0.5) * 10.0
        gmap, minx, maxx, miny, maxy = generate_gaussian_grid_map(
            ox, oy, xyreso, STD)<N><N>
        if show_animation:  # pragma: no cover
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect('key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])
            draw_heatmap(gmap, minx, maxx, miny, maxy, xyreso)
            plt.plot(ox, oy, "xr")
            plt.plot(0.0, 0.0, "ob")
            plt.pause(1.0)<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Grid map library in python<N><N>
author: Atsushi Sakai<N><N>
"""<N><N>import matplotlib.pyplot as plt
import numpy as np<N><N>

class GridMap:
    """
    GridMap class
    """<N><N>
    def __init__(self, width, height, resolution,
                 center_x, center_y, init_val=0.0):
        """__init__<N><N>
        :param width: number of grid for width
        :param height: number of grid for heigt
        :param resolution: grid resolution [m]
        :param center_x: center x position  [m]
        :param center_y: center y position [m]
        :param init_val: initial value for all grid
        """
        self.width = width
        self.height = height
        self.resolution = resolution
        self.center_x = center_x
        self.center_y = center_y<N><N>
        self.left_lower_x = self.center_x - self.width / 2.0 * self.resolution
        self.left_lower_y = self.center_y - self.height / 2.0 * self.resolution<N><N>
        self.ndata = self.width * self.height
        self.data = [init_val] * self.ndata<N><N>
    def get_value_from_xy_index(self, x_ind, y_ind):
        """get_value_from_xy_index<N><N>
        when the index is out of grid map area, return None<N><N>
        :param x_ind: x index
        :param y_ind: y index
        """<N><N>
        grid_ind = self.calc_grid_index_from_xy_index(x_ind, y_ind)<N><N>
        if 0 <= grid_ind < self.ndata:
            return self.data[grid_ind]
        else:
            return None<N><N>
    def get_xy_index_from_xy_pos(self, x_pos, y_pos):
        """get_xy_index_from_xy_pos<N><N>
        :param x_pos: x position [m]
        :param y_pos: y position [m]
        """
        x_ind = self.calc_xy_index_from_position(
            x_pos, self.left_lower_x, self.width)
        y_ind = self.calc_xy_index_from_position(
            y_pos, self.left_lower_y, self.height)<N><N>
        return x_ind, y_ind<N><N>
    def set_value_from_xy_pos(self, x_pos, y_pos, val):
        """set_value_from_xy_pos<N><N>
        return bool flag, which means setting value is succeeded or not<N><N>
        :param x_pos: x position [m]
        :param y_pos: y position [m]
        :param val: grid value
        """<N><N>
        x_ind, y_ind = self.get_xy_index_from_xy_pos(x_pos, y_pos)<N><N>
        if (not x_ind) or (not y_ind):
            return False  # NG<N><N>
        flag = self.set_value_from_xy_index(x_ind, y_ind, val)<N><N>
        return flag<N><N>
    def set_value_from_xy_index(self, x_ind, y_ind, val):
        """set_value_from_xy_index<N><N>
        return bool flag, which means setting value is succeeded or not<N><N>
        :param x_ind: x index
        :param y_ind: y index
        :param val: grid value
        """<N><N>
        if (x_ind is None) or (y_ind is None):
            return False, False<N><N>
        grid_ind = int(y_ind * self.width + x_ind)<N><N>
        if 0 <= grid_ind < self.ndata:
            self.data[grid_ind] = val
            return True  # OK
        else:
            return False  # NG<N><N>
    def set_value_from_polygon(self, pol_x, pol_y, val, inside=True):
        """set_value_from_polygon<N><N>
        Setting value inside or outside polygon<N><N>
        :param pol_x: x position list for a polygon
        :param pol_y: y position list for a polygon
        :param val: grid value
        :param inside: setting data inside or outside
        """<N><N>
        # making ring polygon
        if (pol_x[0] != pol_x[-1]) or (pol_y[0] != pol_y[-1]):
            pol_x.append(pol_x[0])
            pol_y.append(pol_y[0])<N><N>
        # setting value for all grid
        for x_ind in range(self.width):
            for y_ind in range(self.height):
                x_pos, y_pos = self.calc_grid_central_xy_position_from_xy_index(
                    x_ind, y_ind)<N><N>
                flag = self.check_inside_polygon(x_pos, y_pos, pol_x, pol_y)<N><N>
                if flag is inside:
                    self.set_value_from_xy_index(x_ind, y_ind, val)<N><N>
    def calc_grid_index_from_xy_index(self, x_ind, y_ind):
        grid_ind = int(y_ind * self.width + x_ind)
        return grid_ind<N><N>
    def calc_grid_central_xy_position_from_xy_index(self, x_ind, y_ind):
        x_pos = self.calc_grid_central_xy_position_from_index(
            x_ind, self.left_lower_x)
        y_pos = self.calc_grid_central_xy_position_from_index(
            y_ind, self.left_lower_y)<N><N>
        return x_pos, y_pos<N><N>
    def calc_grid_central_xy_position_from_index(self, index, lower_pos):
        return lower_pos + index * self.resolution + self.resolution / 2.0<N><N>
    def calc_xy_index_from_position(self, pos, lower_pos, max_index):
        ind = int(np.floor((pos - lower_pos) / self.resolution))
        if 0 <= ind <= max_index:
            return ind
        else:
            return None<N><N>
    def check_occupied_from_xy_index(self, xind, yind, occupied_val=1.0):<N><N>
        val = self.get_value_from_xy_index(xind, yind)<N><N>
        if val is None or val >= occupied_val:
            return True
        else:
            return False<N><N>
    def expand_grid(self):
        xinds, yinds = [], []<N><N>
        for ix in range(self.width):
            for iy in range(self.height):
                if self.check_occupied_from_xy_index(ix, iy):
                    xinds.append(ix)
                    yinds.append(iy)<N><N>
        for (ix, iy) in zip(xinds, yinds):
            self.set_value_from_xy_index(ix + 1, iy, val=1.0)
            self.set_value_from_xy_index(ix, iy + 1, val=1.0)
            self.set_value_from_xy_index(ix + 1, iy + 1, val=1.0)
            self.set_value_from_xy_index(ix - 1, iy, val=1.0)
            self.set_value_from_xy_index(ix, iy - 1, val=1.0)
            self.set_value_from_xy_index(ix - 1, iy - 1, val=1.0)<N><N>
    @staticmethod
    def check_inside_polygon(iox, ioy, x, y):<N><N>
        npoint = len(x) - 1
        inside = False
        for i1 in range(npoint):
            i2 = (i1 + 1) % (npoint + 1)<N><N>
            if x[i1] >= x[i2]:
                min_x, max_x = x[i2], x[i1]
            else:
                min_x, max_x = x[i1], x[i2]
            if not min_x < iox < max_x:
                continue<N><N>
            tmp1 = (y[i2] - y[i1]) / (x[i2] - x[i1])
            if (y[i1] + tmp1 * (iox - x[i1]) - ioy) > 0.0:
                inside = not inside<N><N>
        return inside<N><N>
    def print_grid_map_info(self):
        print("width:", self.width)
        print("height:", self.height)
        print("resolution:", self.resolution)
        print("center_x:", self.center_x)
        print("center_y:", self.center_y)
        print("left_lower_x:", self.left_lower_x)
        print("left_lower_y:", self.left_lower_y)
        print("ndata:", self.ndata)<N><N>
    def plot_grid_map(self, ax=None):<N><N>
        grid_data = np.reshape(np.array(self.data), (self.height, self.width))
        if not ax:
            fig, ax = plt.subplots()
        heat_map = ax.pcolor(grid_data, cmap="Blues", vmin=0.0, vmax=1.0)
        plt.axis("equal")
        # plt.show()<N><N>
        return heat_map<N><N>

def test_polygon_set():
    ox = [0.0, 20.0, 50.0, 100.0, 130.0, 40.0]
    oy = [0.0, -20.0, 0.0, 30.0, 60.0, 80.0]<N><N>
    grid_map = GridMap(600, 290, 0.7, 60.0, 30.5)<N><N>
    grid_map.set_value_from_polygon(ox, oy, 1.0, inside=False)<N><N>
    grid_map.plot_grid_map()<N><N>
    plt.axis("equal")
    plt.grid(True)<N><N>

def test_position_set():
    grid_map = GridMap(100, 120, 0.5, 10.0, -0.5)<N><N>
    grid_map.set_value_from_xy_pos(10.1, -1.1, 1.0)
    grid_map.set_value_from_xy_pos(10.1, -0.1, 1.0)
    grid_map.set_value_from_xy_pos(10.1, 1.1, 1.0)
    grid_map.set_value_from_xy_pos(11.1, 0.1, 1.0)
    grid_map.set_value_from_xy_pos(10.1, 0.1, 1.0)
    grid_map.set_value_from_xy_pos(9.1, 0.1, 1.0)<N><N>
    grid_map.plot_grid_map()<N><N>

def main():
    print("start!!")<N><N>
    test_position_set()
    test_polygon_set()<N><N>
    plt.show()<N><N>
    print("done!!")<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Object clustering with k-means algorithm<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math
import matplotlib.pyplot as plt
import random<N><N>
# k means parameters
MAX_LOOP = 10
DCOST_TH = 0.1
show_animation = True<N><N>

def kmeans_clustering(rx, ry, nc):
    clusters = Clusters(rx, ry, nc)
    clusters.calc_centroid()<N><N>
    pre_cost = float("inf")
    for loop in range(MAX_LOOP):
        print("loop:", loop)
        cost = clusters.update_clusters()
        clusters.calc_centroid()<N><N>
        d_cost = abs(cost - pre_cost)
        if d_cost < DCOST_TH:
            break
        pre_cost = cost<N><N>
    return clusters<N><N>

class Clusters:<N><N>
    def __init__(self, x, y, n_label):
        self.x = x
        self.y = y
        self.n_data = len(self.x)
        self.n_label = n_label
        self.labels = [random.randint(0, n_label - 1)
                       for _ in range(self.n_data)]
        self.center_x = [0.0 for _ in range(n_label)]
        self.center_y = [0.0 for _ in range(n_label)]<N><N>
    def plot_cluster(self):
        for label in set(self.labels):
            x, y = self._get_labeled_x_y(label)
            plt.plot(x, y, ".")<N><N>
    def calc_centroid(self):
        for label in set(self.labels):
            x, y = self._get_labeled_x_y(label)
            n_data = len(x)
            self.center_x[label] = sum(x) / n_data
            self.center_y[label] = sum(y) / n_data<N><N>
    def update_clusters(self):
        cost = 0.0<N><N>
        for ip in range(self.n_data):
            px = self.x[ip]
            py = self.y[ip]<N><N>
            dx = [icx - px for icx in self.center_x]
            dy = [icy - py for icy in self.center_y]<N><N>
            dist_list = [math.hypot(idx, idy) for (idx, idy) in zip(dx, dy)]
            min_dist = min(dist_list)
            min_id = dist_list.index(min_dist)
            self.labels[ip] = min_id
            cost += min_dist<N><N>
        return cost<N><N>
    def _get_labeled_x_y(self, target_label):
        x = [self.x[i] for i, label in enumerate(self.labels) if label == target_label]
        y = [self.y[i] for i, label in enumerate(self.labels) if label == target_label]
        return x, y<N><N>

def calc_raw_data(cx, cy, n_points, rand_d):
    rx, ry = [], []<N><N>
    for (icx, icy) in zip(cx, cy):
        for _ in range(n_points):
            rx.append(icx + rand_d * (random.random() - 0.5))
            ry.append(icy + rand_d * (random.random() - 0.5))<N><N>
    return rx, ry<N><N>

def update_positions(cx, cy):
    # object moving parameters
    DX1 = 0.4
    DY1 = 0.5
    DX2 = -0.3
    DY2 = -0.5<N><N>
    cx[0] += DX1
    cy[0] += DY1
    cx[1] += DX2
    cy[1] += DY2<N><N>
    return cx, cy<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    cx = [0.0, 8.0]
    cy = [0.0, 8.0]
    n_points = 10
    rand_d = 3.0
    n_cluster = 2
    sim_time = 15.0
    dt = 1.0
    time = 0.0<N><N>
    while time <= sim_time:
        print("Time:", time)
        time += dt<N><N>
        # objects moving simulation
        cx, cy = update_positions(cx, cy)
        raw_x, raw_y = calc_raw_data(cx, cy, n_points, rand_d)<N><N>
        clusters = kmeans_clustering(raw_x, raw_y, n_cluster)<N><N>
        # for animation
        if show_animation:  # pragma: no cover
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect('key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])
            clusters.plot_cluster()
            plt.plot(cx, cy, "or")
            plt.xlim(-2.0, 10.0)
            plt.ylim(-2.0, 10.0)
            plt.pause(dt)<N><N>
    print("Done")<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>LIDAR to 2D grid map example<N><N>
author: Erno Horvath, Csaba Hajdu based on Atsushi Sakai's scripts<N><N>
"""<N><N>import math
from collections import deque<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
EXTEND_AREA = 1.0<N><N>

def file_read(f):
    """
    Reading LIDAR laser beams (angles and corresponding distance data)
    """
    with open(f) as data:
        measures = [line.split(",") for line in data]
    angles = []
    distances = []
    for measure in measures:
        angles.append(float(measure[0]))
        distances.append(float(measure[1]))
    angles = np.array(angles)
    distances = np.array(distances)
    return angles, distances<N><N>
"""<N><N>Ray casting 2D grid map example<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math
import numpy as np
import matplotlib.pyplot as plt<N><N>
EXTEND_AREA = 10.0<N><N>
show_animation = True<N><N>

def calc_grid_map_config(ox, oy, xyreso):
    minx = round(min(ox) - EXTEND_AREA / 2.0)
    miny = round(min(oy) - EXTEND_AREA / 2.0)
    maxx = round(max(ox) + EXTEND_AREA / 2.0)
    maxy = round(max(oy) + EXTEND_AREA / 2.0)
    xw = int(round((maxx - minx) / xyreso))
    yw = int(round((maxy - miny) / xyreso))<N><N>
    return minx, miny, maxx, maxy, xw, yw<N><N>

class precastDB:<N><N>
    def __init__(self):
        self.px = 0.0
        self.py = 0.0
        self.d = 0.0
        self.angle = 0.0
        self.ix = 0
        self.iy = 0<N><N>
    def __str__(self):
        return str(self.px) + "," + str(self.py) + "," + str(self.d) + "," + str(self.angle)<N><N>

def atan_zero_to_twopi(y, x):
    angle = math.atan2(y, x)
    if angle < 0.0:
        angle += math.pi * 2.0<N><N>
    return angle<N><N>

def precasting(minx, miny, xw, yw, xyreso, yawreso):<N><N>
    precast = [[] for i in range(int(round((math.pi * 2.0) / yawreso)) + 1)]<N><N>
    for ix in range(xw):
        for iy in range(yw):
            px = ix * xyreso + minx
            py = iy * xyreso + miny<N><N>
            d = math.hypot(px, py)
            angle = atan_zero_to_twopi(py, px)
            angleid = int(math.floor(angle / yawreso))<N><N>
            pc = precastDB()<N><N>
            pc.px = px
            pc.py = py
            pc.d = d
            pc.ix = ix
            pc.iy = iy
            pc.angle = angle<N><N>
            precast[angleid].append(pc)<N><N>
    return precast<N><N>

def generate_ray_casting_grid_map(ox, oy, xyreso, yawreso):<N><N>
    minx, miny, maxx, maxy, xw, yw = calc_grid_map_config(ox, oy, xyreso)<N><N>
    pmap = [[0.0 for i in range(yw)] for i in range(xw)]<N><N>
    precast = precasting(minx, miny, xw, yw, xyreso, yawreso)<N><N>
    for (x, y) in zip(ox, oy):<N><N>
        d = math.hypot(x, y)
        angle = atan_zero_to_twopi(y, x)
        angleid = int(math.floor(angle / yawreso))<N><N>
        gridlist = precast[angleid]<N><N>
        ix = int(round((x - minx) / xyreso))
        iy = int(round((y - miny) / xyreso))<N><N>
        for grid in gridlist:
            if grid.d > d:
                pmap[grid.ix][grid.iy] = 0.5<N><N>
        pmap[ix][iy] = 1.0<N><N>
    return pmap, minx, maxx, miny, maxy, xyreso<N><N>

def draw_heatmap(data, minx, maxx, miny, maxy, xyreso):
    x, y = np.mgrid[slice(minx - xyreso / 2.0, maxx + xyreso / 2.0, xyreso),
                    slice(miny - xyreso / 2.0, maxy + xyreso / 2.0, xyreso)]
    plt.pcolor(x, y, data, vmax=1.0, cmap=plt.cm.Blues)
    plt.axis("equal")<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    xyreso = 0.25  # x-y grid resolution [m]
    yawreso = np.deg2rad(10.0)  # yaw angle resolution [rad]<N><N>
    for i in range(5):
        ox = (np.random.rand(4) - 0.5) * 10.0
        oy = (np.random.rand(4) - 0.5) * 10.0
        pmap, minx, maxx, miny, maxy, xyreso = generate_ray_casting_grid_map(
            ox, oy, xyreso, yawreso)<N><N>
        if show_animation:  # pragma: no cover
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect('key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])
            draw_heatmap(pmap, minx, maxx, miny, maxy, xyreso)
            plt.plot(ox, oy, "xr")
            plt.plot(0.0, 0.0, "ob")
            plt.pause(1.0)<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Object shape recognition with L-shape fitting<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:
- Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners -
The Robotics Institute Carnegie Mellon University
https://www.ri.cmu.edu/publications/
efficient-l-shape-fitting-for-vehicle-detection-using-laser-scanners/<N><N>
"""<N><N>import matplotlib.pyplot as plt
import numpy as np
import itertools
from enum import Enum
from scipy.spatial.transform import Rotation as Rot<N><N>
from Mapping.rectangle_fitting.simulator \
    import VehicleSimulator, LidarSimulator<N><N>
show_animation = True<N><N>

class LShapeFitting:<N><N>
    class Criteria(Enum):
        AREA = 1
        CLOSENESS = 2
        VARIANCE = 3<N><N>
    def __init__(self):
        # Parameters
        self.criteria = self.Criteria.VARIANCE
        self.min_dist_of_closeness_criteria = 0.01  # [m]
        self.d_theta_deg_for_search = 1.0  # [deg]
        self.R0 = 3.0  # [m] range segmentation param
        self.Rd = 0.001  # [m] range segmentation param<N><N>
    def fitting(self, ox, oy):<N><N>
        # step1: Adaptive Range Segmentation
        id_sets = self._adoptive_range_segmentation(ox, oy)<N><N>
        # step2 Rectangle search
        rects = []
        for ids in id_sets:  # for each cluster
            cx = [ox[i] for i in range(len(ox)) if i in ids]
            cy = [oy[i] for i in range(len(oy)) if i in ids]
            rects.append(self._rectangle_search(cx, cy))<N><N>
        return rects, id_sets<N><N>
    @staticmethod
    def _calc_area_criterion(c1, c2):
        c1_max = max(c1)
        c2_max = max(c2)
        c1_min = min(c1)
        c2_min = min(c2)<N><N>
        alpha = -(c1_max - c1_min) * (c2_max - c2_min)<N><N>
        return alpha<N><N>
    def _calc_closeness_criterion(self, c1, c2):
        c1_max = max(c1)
        c2_max = max(c2)
        c1_min = min(c1)
        c2_min = min(c2)<N><N>
        # Vectorization
        D1 = np.minimum(c1_max - c1, c1 - c1_min)
        D2 = np.minimum(c2_max - c2, c2 - c2_min)
        d = np.maximum(np.minimum(D1, D2), self.min_dist_of_closeness_criteria)
        beta = (1.0 / d).sum()<N><N>
        return beta<N><N>
    @staticmethod
    def _calc_variance_criterion(c1, c2):
        c1_max = max(c1)
        c2_max = max(c2)
        c1_min = min(c1)
        c2_min = min(c2)<N><N>
        # Vectorization
        D1 = np.minimum(c1_max - c1, c1 - c1_min)
        D2 = np.minimum(c2_max - c2, c2 - c2_min)
        E1 = D1[D1 < D2]
        E2 = D2[D1 >= D2]
        V1 = - np.var(E1) if len(E1) > 0 else 0.
        V2 = - np.var(E2) if len(E2) > 0 else 0.
        gamma = V1 + V2<N><N>
        return gamma<N><N>
    def _rectangle_search(self, x, y):<N><N>
        X = np.array([x, y]).T<N><N>
        d_theta = np.deg2rad(self.d_theta_deg_for_search)
        min_cost = (-float('inf'), None)
        for theta in np.arange(0.0, np.pi / 2.0 - d_theta, d_theta):<N><N>
            rot = Rot.from_euler('z', theta).as_matrix()[0:2, 0:2]
            c = X @ rot
            c1 = c[:, 0]
            c2 = c[:, 1]<N><N>
            # Select criteria
            cost = 0.0
            if self.criteria == self.Criteria.AREA:
                cost = self._calc_area_criterion(c1, c2)
            elif self.criteria == self.Criteria.CLOSENESS:
                cost = self._calc_closeness_criterion(c1, c2)
            elif self.criteria == self.Criteria.VARIANCE:
                cost = self._calc_variance_criterion(c1, c2)<N><N>
            if min_cost[0] < cost:
                min_cost = (cost, theta)<N><N>
        # calc best rectangle
        sin_s = np.sin(min_cost[1])
        cos_s = np.cos(min_cost[1])<N><N>
        c1_s = X @ np.array([cos_s, sin_s]).T
        c2_s = X @ np.array([-sin_s, cos_s]).T<N><N>
        rect = RectangleData()
        rect.a[0] = cos_s
        rect.b[0] = sin_s
        rect.c[0] = min(c1_s)
        rect.a[1] = -sin_s
        rect.b[1] = cos_s
        rect.c[1] = min(c2_s)
        rect.a[2] = cos_s
        rect.b[2] = sin_s
        rect.c[2] = max(c1_s)
        rect.a[3] = -sin_s
        rect.b[3] = cos_s
        rect.c[3] = max(c2_s)<N><N>
        return rect<N><N>
    def _adoptive_range_segmentation(self, ox, oy):<N><N>
        # Setup initial cluster
        S = []
        for i, _ in enumerate(ox):
            C = set()
            R = self.R0 + self.Rd * np.linalg.norm([ox[i], oy[i]])
            for j, _ in enumerate(ox):
                d = np.hypot(ox[i] - ox[j], oy[i] - oy[j])
                if d <= R:
                    C.add(j)
            S.append(C)<N><N>
        # Merge cluster
        while 1:
            no_change = True
            for (c1, c2) in list(itertools.permutations(range(len(S)), 2)):
                if S[c1] & S[c2]:
                    S[c1] = (S[c1] | S.pop(c2))
                    no_change = False
                    break
            if no_change:
                break<N><N>
        return S<N><N>

class RectangleData:<N><N>
    def __init__(self):
        self.a = [None] * 4
        self.b = [None] * 4
        self.c = [None] * 4<N><N>
        self.rect_c_x = [None] * 5
        self.rect_c_y = [None] * 5<N><N>
    def plot(self):
        self.calc_rect_contour()
        plt.plot(self.rect_c_x, self.rect_c_y, "-r")<N><N>
    def calc_rect_contour(self):<N><N>
""" <N><N>
Simulator<N><N>
author: Atsushi Sakai<N><N>
"""<N><N>import numpy as np
import matplotlib.pyplot as plt
import math
import random
from scipy.spatial.transform import Rotation as Rot<N><N>

class VehicleSimulator:<N><N>
    def __init__(self, i_x, i_y, i_yaw, i_v, max_v, w, L):
        self.x = i_x
        self.y = i_y
        self.yaw = i_yaw
        self.v = i_v
        self.max_v = max_v
        self.W = w
        self.L = L
        self._calc_vehicle_contour()<N><N>
    def update(self, dt, a, omega):
        self.x += self.v * np.cos(self.yaw) * dt
        self.y += self.v * np.sin(self.yaw) * dt
        self.yaw += omega * dt
        self.v += a * dt
        if self.v >= self.max_v:
            self.v = self.max_v<N><N>
    def plot(self):
        plt.plot(self.x, self.y, ".b")<N><N>
        # convert global coordinate
        gx, gy = self.calc_global_contour()
        plt.plot(gx, gy, "--b")<N><N>
    def calc_global_contour(self):
        rot = Rot.from_euler('z', self.yaw).as_matrix()[0:2, 0:2]
        gxy = np.stack([self.vc_x, self.vc_y]).T @ rot
        gx = gxy[:, 0] + self.x
        gy = gxy[:, 1] + self.y<N><N>
        return gx, gy<N><N>
    def _calc_vehicle_contour(self):<N><N>
        self.vc_x = []
        self.vc_y = []<N><N>
        self.vc_x.append(self.L / 2.0)
        self.vc_y.append(self.W / 2.0)<N><N>
        self.vc_x.append(self.L / 2.0)
        self.vc_y.append(-self.W / 2.0)<N><N>
        self.vc_x.append(-self.L / 2.0)
        self.vc_y.append(-self.W / 2.0)<N><N>
        self.vc_x.append(-self.L / 2.0)
        self.vc_y.append(self.W / 2.0)<N><N>
        self.vc_x.append(self.L / 2.0)
        self.vc_y.append(self.W / 2.0)<N><N>
        self.vc_x, self.vc_y = self._interpolate(self.vc_x, self.vc_y)<N><N>
    @staticmethod
    def _interpolate(x, y):
        rx, ry = [], []
        d_theta = 0.05
        for i in range(len(x) - 1):
            rx.extend([(1.0 - theta) * x[i] + theta * x[i + 1]
                       for theta in np.arange(0.0, 1.0, d_theta)])
            ry.extend([(1.0 - theta) * y[i] + theta * y[i + 1]
                       for theta in np.arange(0.0, 1.0, d_theta)])<N><N>
        rx.extend([(1.0 - theta) * x[len(x) - 1] + theta * x[1]
                   for theta in np.arange(0.0, 1.0, d_theta)])
        ry.extend([(1.0 - theta) * y[len(y) - 1] + theta * y[1]
                   for theta in np.arange(0.0, 1.0, d_theta)])<N><N>
        return rx, ry<N><N>

class LidarSimulator:<N><N>
    def __init__(self):
        self.range_noise = 0.01<N><N>
    def get_observation_points(self, v_list, angle_resolution):
        x, y, angle, r = [], [], [], []<N><N>
        # store all points
        for v in v_list:<N><N>
            gx, gy = v.calc_global_contour()<N><N>
            for vx, vy in zip(gx, gy):
                v_angle = math.atan2(vy, vx)
                vr = np.hypot(vx, vy) * random.uniform(1.0 - self.range_noise,
                                                       1.0 + self.range_noise)<N><N>
                x.append(vx)
                y.append(vy)
                angle.append(v_angle)
                r.append(vr)<N><N>
        # ray casting filter
        rx, ry = self.ray_casting_filter(angle, r, angle_resolution)<N><N>
        return rx, ry<N><N>
    @staticmethod
    def ray_casting_filter(theta_l, range_l, angle_resolution):
        rx, ry = [], []
        range_db = [float("inf") for _ in range(
            int(np.floor((np.pi * 2.0) / angle_resolution)) + 1)]<N><N>
        for i in range(len(theta_l)):
            angle_id = int(round(theta_l[i] / angle_resolution))<N><N>
            if range_db[angle_id] > range_l[i]:
                range_db[angle_id] = range_l[i]<N><N>
        for i in range(len(range_db)):
            t = i * angle_resolution
            if range_db[i] != float("inf"):
                rx.append(range_db[i] * np.cos(t))
                ry.append(range_db[i] * np.sin(t))<N><N>
        return rx, ry<N><N>

def main():
    print("start!!")<N><N>
    print("done!!")<N><N>

if __name__ == '__main__':
    main()
<N><N>
import os
import sys<N><N>sys.path.append(os.path.dirname(os.path.abspath(__file__)))

"""<N><N>A* grid planning<N><N>
author: Atsushi Sakai(@Atsushi_twi)
        Nikos Kanargias (nkana@tee.gr)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/A*_search_algorithm)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class AStarPlanner:<N><N>
    def __init__(self, ox, oy, resolution, rr):
        """
        Initialize grid map for a star planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.resolution = resolution
        self.rr = rr
        self.min_x, self.min_y = 0, 0
        self.max_x, self.max_y = 0, 0
        self.obstacle_map = None
        self.x_width, self.y_width = 0, 0
        self.motion = self.get_motion_model()
        self.calc_obstacle_map(ox, oy)<N><N>
    class Node:
        def __init__(self, x, y, cost, parent_index):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.parent_index = parent_index<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent_index)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        A star path search<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gy: goal y position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        start_node = self.Node(self.calc_xy_index(sx, self.min_x),
                               self.calc_xy_index(sy, self.min_y), 0.0, -1)
        goal_node = self.Node(self.calc_xy_index(gx, self.min_x),
                              self.calc_xy_index(gy, self.min_y), 0.0, -1)<N><N>
        open_set, closed_set = dict(), dict()
        open_set[self.calc_grid_index(start_node)] = start_node<N><N>
        while 1:
            if len(open_set) == 0:
                print("Open set is empty..")
                break<N><N>
            c_id = min(
                open_set,
                key=lambda o: open_set[o].cost + self.calc_heuristic(goal_node,
                                                                     open_set[
                                                                         o]))
            current = open_set[c_id]<N><N>
"""
A* algorithm
Author: Weicent
randomly generate obstacles, start and goal point
searching path from start and end simultaneously
"""<N><N>
import numpy as np
import matplotlib.pyplot as plt
import math<N><N>
show_animation = True<N><N>

class Node:
    """node with properties of g, h, coordinate and parent node"""<N><N>
    def __init__(self, G=0, H=0, coordinate=None, parent=None):
        self.G = G
        self.H = H
        self.F = G + H
        self.parent = parent
        self.coordinate = coordinate<N><N>
    def reset_f(self):
        self.F = self.G + self.H<N><N>

def hcost(node_coordinate, goal):
    dx = abs(node_coordinate[0] - goal[0])
    dy = abs(node_coordinate[1] - goal[1])
    hcost = dx + dy
    return hcost<N><N>

def gcost(fixed_node, update_node_coordinate):
    dx = abs(fixed_node.coordinate[0] - update_node_coordinate[0])
    dy = abs(fixed_node.coordinate[1] - update_node_coordinate[1])
    gc = math.hypot(dx, dy)  # gc = move from fixed_node to update_node
    gcost = fixed_node.G + gc  # gcost = move from start point to update_node
    return gcost<N><N>
"""
a star variants
author: Sarim Mehdi(muhammadsarim.mehdi@studio.unibo.it)
Source: http://theory.stanford.edu/~amitp/GameProgramming/Variations.html
"""<N><N>
import numpy as np
import matplotlib.pyplot as plt<N><N>
show_animation = True
use_beam_search = False
use_iterative_deepening = False
use_dynamic_weighting = False
use_theta_star = False
use_jump_point = False<N><N>
beam_capacity = 30
max_theta = 5
only_corners = False
max_corner = 5
w, epsilon, upper_bound_depth = 1, 4, 500<N><N>

def draw_horizontal_line(start_x, start_y, length, o_x, o_y, o_dict):
    for i in range(start_x, start_x + length):
        for j in range(start_y, start_y + 2):
            o_x.append(i)
            o_y.append(j)
            o_dict[(i, j)] = True<N><N>

def draw_vertical_line(start_x, start_y, length, o_x, o_y, o_dict):
    for i in range(start_x, start_x + 2):
        for j in range(start_y, start_y + length):
            o_x.append(i)
            o_y.append(j)
            o_dict[(i, j)] = True<N><N>

def in_line_of_sight(obs_grid, x1, y1, x2, y2):
    t = 0
    while t <= 0.5:
        xt = (1 - t) * x1 + t * x2
        yt = (1 - t) * y1 + t * y2
        if obs_grid[(int(xt), int(yt))]:
            return False, None
        xt = (1 - t) * x2 + t * x1
        yt = (1 - t) * y2 + t * y1
        if obs_grid[(int(xt), int(yt))]:
            return False, None
        t += 0.001
    dist = np.linalg.norm(np.array([x1, y1] - np.array([x2, y2])))
    return True, dist<N><N>
"""
Batch Informed Trees based path planning:
Uses a heuristic to efficiently search increasingly dense
RGGs while reusing previous information. Provides faster
convergence that RRT*, Informed RRT* and other sampling based
methods.<N><N>
Uses lazy connecting by combining sampling based methods and A*
like incremental graph search algorithms.<N><N>
author: Karan Chawla(@karanchawla)
        Atsushi Sakai(@Atsushi_twi)<N><N>
Reference: https://arxiv.org/abs/1405.5848
"""<N><N>
import math
import random<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
show_animation = True<N><N>

class RTree(object):
    # Class to represent the explicit tree created
    # while sampling through the state space<N><N>
    def __init__(self, start=None, lowerLimit=None, upperLimit=None,
                 resolution=1.0):<N><N>
"""<N><N>Path planning with Bezier curve.<N><N>
author: Atsushi Sakai(@Atsushi_twi)<N><N>
"""<N><N>import matplotlib.pyplot as plt
import numpy as np
import scipy.special<N><N>
show_animation = True<N><N>

def calc_4points_bezier_path(sx, sy, syaw, ex, ey, eyaw, offset):
    """
    Compute control points and path given start and end position.<N><N>
"""<N><N>Bidirectional A* grid planning<N><N>
author: Erwin Lejeune (@spida_rwin)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/Bidirectional_search)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class BidirectionalAStarPlanner:<N><N>
    def __init__(self, ox, oy, resolution, rr):
        """
        Initialize grid map for a star planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.min_x, self.min_y = None, None
        self.max_x, self.max_y = None, None
        self.x_width, self.y_width, self.obstacle_map = None, None, None
        self.resolution = resolution
        self.rr = rr
        self.calc_obstacle_map(ox, oy)
        self.motion = self.get_motion_model()<N><N>
    class Node:
        def __init__(self, x, y, cost, parent_index):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.parent_index = parent_index<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent_index)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        Bidirectional A star path search<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gy: goal y position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        start_node = self.Node(self.calc_xy_index(sx, self.min_x),
                               self.calc_xy_index(sy, self.min_y), 0.0, -1)
        goal_node = self.Node(self.calc_xy_index(gx, self.min_x),
                              self.calc_xy_index(gy, self.min_y), 0.0, -1)<N><N>
        open_set_A, closed_set_A = dict(), dict()
        open_set_B, closed_set_B = dict(), dict()
        open_set_A[self.calc_grid_index(start_node)] = start_node
        open_set_B[self.calc_grid_index(goal_node)] = goal_node<N><N>
        current_A = start_node
        current_B = goal_node
        meet_point_A, meet_point_B = None, None<N><N>
        while 1:
            if len(open_set_A) == 0:
                print("Open set A is empty..")
                break<N><N>
            if len(open_set_B) == 0:
                print("Open set B is empty..")
                break<N><N>
            c_id_A = min(
                open_set_A,
                key=lambda o: self.find_total_cost(open_set_A, o, current_B))<N><N>
            current_A = open_set_A[c_id_A]<N><N>
            c_id_B = min(
                open_set_B,
                key=lambda o: self.find_total_cost(open_set_B, o, current_A))<N><N>
            current_B = open_set_B[c_id_B]<N><N>
"""<N><N>Bidirectional Breadth-First grid planning<N><N>
author: Erwin Lejeune (@spida_rwin)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/Breadth-first_search)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class BidirectionalBreadthFirstSearchPlanner:<N><N>
    def __init__(self, ox, oy, resolution, rr):
        """
        Initialize grid map for bfs planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.min_x, self.min_y = None, None
        self.max_x, self.max_y = None, None
        self.x_width, self.y_width, self.obstacle_map = None, None, None
        self.resolution = resolution
        self.rr = rr
        self.calc_obstacle_map(ox, oy)
        self.motion = self.get_motion_model()<N><N>
    class Node:
        def __init__(self, x, y, cost, parent_index, parent):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.parent_index = parent_index
            self.parent = parent<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent_index)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        Bidirectional Breadth First search based planning<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gy: goal y position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        start_node = self.Node(self.calc_xy_index(sx, self.min_x),
                               self.calc_xy_index(sy, self.min_y), 0.0, -1,
                               None)
        goal_node = self.Node(self.calc_xy_index(gx, self.min_x),
                              self.calc_xy_index(gy, self.min_y), 0.0, -1,
                              None)<N><N>
        open_set_A, closed_set_A = dict(), dict()
        open_set_B, closed_set_B = dict(), dict()
        open_set_B[self.calc_grid_index(goal_node)] = goal_node
        open_set_A[self.calc_grid_index(start_node)] = start_node<N><N>
        meet_point_A, meet_point_B = None, None<N><N>
        while 1:
            if len(open_set_A) == 0:
                print("Open set A is empty..")
                break<N><N>
            if len(open_set_B) == 0:
                print("Open set B is empty")
                break<N><N>
            current_A = open_set_A.pop(list(open_set_A.keys())[0])
            current_B = open_set_B.pop(list(open_set_B.keys())[0])<N><N>
            c_id_A = self.calc_grid_index(current_A)
            c_id_B = self.calc_grid_index(current_B)<N><N>
            closed_set_A[c_id_A] = current_A
            closed_set_B[c_id_B] = current_B<N><N>
"""<N><N>Breadth-First grid planning<N><N>
author: Erwin Lejeune (@spida_rwin)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/Breadth-first_search)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class BreadthFirstSearchPlanner:<N><N>
    def __init__(self, ox, oy, reso, rr):
        """
        Initialize grid map for bfs planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.reso = reso
        self.rr = rr
        self.calc_obstacle_map(ox, oy)
        self.motion = self.get_motion_model()<N><N>
    class Node:
        def __init__(self, x, y, cost, parent_index, parent):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.parent_index = parent_index
            self.parent = parent<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent_index)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        Breadth First search based planning<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gy: goal y position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        nstart = self.Node(self.calc_xyindex(sx, self.minx),
                           self.calc_xyindex(sy, self.miny), 0.0, -1, None)
        ngoal = self.Node(self.calc_xyindex(gx, self.minx),
                          self.calc_xyindex(gy, self.miny), 0.0, -1, None)<N><N>
        open_set, closed_set = dict(), dict()
        open_set[self.calc_grid_index(nstart)] = nstart<N><N>
        while 1:
            if len(open_set) == 0:
                print("Open set is empty..")
                break<N><N>
            current = open_set.pop(list(open_set.keys())[0])<N><N>
            c_id = self.calc_grid_index(current)<N><N>
            closed_set[c_id] = current<N><N>
"""<N><N>Path Planner with B-Spline<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import numpy as np
import matplotlib.pyplot as plt
import scipy.interpolate as scipy_interpolate<N><N>

def approximate_b_spline_path(x: list, y: list, n_path_points: int,
                              degree: int = 3) -> tuple:
    """
    approximate points with a B-Spline path<N><N>
    :param x: x position list of approximated points
    :param y: y position list of approximated points
    :param n_path_points: number of path points
    :param degree: (Optional) B Spline curve degree
    :return: x and y position list of the result path
    """
    t = range(len(x))
    x_tup = scipy_interpolate.splrep(t, x, k=degree)
    y_tup = scipy_interpolate.splrep(t, y, k=degree)<N><N>
    x_list = list(x_tup)
    x_list[1] = x + [0.0, 0.0, 0.0, 0.0]<N><N>
    y_list = list(y_tup)
    y_list[1] = y + [0.0, 0.0, 0.0, 0.0]<N><N>
    ipl_t = np.linspace(0.0, len(x) - 1, n_path_points)
    rx = scipy_interpolate.splev(ipl_t, x_list)
    ry = scipy_interpolate.splev(ipl_t, y_list)<N><N>
    return rx, ry<N><N>

def interpolate_b_spline_path(x: list, y: list, n_path_points: int,
                              degree: int = 3) -> tuple:
    """
    interpolate points with a B-Spline path<N><N>
    :param x: x positions of interpolated points
    :param y: y positions of interpolated points
    :param n_path_points: number of path points
    :param degree: B-Spline degree
    :return: x and y position list of the result path
    """
    ipl_t = np.linspace(0.0, len(x) - 1, len(x))
    spl_i_x = scipy_interpolate.make_interp_spline(ipl_t, x, k=degree)
    spl_i_y = scipy_interpolate.make_interp_spline(ipl_t, y, k=degree)<N><N>
    travel = np.linspace(0.0, len(x) - 1, n_path_points)
    return spl_i_x(travel), spl_i_y(travel)<N><N>

def main():
    print(__file__ + " start!!")
    # way points
    way_point_x = [-1.0, 3.0, 4.0, 2.0, 1.0]
    way_point_y = [0.0, -3.0, 1.0, 1.0, 3.0]
    n_course_point = 100  # sampling number<N><N>
    rax, ray = approximate_b_spline_path(way_point_x, way_point_y,
                                         n_course_point)
    rix, riy = interpolate_b_spline_path(way_point_x, way_point_y,
                                         n_course_point)<N><N>
    # show results
    plt.plot(way_point_x, way_point_y, '-og', label="way points")
    plt.plot(rax, ray, '-r', label="Approximated B-Spline path")
    plt.plot(rix, riy, '-b', label="Interpolated B-Spline path")
    plt.grid(True)
    plt.legend()
    plt.axis("equal")
    plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""
Bug Planning
author: Sarim Mehdi(muhammadsarim.mehdi@studio.unibo.it)
Source: https://sites.google.com/site/ece452bugalgorithms/
"""<N><N>
import numpy as np
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>
"""<N><N>Path planning Sample Code with Closed loop RRT for car like robot.<N><N>
author: AtsushiSakai(@Atsushi_twi)<N><N>
"""<N><N>import matplotlib.pyplot as plt
import numpy as np
import reeds_shepp_path_planning
from rrt_star_reeds_shepp import RRTStarReedsShepp<N><N>
import pure_pursuit
import unicycle_model<N><N>
show_animation = True<N><N>

class ClosedLoopRRTStar(RRTStarReedsShepp):
    """
    Class for Closed loop RRT star planning
    """<N><N>
    def __init__(self, start, goal, obstacle_list, rand_area,
                 max_iter=200,
                 connect_circle_dist=50.0
                 ):
        super().__init__(start, goal, obstacle_list, rand_area,
                         max_iter=max_iter,
                         connect_circle_dist=connect_circle_dist,
                         )<N><N>
        self.target_speed = 10.0 / 3.6
        self.yaw_th = np.deg2rad(3.0)
        self.xy_th = 0.5
        self.invalid_travel_ratio = 5.0<N><N>
    def planning(self, animation=True):
        """
        do planning<N><N>
        animation: flag for animation on or off
        """
        # planning with RRTStarReedsShepp
        super().planning(animation=animation)<N><N>
        # generate coruse
        path_indexs = self.get_goal_indexes()<N><N>
        flag, x, y, yaw, v, t, a, d = self.search_best_feasible_path(
            path_indexs)<N><N>
        return flag, x, y, yaw, v, t, a, d<N><N>
    def search_best_feasible_path(self, path_indexs):<N><N>
        print("Start search feasible path")<N><N>
        best_time = float("inf")<N><N>
        fx, fy, fyaw, fv, ft, fa, fd = None, None, None, None, None, None, None<N><N>
        # pure pursuit tracking
        for ind in path_indexs:
            path = self.generate_final_course(ind)<N><N>
            flag, x, y, yaw, v, t, a, d = self.check_tracking_path_is_feasible(
                path)<N><N>
            if flag and best_time >= t[-1]:
                print("feasible path is found")
                best_time = t[-1]
                fx, fy, fyaw, fv, ft, fa, fd = x, y, yaw, v, t, a, d<N><N>
        print("best time is")
        print(best_time)<N><N>
        if fx:
            fx.append(self.end.x)
            fy.append(self.end.y)
            fyaw.append(self.end.yaw)
            return True, fx, fy, fyaw, fv, ft, fa, fd<N><N>
        return False, None, None, None, None, None, None, None<N><N>
    def check_tracking_path_is_feasible(self, path):
        cx = np.array([state[0] for state in path])[::-1]
        cy = np.array([state[1] for state in path])[::-1]
        cyaw = np.array([state[2] for state in path])[::-1]<N><N>
        goal = [cx[-1], cy[-1], cyaw[-1]]<N><N>
        cx, cy, cyaw = pure_pursuit.extend_path(cx, cy, cyaw)<N><N>
        speed_profile = pure_pursuit.calc_speed_profile(
            cx, cy, cyaw, self.target_speed)<N><N>
        t, x, y, yaw, v, a, d, find_goal = pure_pursuit.closed_loop_prediction(
            cx, cy, cyaw, speed_profile, goal)
        yaw = [reeds_shepp_path_planning.pi_2_pi(iyaw) for iyaw in yaw]<N><N>
        if not find_goal:
            print("cannot reach goal")<N><N>
        if abs(yaw[-1] - goal[2]) >= self.yaw_th * 10.0:
            print("final angle is bad")
            find_goal = False<N><N>
        travel = unicycle_model.dt * sum(np.abs(v))
        origin_travel = sum(np.hypot(np.diff(cx), np.diff(cy)))<N><N>
        if (travel / origin_travel) >= self.invalid_travel_ratio:
            print("path is too long")
            find_goal = False<N><N>
        if not self.collision_check_with_xy(x, y, self.obstacle_list):
            print("This path is collision")
            find_goal = False<N><N>
        return find_goal, x, y, yaw, v, t, a, d<N><N>
    def get_goal_indexes(self):
        goalinds = []
        for (i, node) in enumerate(self.node_list):
            if self.calc_dist_to_goal(node.x, node.y) <= self.xy_th:
                goalinds.append(i)
        print("OK XY TH num is")
        print(len(goalinds))<N><N>
        # angle check
        fgoalinds = []
        for i in goalinds:
            if abs(self.node_list[i].yaw - self.end.yaw) <= self.yaw_th:
                fgoalinds.append(i)
        print("OK YAW TH num is")
        print(len(fgoalinds))<N><N>
        return fgoalinds<N><N>
    @staticmethod
    def collision_check_with_xy(x, y, obstacle_list):<N><N>
        for (ox, oy, size) in obstacle_list:
            for (ix, iy) in zip(x, y):
                dx = ox - ix
                dy = oy - iy
                d = dx * dx + dy * dy
                if d <= size ** 2:
                    return False  # collision<N><N>
        return True  # safe<N><N>

def main(gx=6.0, gy=7.0, gyaw=np.deg2rad(90.0), max_iter=100):
    print("Start" + __file__)
    # ====Search Path with RRT====
    obstacle_list = [
        (5, 5, 1),
        (4, 6, 1),
        (4, 8, 1),
        (4, 10, 1),
        (6, 5, 1),
        (7, 5, 1),
        (8, 6, 1),
        (8, 8, 1),
        (8, 10, 1)
    ]  # [x,y,size(radius)]<N><N>
    # Set Initial parameters
    start = [0.0, 0.0, np.deg2rad(0.0)]
    goal = [gx, gy, gyaw]<N><N>
    closed_loop_rrt_star = ClosedLoopRRTStar(start, goal,
                                             obstacle_list,
                                             [-2.0, 20.0],
                                             max_iter=max_iter)
    flag, x, y, yaw, v, t, a, d = closed_loop_rrt_star.planning(
        animation=show_animation)<N><N>
    if not flag:
        print("cannot find feasible path")<N><N>
    # Draw final path
    if show_animation:
        closed_loop_rrt_star.draw_graph()
        plt.plot(x, y, '-r')
        plt.grid(True)
        plt.pause(0.001)<N><N>
        plt.subplots(1)
        plt.plot(t, [np.rad2deg(iyaw) for iyaw in yaw[:-1]], '-r')
        plt.xlabel("time[s]")
        plt.ylabel("Yaw[deg]")
        plt.grid(True)<N><N>
        plt.subplots(1)
        plt.plot(t, [iv * 3.6 for iv in v], '-r')<N><N>
        plt.xlabel("time[s]")
        plt.ylabel("velocity[km/h]")
        plt.grid(True)<N><N>
        plt.subplots(1)
        plt.plot(t, a, '-r')
        plt.xlabel("time[s]")
        plt.ylabel("accel[m/ss]")
        plt.grid(True)<N><N>
        plt.subplots(1)
        plt.plot(t, [np.rad2deg(td) for td in d], '-r')
        plt.xlabel("time[s]")
        plt.ylabel("Steering angle[deg]")
        plt.grid(True)<N><N>
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Path tracking simulation with pure pursuit steering control and PID speed control.<N><N>
author: Atsushi Sakai<N><N>
"""
import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
import unicycle_model<N><N>
Kp = 2.0  # speed propotional gain
Lf = 0.5  # look-ahead distance
T = 100.0  # max simulation time
goal_dis = 0.5
stop_speed = 0.5<N><N>
#  animation = True
animation = False<N><N>

def PIDControl(target, current):
    a = Kp * (target - current)<N><N>
    if a > unicycle_model.accel_max:
        a = unicycle_model.accel_max
    elif a < -unicycle_model.accel_max:
        a = -unicycle_model.accel_max<N><N>
    return a<N><N>

def pure_pursuit_control(state, cx, cy, pind):<N><N>
    ind, dis = calc_target_index(state, cx, cy)<N><N>
    if pind >= ind:
        ind = pind<N><N>
    #  print(parent_index, ind)
    if ind < len(cx):
        tx = cx[ind]
        ty = cy[ind]
    else:
        tx = cx[-1]
        ty = cy[-1]
        ind = len(cx) - 1<N><N>
    alpha = math.atan2(ty - state.y, tx - state.x) - state.yaw<N><N>
    if state.v <= 0.0:  # back
        alpha = math.pi - alpha<N><N>
    delta = math.atan2(2.0 * unicycle_model.L * math.sin(alpha) / Lf, 1.0)<N><N>
    if delta > unicycle_model.steer_max:
        delta = unicycle_model.steer_max
    elif delta < - unicycle_model.steer_max:
        delta = -unicycle_model.steer_max<N><N>
    return delta, ind, dis<N><N>

def calc_target_index(state, cx, cy):
    dx = [state.x - icx for icx in cx]
    dy = [state.y - icy for icy in cy]<N><N>
    d = np.hypot(dx, dy)
    mindis = min(d)<N><N>
    ind = np.argmin(d)<N><N>
    L = 0.0<N><N>
    while Lf > L and (ind + 1) < len(cx):
        dx = cx[ind + 1] - cx[ind]
        dy = cy[ind + 1] - cy[ind]
        L += math.hypot(dx, dy)
        ind += 1<N><N>
    #  print(mindis)
    return ind, mindis<N><N>

def closed_loop_prediction(cx, cy, cyaw, speed_profile, goal):<N><N>
    state = unicycle_model.State(x=-0.0, y=-0.0, yaw=0.0, v=0.0)<N><N>
    #  lastIndex = len(cx) - 1
    time = 0.0
    x = [state.x]
    y = [state.y]
    yaw = [state.yaw]
    v = [state.v]
    t = [0.0]
    a = [0.0]
    d = [0.0]
    target_ind, mindis = calc_target_index(state, cx, cy)
    find_goal = False<N><N>
    maxdis = 0.5<N><N>
    while T >= time:
        di, target_ind, dis = pure_pursuit_control(state, cx, cy, target_ind)<N><N>
        target_speed = speed_profile[target_ind]
        target_speed = target_speed * \
            (maxdis - min(dis, maxdis - 0.1)) / maxdis<N><N>
        ai = PIDControl(target_speed, state.v)
        state = unicycle_model.update(state, ai, di)<N><N>
        if abs(state.v) <= stop_speed and target_ind <= len(cx) - 2:
            target_ind += 1<N><N>
        time = time + unicycle_model.dt<N><N>
        # check goal
        dx = state.x - goal[0]
        dy = state.y - goal[1]
        if math.hypot(dx, dy) <= goal_dis:
            find_goal = True
            break<N><N>
        x.append(state.x)
        y.append(state.y)
        yaw.append(state.yaw)
        v.append(state.v)
        t.append(time)
        a.append(ai)
        d.append(di)<N><N>
"""<N><N>Unicycle model class<N><N>
author Atsushi Sakai<N><N>
"""<N><N>import math
import numpy as np<N><N>
dt = 0.05  # [s]
L = 0.9  # [m]
steer_max = np.deg2rad(40.0)
curvature_max = math.tan(steer_max) / L
curvature_max = 1.0 / curvature_max + 1.0<N><N>
accel_max = 5.0<N><N>

class State:<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v<N><N>

def update(state, a, delta):<N><N>
    state.x = state.x + state.v * math.cos(state.yaw) * dt
    state.y = state.y + state.v * math.sin(state.yaw) * dt
    state.yaw = state.yaw + state.v / L * math.tan(delta) * dt
    state.yaw = pi_2_pi(state.yaw)
    state.v = state.v + a * dt<N><N>
    return state<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

if __name__ == '__main__':  # pragma: no cover
    print("start unicycle simulation")
    import matplotlib.pyplot as plt<N><N>
    T = 100
    a = [1.0] * T
    delta = [np.deg2rad(1.0)] * T
    #  print(delta)
    #  print(a, delta)<N><N>
    state = State()<N><N>
    x = []
    y = []
    yaw = []
    v = []<N><N>
    for (ai, di) in zip(a, delta):
        state = update(state, ai, di)<N><N>
        x.append(state.x)
        y.append(state.y)
        yaw.append(state.yaw)
        v.append(state.v)<N><N>
    plt.subplots(1)
    plt.plot(x, y)
    plt.axis("equal")
    plt.grid(True)<N><N>
    plt.subplots(1)
    plt.plot(v)
    plt.grid(True)<N><N>
    plt.show()
<N><N>
import os
import sys<N><N>sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(
    os.path.abspath(__file__)) + "/../ReedsSheppPath/")
sys.path.append(os.path.dirname(
    os.path.abspath(__file__)) + "/../RRTStarReedsShepp/")

"""
Cubic spline planner<N><N>
Author: Atsushi Sakai(@Atsushi_twi)<N><N>
"""
import math
import numpy as np
import bisect<N><N>

class Spline:
    """
    Cubic Spline class
    """<N><N>
    def __init__(self, x, y):
        self.b, self.c, self.d, self.w = [], [], [], []<N><N>
        self.x = x
        self.y = y<N><N>
        self.nx = len(x)  # dimension of x
        h = np.diff(x)<N><N>
        # calc coefficient c
        self.a = [iy for iy in y]<N><N>
        # calc coefficient c
        A = self.__calc_A(h)
        B = self.__calc_B(h)
        self.c = np.linalg.solve(A, B)
        #  print(self.c1)<N><N>
        # calc spline coefficient b and d
        for i in range(self.nx - 1):
            self.d.append((self.c[i + 1] - self.c[i]) / (3.0 * h[i]))
            tb = (self.a[i + 1] - self.a[i]) / h[i] - h[i] * \
                (self.c[i + 1] + 2.0 * self.c[i]) / 3.0
            self.b.append(tb)<N><N>
    def calc(self, t):
        """
        Calc position<N><N>
        if t is outside of the input x, return None<N><N>
        """<N><N>
        if t < self.x[0]:
            return None
        elif t > self.x[-1]:
            return None<N><N>
        i = self.__search_index(t)
        dx = t - self.x[i]
        result = self.a[i] + self.b[i] * dx + \
            self.c[i] * dx ** 2.0 + self.d[i] * dx ** 3.0<N><N>
        return result<N><N>
    def calcd(self, t):
        """
        Calc first derivative<N><N>
        if t is outside of the input x, return None
        """<N><N>
        if t < self.x[0]:
            return None
        elif t > self.x[-1]:
            return None<N><N>
        i = self.__search_index(t)
        dx = t - self.x[i]
        result = self.b[i] + 2.0 * self.c[i] * dx + 3.0 * self.d[i] * dx ** 2.0
        return result<N><N>
    def calcdd(self, t):
        """
        Calc second derivative
        """<N><N>
        if t < self.x[0]:
            return None
        elif t > self.x[-1]:
            return None<N><N>
        i = self.__search_index(t)
        dx = t - self.x[i]
        result = 2.0 * self.c[i] + 6.0 * self.d[i] * dx
        return result<N><N>
    def __search_index(self, x):
        """
        search data segment index
        """
        return bisect.bisect(self.x, x) - 1<N><N>
    def __calc_A(self, h):
        """
        calc matrix A for spline coefficient c
        """
        A = np.zeros((self.nx, self.nx))
        A[0, 0] = 1.0
        for i in range(self.nx - 1):
            if i != (self.nx - 2):
                A[i + 1, i + 1] = 2.0 * (h[i] + h[i + 1])
            A[i + 1, i] = h[i]
            A[i, i + 1] = h[i]<N><N>
        A[0, 1] = 0.0
        A[self.nx - 1, self.nx - 2] = 0.0
        A[self.nx - 1, self.nx - 1] = 1.0
        #  print(A)
        return A<N><N>
    def __calc_B(self, h):
        """
        calc matrix B for spline coefficient c
        """
        B = np.zeros(self.nx)
        for i in range(self.nx - 2):
            B[i + 1] = 3.0 * (self.a[i + 2] - self.a[i + 1]) / \
                h[i + 1] - 3.0 * (self.a[i + 1] - self.a[i]) / h[i]
        return B<N><N>

class Spline2D:
    """
    2D Cubic Spline class<N><N>
    """<N><N>
    def __init__(self, x, y):
        self.s = self.__calc_s(x, y)
        self.sx = Spline(self.s, x)
        self.sy = Spline(self.s, y)<N><N>
    def __calc_s(self, x, y):
        dx = np.diff(x)
        dy = np.diff(y)
        self.ds = np.hypot(dx, dy)
        s = [0]
        s.extend(np.cumsum(self.ds))
        return s<N><N>
    def calc_position(self, s):
        """
        calc position
        """
        x = self.sx.calc(s)
        y = self.sy.calc(s)<N><N>
        return x, y<N><N>
    def calc_curvature(self, s):
        """
        calc curvature
        """
        dx = self.sx.calcd(s)
        ddx = self.sx.calcdd(s)
        dy = self.sy.calcd(s)
        ddy = self.sy.calcdd(s)
        k = (ddy * dx - ddx * dy) / ((dx ** 2 + dy ** 2)**(3 / 2))
        return k<N><N>
    def calc_yaw(self, s):
        """
        calc yaw
        """
        dx = self.sx.calcd(s)
        dy = self.sy.calcd(s)
        yaw = math.atan2(dy, dx)
        return yaw<N><N>

def calc_spline_course(x, y, ds=0.1):
    sp = Spline2D(x, y)
    s = list(np.arange(0, sp.s[-1], ds))<N><N>
    rx, ry, ryaw, rk = [], [], [], []
    for i_s in s:
        ix, iy = sp.calc_position(i_s)
        rx.append(ix)
        ry.append(iy)
        ryaw.append(sp.calc_yaw(i_s))
        rk.append(sp.calc_curvature(i_s))<N><N>
    return rx, ry, ryaw, rk, s<N><N>

def main():  # pragma: no cover
    print("Spline 2D test")
    import matplotlib.pyplot as plt
    x = [-2.5, 0.0, 2.5, 5.0, 7.5, 3.0, -1.0]
    y = [0.7, -6, 5, 6.5, 0.0, 5.0, -2.0]
    ds = 0.1  # [m] distance of each intepolated points<N><N>
    sp = Spline2D(x, y)
    s = np.arange(0, sp.s[-1], ds)<N><N>
    rx, ry, ryaw, rk = [], [], [], []
    for i_s in s:
        ix, iy = sp.calc_position(i_s)
        rx.append(ix)
        ry.append(iy)
        ryaw.append(sp.calc_yaw(i_s))
        rk.append(sp.calc_curvature(i_s))<N><N>
    plt.subplots(1)
    plt.plot(x, y, "xb", label="input")
    plt.plot(rx, ry, "-r", label="spline")
    plt.grid(True)
    plt.axis("equal")
    plt.xlabel("x[m]")
    plt.ylabel("y[m]")
    plt.legend()<N><N>
    plt.subplots(1)
    plt.plot(s, [np.rad2deg(iyaw) for iyaw in ryaw], "-r", label="yaw")
    plt.grid(True)
    plt.legend()
    plt.xlabel("line length[m]")
    plt.ylabel("yaw angle[deg]")<N><N>
    plt.subplots(1)
    plt.plot(s, rk, "-r", label="curvature")
    plt.grid(True)
    plt.legend()
    plt.xlabel("line length[m]")
    plt.ylabel("curvature [1/m]")<N><N>
    plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Depth-First grid planning<N><N>
author: Erwin Lejeune (@spida_rwin)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/Depth-first_search)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class DepthFirstSearchPlanner:<N><N>
    def __init__(self, ox, oy, reso, rr):
        """
        Initialize grid map for Depth-First planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.reso = reso
        self.rr = rr
        self.calc_obstacle_map(ox, oy)
        self.motion = self.get_motion_model()<N><N>
    class Node:
        def __init__(self, x, y, cost, parent_index, parent):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.parent_index = parent_index
            self.parent = parent<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent_index)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        Depth First search<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gy: goal y position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        nstart = self.Node(self.calc_xyindex(sx, self.minx),
                           self.calc_xyindex(sy, self.miny), 0.0, -1, None)
        ngoal = self.Node(self.calc_xyindex(gx, self.minx),
                          self.calc_xyindex(gy, self.miny), 0.0, -1, None)<N><N>
        open_set, closed_set = dict(), dict()
        open_set[self.calc_grid_index(nstart)] = nstart<N><N>
        while 1:
            if len(open_set) == 0:
                print("Open set is empty..")
                break<N><N>
            current = open_set.pop(list(open_set.keys())[-1])
            c_id = self.calc_grid_index(current)<N><N>
"""<N><N>Grid based Dijkstra planning<N><N>
author: Atsushi Sakai(@Atsushi_twi)<N><N>
"""<N><N>import matplotlib.pyplot as plt
import math<N><N>
show_animation = True<N><N>

class Dijkstra:<N><N>
    def __init__(self, ox, oy, resolution, robot_radius):
        """
        Initialize map for a star planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.min_x = None
        self.min_y = None
        self.max_x = None
        self.max_y = None
        self.x_width = None
        self.y_width = None
        self.obstacle_map = None<N><N>
        self.resolution = resolution
        self.robot_radius = robot_radius
        self.calc_obstacle_map(ox, oy)
        self.motion = self.get_motion_model()<N><N>
    class Node:
        def __init__(self, x, y, cost, parent_index):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.parent_index = parent_index  # index of previous Node<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent_index)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        dijkstra path search<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gx: goal x position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        start_node = self.Node(self.calc_xy_index(sx, self.min_x),
                               self.calc_xy_index(sy, self.min_y), 0.0, -1)
        goal_node = self.Node(self.calc_xy_index(gx, self.min_x),
                              self.calc_xy_index(gy, self.min_y), 0.0, -1)<N><N>
        open_set, closed_set = dict(), dict()
        open_set[self.calc_index(start_node)] = start_node<N><N>
        while 1:
            c_id = min(open_set, key=lambda o: open_set[o].cost)
            current = open_set[c_id]<N><N>
"""<N><N>D* grid planning<N><N>
author: Nirnay Roy<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/D*)<N><N>
"""
import math<N><N>
from sys import maxsize<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class State:<N><N>
    def __init__(self, x, y):
        self.x = x
        self.y = y
        self.parent = None
        self.state = "."
        self.t = "new"  # tag for state
        self.h = 0
        self.k = 0<N><N>
    def cost(self, state):
        if self.state == "#" or state.state == "#":
            return maxsize<N><N>
        return math.sqrt(math.pow((self.x - state.x), 2) +
                         math.pow((self.y - state.y), 2))<N><N>
    def set_state(self, state):
        """
        .: new
        #: obstacle
        e: oparent of current state
        *: closed state
        s: current state
        """
        if state not in ["s", ".", "#", "e", "*"]:
            return
        self.state = state<N><N>

class Map:<N><N>
    def __init__(self, row, col):
        self.row = row
        self.col = col
        self.map = self.init_map()<N><N>
    def init_map(self):
        map_list = []
        for i in range(self.row):
            tmp = []
            for j in range(self.col):
                tmp.append(State(i, j))
            map_list.append(tmp)
        return map_list<N><N>
    def get_neighbors(self, state):
        state_list = []
        for i in [-1, 0, 1]:
            for j in [-1, 0, 1]:
                if i == 0 and j == 0:
                    continue
                if state.x + i < 0 or state.x + i >= self.row:
                    continue
                if state.y + j < 0 or state.y + j >= self.col:
                    continue
                state_list.append(self.map[state.x + i][state.y + j])
        return state_list<N><N>
    def set_obstacle(self, point_list):
        for x, y in point_list:
            if x < 0 or x >= self.row or y < 0 or y >= self.col:
                continue<N><N>
            self.map[x][y].set_state("#")<N><N>

class Dstar:
    def __init__(self, maps):
        self.map = maps
        self.open_list = set()<N><N>
    def process_state(self):
        x = self.min_state()<N><N>
        if x is None:
            return -1<N><N>
        k_old = self.get_kmin()
        self.remove(x)<N><N>
"""<N><N>Dubins path planner sample code<N><N>
author Atsushi Sakai(@Atsushi_twi)<N><N>
"""
import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot<N><N>
show_animation = True<N><N>

def mod2pi(theta):
    return theta - 2.0 * math.pi * math.floor(theta / 2.0 / math.pi)<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def left_straight_left(alpha, beta, d):
    sa = math.sin(alpha)
    sb = math.sin(beta)
    ca = math.cos(alpha)
    cb = math.cos(beta)
    c_ab = math.cos(alpha - beta)<N><N>
    tmp0 = d + sa - sb<N><N>
    mode = ["L", "S", "L"]
    p_squared = 2 + (d * d) - (2 * c_ab) + (2 * d * (sa - sb))
    if p_squared < 0:
        return None, None, None, mode
    tmp1 = math.atan2((cb - ca), tmp0)
    t = mod2pi(-alpha + tmp1)
    p = math.sqrt(p_squared)
    q = mod2pi(beta - tmp1)<N><N>
    return t, p, q, mode<N><N>

def right_straight_right(alpha, beta, d):
    sa = math.sin(alpha)
    sb = math.sin(beta)
    ca = math.cos(alpha)
    cb = math.cos(beta)
    c_ab = math.cos(alpha - beta)<N><N>
    tmp0 = d - sa + sb
    mode = ["R", "S", "R"]
    p_squared = 2 + (d * d) - (2 * c_ab) + (2 * d * (sb - sa))
    if p_squared < 0:
        return None, None, None, mode
    tmp1 = math.atan2((ca - cb), tmp0)
    t = mod2pi(alpha - tmp1)
    p = math.sqrt(p_squared)
    q = mod2pi(-beta + tmp1)<N><N>
    return t, p, q, mode<N><N>

def left_straight_right(alpha, beta, d):
    sa = math.sin(alpha)
    sb = math.sin(beta)
    ca = math.cos(alpha)
    cb = math.cos(beta)
    c_ab = math.cos(alpha - beta)<N><N>
    p_squared = -2 + (d * d) + (2 * c_ab) + (2 * d * (sa + sb))
    mode = ["L", "S", "R"]
    if p_squared < 0:
        return None, None, None, mode
    p = math.sqrt(p_squared)
    tmp2 = math.atan2((-ca - cb), (d + sa + sb)) - math.atan2(-2.0, p)
    t = mod2pi(-alpha + tmp2)
    q = mod2pi(-mod2pi(beta) + tmp2)<N><N>
    return t, p, q, mode<N><N>

def right_straight_left(alpha, beta, d):
    sa = math.sin(alpha)
    sb = math.sin(beta)
    ca = math.cos(alpha)
    cb = math.cos(beta)
    c_ab = math.cos(alpha - beta)<N><N>
    p_squared = (d * d) - 2 + (2 * c_ab) - (2 * d * (sa + sb))
    mode = ["R", "S", "L"]
    if p_squared < 0:
        return None, None, None, mode
    p = math.sqrt(p_squared)
    tmp2 = math.atan2((ca + cb), (d - sa - sb)) - math.atan2(2.0, p)
    t = mod2pi(alpha - tmp2)
    q = mod2pi(beta - tmp2)<N><N>
    return t, p, q, mode<N><N>

def right_left_right(alpha, beta, d):
    sa = math.sin(alpha)
    sb = math.sin(beta)
    ca = math.cos(alpha)
    cb = math.cos(beta)
    c_ab = math.cos(alpha - beta)<N><N>
    mode = ["R", "L", "R"]
    tmp_rlr = (6.0 - d * d + 2.0 * c_ab + 2.0 * d * (sa - sb)) / 8.0
    if abs(tmp_rlr) > 1.0:
        return None, None, None, mode<N><N>
    p = mod2pi(2 * math.pi - math.acos(tmp_rlr))
    t = mod2pi(alpha - math.atan2(ca - cb, d - sa + sb) + mod2pi(p / 2.0))
    q = mod2pi(alpha - beta - t + mod2pi(p))
    return t, p, q, mode<N><N>

def left_right_left(alpha, beta, d):
    sa = math.sin(alpha)
    sb = math.sin(beta)
    ca = math.cos(alpha)
    cb = math.cos(beta)
    c_ab = math.cos(alpha - beta)<N><N>
    mode = ["L", "R", "L"]
    tmp_lrl = (6.0 - d * d + 2.0 * c_ab + 2.0 * d * (- sa + sb)) / 8.0
    if abs(tmp_lrl) > 1:
        return None, None, None, mode
    p = mod2pi(2 * math.pi - math.acos(tmp_lrl))
    t = mod2pi(-alpha - math.atan2(ca - cb, d + sa - sb) + p / 2.0)
    q = mod2pi(mod2pi(beta) - alpha - t + mod2pi(p))<N><N>
    return t, p, q, mode<N><N>

def dubins_path_planning_from_origin(end_x, end_y, end_yaw, curvature,
                                     step_size):
    dx = end_x
    dy = end_y
    D = math.hypot(dx, dy)
    d = D * curvature<N><N>
    theta = mod2pi(math.atan2(dy, dx))
    alpha = mod2pi(- theta)
    beta = mod2pi(end_yaw - theta)<N><N>
    planners = [left_straight_left, right_straight_right, left_straight_right,
                right_straight_left, right_left_right,
                left_right_left]<N><N>
    best_cost = float("inf")
    bt, bp, bq, best_mode = None, None, None, None<N><N>
    for planner in planners:
        t, p, q, mode = planner(alpha, beta, d)
        if t is None:
            continue<N><N>
        cost = (abs(t) + abs(p) + abs(q))
        if best_cost > cost:
            bt, bp, bq, best_mode = t, p, q, mode
            best_cost = cost
    lengths = [bt, bp, bq]<N><N>
    x_list, y_list, yaw_list, directions = generate_local_course(
        sum(lengths), lengths, best_mode, curvature, step_size)<N><N>
    return x_list, y_list, yaw_list, best_mode, best_cost<N><N>
"""<N><N>Mobile robot motion planning sample with Dynamic Window Approach<N><N>
author: Atsushi Sakai (@Atsushi_twi), Göktuğ Karakaşlı<N><N>
"""<N><N>import math
from enum import Enum<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
show_animation = True<N><N>

def dwa_control(x, config, goal, ob):
    """
    Dynamic Window Approach control
    """
    dw = calc_dynamic_window(x, config)<N><N>
    u, trajectory = calc_control_and_trajectory(x, dw, config, goal, ob)<N><N>
    return u, trajectory<N><N>

class RobotType(Enum):
    circle = 0
    rectangle = 1<N><N>

class Config:
    """
    simulation parameter class
    """<N><N>
"""<N><N>eta^3 polynomials planner<N><N>
author: Joe Dinius, Ph.D (https://jwdinius.github.io)
        Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:
- [eta^3-Splines for the Smooth Path Generation of Wheeled Mobile Robots]
(https://ieeexplore.ieee.org/document/4339545/)<N><N>
"""<N><N>import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import quad<N><N>
# NOTE: *_pose is a 3-array:
# 0 - x coord, 1 - y coord, 2 - orientation angle \theta<N><N>
show_animation = True<N><N>

class Eta3Path(object):
    """
    Eta3Path<N><N>
    input
        segments: a list of `Eta3PathSegment` instances
        defining a continuous path
    """<N><N>
    def __init__(self, segments):
        # ensure input has the correct form
        assert(isinstance(segments, list) and isinstance(
            segments[0], Eta3PathSegment))
        # ensure that each segment begins from the previous segment's end (continuity)
        for r, s in zip(segments[:-1], segments[1:]):
            assert(np.array_equal(r.end_pose, s.start_pose))
        self.segments = segments<N><N>
    def calc_path_point(self, u):
        """
        Eta3Path::calc_path_point<N><N>
        input
            normalized interpolation point along path object, 0 <= u <= len(self.segments)
        returns
            2d (x,y) position vector
        """<N><N>
        assert(0 <= u <= len(self.segments))
        if np.isclose(u, len(self.segments)):
            segment_idx = len(self.segments) - 1
            u = 1.
        else:
            segment_idx = int(np.floor(u))
            u -= segment_idx
        return self.segments[segment_idx].calc_point(u)<N><N>

class Eta3PathSegment(object):
    """
    Eta3PathSegment - constructs an eta^3 path segment based on desired
    shaping, eta, and curvature vector, kappa. If either, or both,
    of eta and kappa are not set during initialization,
    they will default to zeros.<N><N>
    input
        start_pose - starting pose array  (x, y, \theta)
        end_pose - ending pose array (x, y, \theta)
        eta - shaping parameters, default=None
        kappa - curvature parameters, default=None
    """<N><N>
"""<N><N>eta^3 polynomials trajectory planner<N><N>
author: Joe Dinius, Ph.D (https://jwdinius.github.io)
        Atsushi Sakai (@Atsushi_twi)<N><N>
Refs:
- https://jwdinius.github.io/blog/2018/eta3traj
- [eta^3-Splines for the Smooth Path Generation of Wheeled Mobile Robots]
(https://ieeexplore.ieee.org/document/4339545/)<N><N>
"""<N><N>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import sys
import os
sys.path.append(os.path.relpath("../Eta3SplinePath"))<N><N>
try:
    from eta3_spline_path import Eta3Path, Eta3PathSegment
except ImportError:
    raise<N><N>
show_animation = True<N><N>

class MaxVelocityNotReached(Exception):
    def __init__(self, actual_vel, max_vel):
        self.message = 'Actual velocity {} does not equal desired max velocity {}!'.format(
            actual_vel, max_vel)<N><N>

class eta3_trajectory(Eta3Path):
    """
    eta3_trajectory<N><N>
    input
        segments: list of `eta3_trajectory_segment` instances defining a continuous trajectory
    """<N><N>
"""
flowfield pathfinding
author: Sarim Mehdi (muhammadsarim.mehdi@studio.unibo.it)
Source: https://leifnode.com/2013/12/flow-field-pathfinding/
"""<N><N>
import numpy as np
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

def draw_horizontal_line(start_x, start_y, length, o_x, o_y, o_dict, path):
    for i in range(start_x, start_x + length):
        for j in range(start_y, start_y + 2):
            o_x.append(i)
            o_y.append(j)
            o_dict[(i, j)] = path<N><N>

def draw_vertical_line(start_x, start_y, length, o_x, o_y, o_dict, path):
    for i in range(start_x, start_x + 2):
        for j in range(start_y, start_y + length):
            o_x.append(i)
            o_y.append(j)
            o_dict[(i, j)] = path<N><N>

class FlowField:
    def __init__(self, obs_grid, goal_x, goal_y, start_x, start_y,
                 limit_x, limit_y):
        self.start_pt = [start_x, start_y]
        self.goal_pt = [goal_x, goal_y]
        self.obs_grid = obs_grid
        self.limit_x, self.limit_y = limit_x, limit_y
        self.cost_field = {}
        self.integration_field = {}
        self.vector_field = {}<N><N>
    def find_path(self):
        self.create_cost_field()
        self.create_integration_field()
        self.assign_vectors()
        self.follow_vectors()<N><N>
"""<N><N>Frenet optimal trajectory generator<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:<N><N>
- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame]
(https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)<N><N>
- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame]
(https://www.youtube.com/watch?v=Cj6tAQe7UCY)<N><N>
"""<N><N>import numpy as np
import matplotlib.pyplot as plt
import copy
import math
import sys
import os<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../QuinticPolynomialsPlanner/")
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../CubicSpline/")<N><N>
try:
    from quintic_polynomials_planner import QuinticPolynomial
    import cubic_spline_planner
except ImportError:
    raise<N><N>
SIM_LOOP = 500<N><N>
"""<N><N>Greedy Best-First grid planning<N><N>
author: Erwin Lejeune (@spida_rwin)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/Best-first_search)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = True<N><N>

class BestFirstSearchPlanner:<N><N>
    def __init__(self, ox, oy, reso, rr):
        """
        Initialize grid map for greedy best-first planning<N><N>
        ox: x position list of Obstacles [m]
        oy: y position list of Obstacles [m]
        resolution: grid resolution [m]
        rr: robot radius[m]
        """<N><N>
        self.reso = reso
        self.rr = rr
        self.calc_obstacle_map(ox, oy)
        self.motion = self.get_motion_model()<N><N>
    class Node:
        def __init__(self, x, y, cost, pind, parent):
            self.x = x  # index of grid
            self.y = y  # index of grid
            self.cost = cost
            self.pind = pind
            self.parent = parent<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.pind)<N><N>
    def planning(self, sx, sy, gx, gy):
        """
        Greedy Best-First search<N><N>
        input:
            s_x: start x position [m]
            s_y: start y position [m]
            gx: goal x position [m]
            gy: goal y position [m]<N><N>
        output:
            rx: x position list of the final path
            ry: y position list of the final path
        """<N><N>
        nstart = self.Node(self.calc_xyindex(sx, self.minx),
                           self.calc_xyindex(sy, self.miny), 0.0, -1, None)
        ngoal = self.Node(self.calc_xyindex(gx, self.minx),
                          self.calc_xyindex(gy, self.miny), 0.0, -1, None)<N><N>
        open_set, closed_set = dict(), dict()
        open_set[self.calc_grid_index(nstart)] = nstart<N><N>
        while 1:
            if len(open_set) == 0:
                print("Open set is empty..")
                break<N><N>
            c_id = min(
                open_set,
                key=lambda o: self.calc_heuristic(ngoal, open_set[o]))<N><N>
            current = open_set[c_id]<N><N>
"""
Grid based sweep planner<N><N>
author: Atsushi Sakai
"""<N><N>
import math
from enum import IntEnum<N><N>
import numpy as np
from scipy.spatial.transform import Rotation as Rot
from Mapping.grid_map_lib.grid_map_lib import GridMap
import matplotlib.pyplot as plt<N><N>
do_animation = True<N><N>

class SweepSearcher:
    class SweepDirection(IntEnum):
        UP = 1
        DOWN = -1<N><N>
    class MovingDirection(IntEnum):
        RIGHT = 1
        LEFT = -1<N><N>
    def __init__(self,
                 moving_direction, sweep_direction, x_inds_goal_y, goal_y):
        self.moving_direction = moving_direction
        self.sweep_direction = sweep_direction
        self.turing_window = []
        self.update_turning_window()
        self.x_indexes_goal_y = x_inds_goal_y
        self.goal_y = goal_y<N><N>
    def move_target_grid(self, c_x_index, c_y_index, grid_map):
        n_x_index = self.moving_direction + c_x_index
        n_y_index = c_y_index<N><N>
import os
import sys<N><N>GRID_MAP_LIB = os.path.dirname(os.path.abspath(__file__)) + \
               "/../../Mapping/"<N><N>sys.path.append(GRID_MAP_LIB)

"""<N><N>Car model for Hybrid A* path planning<N><N>
author: Zheng Zh (@Zhengzh)<N><N>
"""<N><N>from math import sqrt, cos, sin, tan, pi<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot<N><N>
WB = 3.  # rear to front wheel
W = 2.  # width of car
LF = 3.3  # distance from rear to vehicle front end
LB = 1.0  # distance from rear to vehicle back end
MAX_STEER = 0.6  # [rad] maximum steering angle<N><N>
W_BUBBLE_DIST = (LF - LB) / 2.0
W_BUBBLE_R = sqrt(((LF + LB) / 2.0) ** 2 + 1)<N><N>
# vehicle rectangle vertices
VRX = [LF, LF, -LB, -LB, LF]
VRY = [W / 2, -W / 2, -W / 2, W / 2, W / 2]<N><N>

def check_car_collision(x_list, y_list, yaw_list, ox, oy, kd_tree):
    for i_x, i_y, i_yaw in zip(x_list, y_list, yaw_list):
        cx = i_x + W_BUBBLE_DIST * cos(i_yaw)
        cy = i_y + W_BUBBLE_DIST * sin(i_yaw)<N><N>
        ids = kd_tree.query_ball_point([cx, cy], W_BUBBLE_R)<N><N>
        if not ids:
            continue<N><N>
        if not rectangle_check(i_x, i_y, i_yaw,
                               [ox[i] for i in ids], [oy[i] for i in ids]):
            return False  # collision<N><N>
    return True  # no collision<N><N>

def rectangle_check(x, y, yaw, ox, oy):
    # transform obstacles to base link frame
    rot = Rot.from_euler('z', yaw).as_matrix()[0:2, 0:2]
    for iox, ioy in zip(ox, oy):
        tx = iox - x
        ty = ioy - y
        converted_xy = np.stack([tx, ty]).T @ rot
        rx, ry = converted_xy[0], converted_xy[1]<N><N>
        if not (rx > LF or rx < -LB or ry > W / 2.0 or ry < -W / 2.0):
            return False  # no collision<N><N>
    return True  # collision<N><N>

def plot_arrow(x, y, yaw, length=1.0, width=0.5, fc="r", ec="k"):
    """Plot arrow."""
    if not isinstance(x, float):
        for (i_x, i_y, i_yaw) in zip(x, y, yaw):
            plot_arrow(i_x, i_y, i_yaw)
    else:
        plt.arrow(x, y, length * cos(yaw), length * sin(yaw),
                  fc=fc, ec=ec, head_width=width, head_length=width, alpha=0.4)<N><N>

def plot_car(x, y, yaw):
    car_color = '-k'
    c, s = cos(yaw), sin(yaw)
    rot = Rot.from_euler('z', -yaw).as_matrix()[0:2, 0:2]
    car_outline_x, car_outline_y = [], []
    for rx, ry in zip(VRX, VRY):
        converted_xy = np.stack([rx, ry]).T @ rot
        car_outline_x.append(converted_xy[0]+x)
        car_outline_y.append(converted_xy[1]+y)<N><N>
    arrow_x, arrow_y, arrow_yaw = c * 1.5 + x, s * 1.5 + y, yaw
    plot_arrow(arrow_x, arrow_y, arrow_yaw)<N><N>
    plt.plot(car_outline_x, car_outline_y, car_color)<N><N>

def pi_2_pi(angle):
    return (angle + pi) % (2 * pi) - pi<N><N>

def move(x, y, yaw, distance, steer, L=WB):
    x += distance * cos(yaw)
    y += distance * sin(yaw)
    yaw += pi_2_pi(distance * tan(steer) / L)  # distance/2<N><N>
    return x, y, yaw<N><N>

def main():
    x, y, yaw = 0., 0., 1.
    plt.axis('equal')
    plot_car(x, y, yaw)
    plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>A* grid based planning<N><N>
author: Nikos Kanargias (nkana@tee.gr)<N><N>
See Wikipedia article (https://en.wikipedia.org/wiki/A*_search_algorithm)<N><N>
"""<N><N>import heapq
import math<N><N>
import matplotlib.pyplot as plt<N><N>
show_animation = False<N><N>

class Node:<N><N>
    def __init__(self, x, y, cost, parent_index):
        self.x = x
        self.y = y
        self.cost = cost
        self.parent_index = parent_index<N><N>
    def __str__(self):
        return str(self.x) + "," + str(self.y) + "," + str(
            self.cost) + "," + str(self.parent_index)<N><N>

def calc_final_path(goal_node, closed_node_set, resolution):
    # generate final course
    rx, ry = [goal_node.x * resolution], [goal_node.y * resolution]
    parent_index = goal_node.parent_index
    while parent_index != -1:
        n = closed_node_set[parent_index]
        rx.append(n.x * resolution)
        ry.append(n.y * resolution)
        parent_index = n.parent_index<N><N>
    return rx, ry<N><N>

def calc_distance_heuristic(gx, gy, ox, oy, resolution, rr):
    """
    gx: goal x position [m]
    gx: goal x position [m]
    ox: x position list of Obstacles [m]
    oy: y position list of Obstacles [m]
    resolution: grid resolution [m]
    rr: robot radius[m]
    """<N><N>
    goal_node = Node(round(gx / resolution), round(gy / resolution), 0.0, -1)
    ox = [iox / resolution for iox in ox]
    oy = [ioy / resolution for ioy in oy]<N><N>
    obstacle_map, min_x, min_y, max_x, max_y, x_w, y_w = calc_obstacle_map(
        ox, oy, resolution, rr)<N><N>
    motion = get_motion_model()<N><N>
    open_set, closed_set = dict(), dict()
    open_set[calc_index(goal_node, x_w, min_x, min_y)] = goal_node
    priority_queue = [(0, calc_index(goal_node, x_w, min_x, min_y))]<N><N>
    while 1:
        if not priority_queue:
            break
        cost, c_id = heapq.heappop(priority_queue)
        if c_id in open_set:
            current = open_set[c_id]
            closed_set[c_id] = current
            open_set.pop(c_id)
        else:
            continue<N><N>
        # show graph
        if show_animation:  # pragma: no cover
            plt.plot(current.x * resolution, current.y * resolution, "xc")
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
            if len(closed_set.keys()) % 10 == 0:
                plt.pause(0.001)<N><N>
        # Remove the item from the open set<N><N>
        # expand search grid based on motion model
        for i, _ in enumerate(motion):
            node = Node(current.x + motion[i][0],
                        current.y + motion[i][1],
                        current.cost + motion[i][2], c_id)
            n_id = calc_index(node, x_w, min_x, min_y)<N><N>
            if n_id in closed_set:
                continue<N><N>
            if not verify_node(node, obstacle_map, min_x, min_y, max_x, max_y):
                continue<N><N>
"""<N><N>Hybrid A* path planning<N><N>
author: Zheng Zh (@Zhengzh)<N><N>
"""<N><N>import heapq
import math
import os
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial import cKDTree<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__))
                + "/../ReedsSheppPath")
try:
    from dynamic_programming_heuristic import calc_distance_heuristic
    import reeds_shepp_path_planning as rs
    from car import move, check_car_collision, MAX_STEER, WB, plot_car
except Exception:
    raise<N><N>
XY_GRID_RESOLUTION = 2.0  # [m]
YAW_GRID_RESOLUTION = np.deg2rad(15.0)  # [rad]
MOTION_RESOLUTION = 0.1  # [m] path interpolate resolution
N_STEER = 20  # number of steer command
VR = 1.0  # robot radius<N><N>
SB_COST = 100.0  # switch back penalty cost
BACK_COST = 5.0  # backward penalty cost
STEER_CHANGE_COST = 5.0  # steer angle change penalty cost
STEER_COST = 1.0  # steer angle change penalty cost
H_COST = 5.0  # Heuristic cost<N><N>
show_animation = True<N><N>

class Node:<N><N>
import os
import sys<N><N>sys.path.append(os.path.dirname(os.path.abspath(__file__)))

"""
Informed RRT* path planning<N><N>
author: Karan Chawla
        Atsushi Sakai(@Atsushi_twi)<N><N>
Reference: Informed RRT*: Optimal Sampling-based Path planning Focused via
Direct Sampling of an Admissible Ellipsoidal Heuristic
https://arxiv.org/pdf/1404.2334.pdf<N><N>
"""<N><N>import copy
import math
import random<N><N>
import matplotlib.pyplot as plt
from scipy.spatial.transform import Rotation as Rot
import numpy as np<N><N>
show_animation = True<N><N>

class InformedRRTStar:<N><N>
    def __init__(self, start, goal,
                 obstacleList, randArea,
                 expandDis=0.5, goalSampleRate=10, maxIter=200):<N><N>
        self.start = Node(start[0], start[1])
        self.goal = Node(goal[0], goal[1])
        self.min_rand = randArea[0]
        self.max_rand = randArea[1]
        self.expand_dis = expandDis
        self.goal_sample_rate = goalSampleRate
        self.max_iter = maxIter
        self.obstacle_list = obstacleList
        self.node_list = None<N><N>
    def informed_rrt_star_search(self, animation=True):<N><N>
        self.node_list = [self.start]
        # max length we expect to find in our 'informed' sample space,
        # starts as infinite
        cBest = float('inf')
        solutionSet = set()
        path = None<N><N>
        # Computing the sampling space
        cMin = math.sqrt(pow(self.start.x - self.goal.x, 2)
                         + pow(self.start.y - self.goal.y, 2))
        xCenter = np.array([[(self.start.x + self.goal.x) / 2.0],
                            [(self.start.y + self.goal.y) / 2.0], [0]])
        a1 = np.array([[(self.goal.x - self.start.x) / cMin],
                       [(self.goal.y - self.start.y) / cMin], [0]])<N><N>
        e_theta = math.atan2(a1[1], a1[0])
        # first column of identity matrix transposed
        id1_t = np.array([1.0, 0.0, 0.0]).reshape(1, 3)
        M = a1 @ id1_t
        U, S, Vh = np.linalg.svd(M, True, True)
        C = np.dot(np.dot(U, np.diag(
            [1.0, 1.0, np.linalg.det(U) * np.linalg.det(np.transpose(Vh))])),
                   Vh)<N><N>
        for i in range(self.max_iter):
            # Sample space is defined by cBest
            # cMin is the minimum distance between the start point and the goal
            # xCenter is the midpoint between the start and the goal
            # cBest changes when a new path is found<N><N>
            rnd = self.informed_sample(cBest, cMin, xCenter, C)
            n_ind = self.get_nearest_list_index(self.node_list, rnd)
            nearestNode = self.node_list[n_ind]
            # steer
            theta = math.atan2(rnd[1] - nearestNode.y, rnd[0] - nearestNode.x)
            newNode = self.get_new_node(theta, n_ind, nearestNode)
            d = self.line_cost(nearestNode, newNode)<N><N>
            noCollision = self.check_collision(nearestNode, theta, d)<N><N>
            if noCollision:
                nearInds = self.find_near_nodes(newNode)
                newNode = self.choose_parent(newNode, nearInds)<N><N>
                self.node_list.append(newNode)
                self.rewire(newNode, nearInds)<N><N>
"""<N><N>LQR local path planning<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math
import random<N><N>
import matplotlib.pyplot as plt
import numpy as np
import scipy.linalg as la<N><N>
SHOW_ANIMATION = True<N><N>

class LQRPlanner:<N><N>
    def __init__(self):
        self.MAX_TIME = 100.0  # Maximum simulation time
        self.DT = 0.1  # Time tick
        self.GOAL_DIST = 0.1
        self.MAX_ITER = 150
        self.EPS = 0.01<N><N>
    def lqr_planning(self, sx, sy, gx, gy, show_animation=True):<N><N>
        rx, ry = [sx], [sy]<N><N>
        x = np.array([sx - gx, sy - gy]).reshape(2, 1)  # State vector<N><N>
        # Linear system model
        A, B = self.get_system_model()<N><N>
        found_path = False<N><N>
        time = 0.0
        while time <= self.MAX_TIME:
            time += self.DT<N><N>
            u = self.lqr_control(A, B, x)<N><N>
            x = A @ x + B @ u<N><N>
            rx.append(x[0, 0] + gx)
            ry.append(x[1, 0] + gy)<N><N>
            d = math.sqrt((gx - rx[-1]) ** 2 + (gy - ry[-1]) ** 2)
            if d <= self.GOAL_DIST:
                found_path = True
                break<N><N>
            # animation
            if show_animation:  # pragma: no cover
                # for stopping simulation with the esc key.
                plt.gcf().canvas.mpl_connect('key_release_event',
                        lambda event: [exit(0) if event.key == 'escape' else None])
                plt.plot(sx, sy, "or")
                plt.plot(gx, gy, "ob")
                plt.plot(rx, ry, "-r")
                plt.axis("equal")
                plt.pause(1.0)<N><N>
        if not found_path:
            print("Cannot found path")
            return [], []<N><N>
        return rx, ry<N><N>
    def solve_dare(self, A, B, Q, R):
        """
        solve a discrete time_Algebraic Riccati equation (DARE)
        """
        X, Xn = Q, Q<N><N>
        for i in range(self.MAX_ITER):
            Xn = A.T * X * A - A.T * X * B * \
                 la.inv(R + B.T * X * B) * B.T * X * A + Q
            if (abs(Xn - X)).max() < self.EPS:
                break
            X = Xn<N><N>
        return Xn<N><N>
    def dlqr(self, A, B, Q, R):
        """Solve the discrete time lqr controller.
        x[k+1] = A x[k] + B u[k]
        cost = sum x[k].T*Q*x[k] + u[k].T*R*u[k]
        # ref Bertsekas, p.151
        """<N><N>
        # first, try to solve the ricatti equation
        X = self.solve_dare(A, B, Q, R)<N><N>
        # compute the LQR gain
        K = la.inv(B.T @ X @ B + R) @ (B.T @ X @ A)<N><N>
        eigValues = la.eigvals(A - B @ K)<N><N>
        return K, X, eigValues<N><N>
    def get_system_model(self):<N><N>
        A = np.array([[self.DT, 1.0],
                      [0.0, self.DT]])
        B = np.array([0.0, 1.0]).reshape(2, 1)<N><N>
        return A, B<N><N>
    def lqr_control(self, A, B, x):<N><N>
        Kopt, X, ev = self.dlqr(A, B, np.eye(2), np.eye(1))<N><N>
        u = -Kopt @ x<N><N>
        return u<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    ntest = 10  # number of goal
    area = 100.0  # sampling area<N><N>
    lqr_planner = LQRPlanner()<N><N>
    for i in range(ntest):
        sx = 6.0
        sy = 6.0
        gx = random.uniform(-area, area)
        gy = random.uniform(-area, area)<N><N>
        rx, ry = lqr_planner.lqr_planning(sx, sy, gx, gy, show_animation=SHOW_ANIMATION)<N><N>
        if SHOW_ANIMATION:  # pragma: no cover
            plt.plot(sx, sy, "or")
            plt.plot(gx, gy, "ob")
            plt.plot(rx, ry, "-r")
            plt.axis("equal")
            plt.pause(1.0)<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Path planning code with LQR RRT*<N><N>
author: AtsushiSakai(@Atsushi_twi)<N><N>
"""
import copy
import math
import os
import random
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/../LQRPlanner/")
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/../RRTStar/")<N><N>
try:
    from LQRplanner import LQRPlanner
    from rrt_star import RRTStar
except ImportError:
    raise<N><N>
show_animation = True<N><N>

class LQRRRTStar(RRTStar):
    """
    Class for RRT star planning with LQR planning
    """<N><N>
    def __init__(self, start, goal, obstacle_list, rand_area,
                 goal_sample_rate=10,
                 max_iter=200,
                 connect_circle_dist=50.0,
                 step_size=0.2
                 ):
        """
        Setting Parameter<N><N>
        start:Start Position [x,y]
        goal:Goal Position [x,y]
        obstacleList:obstacle Positions [[x,y,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
        """
        self.start = self.Node(start[0], start[1])
        self.end = self.Node(goal[0], goal[1])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.goal_sample_rate = goal_sample_rate
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list
        self.connect_circle_dist = connect_circle_dist<N><N>
        self.curvature = 1.0
        self.goal_xy_th = 0.5
        self.step_size = step_size<N><N>
        self.lqr_planner = LQRPlanner()<N><N>
    def planning(self, animation=True, search_until_max_iter=True):
        """
        RRT Star planning<N><N>
        animation: flag for animation on or off
        """<N><N>
        self.node_list = [self.start]
        for i in range(self.max_iter):
            print("Iter:", i, ", number of nodes:", len(self.node_list))
            rnd = self.get_random_node()
            nearest_ind = self.get_nearest_node_index(self.node_list, rnd)
            new_node = self.steer(self.node_list[nearest_ind], rnd)<N><N>
            if self.check_collision(new_node, self.obstacle_list):
                near_indexes = self.find_near_nodes(new_node)
                new_node = self.choose_parent(new_node, near_indexes)
                if new_node:
                    self.node_list.append(new_node)
                    self.rewire(new_node, near_indexes)<N><N>
            if animation and i % 5 == 0:
                self.draw_graph(rnd)<N><N>
            if (not search_until_max_iter) and new_node:  # check reaching the goal
                last_index = self.search_best_goal_node()
                if last_index:
                    return self.generate_final_course(last_index)<N><N>
        print("reached max iteration")<N><N>
        last_index = self.search_best_goal_node()
        if last_index:
            return self.generate_final_course(last_index)
        else:
            print("Cannot find path")<N><N>
        return None<N><N>
    def draw_graph(self, rnd=None):
        plt.clf()
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect('key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
        if rnd is not None:
            plt.plot(rnd.x, rnd.y, "^k")
        for node in self.node_list:
            if node.parent:
                plt.plot(node.path_x, node.path_y, "-g")<N><N>
        for (ox, oy, size) in self.obstacle_list:
            plt.plot(ox, oy, "ok", ms=30 * size)<N><N>
        plt.plot(self.start.x, self.start.y, "xr")
        plt.plot(self.end.x, self.end.y, "xr")
        plt.axis([-2, 15, -2, 15])
        plt.grid(True)
        plt.pause(0.01)<N><N>
    def search_best_goal_node(self):
        dist_to_goal_list = [self.calc_dist_to_goal(n.x, n.y) for n in self.node_list]
        goal_inds = [dist_to_goal_list.index(i) for i in dist_to_goal_list if i <= self.goal_xy_th]<N><N>
        if not goal_inds:
            return None<N><N>
        min_cost = min([self.node_list[i].cost for i in goal_inds])
        for i in goal_inds:
            if self.node_list[i].cost == min_cost:
                return i<N><N>
        return None<N><N>
    def calc_new_cost(self, from_node, to_node):<N><N>
        wx, wy = self.lqr_planner.lqr_planning(
            from_node.x, from_node.y, to_node.x, to_node.y, show_animation=False)<N><N>
        px, py, course_lengths = self.sample_path(wx, wy, self.step_size)<N><N>
        if not course_lengths:
            return float("inf")<N><N>
        return from_node.cost + sum(course_lengths)<N><N>
    def get_random_node(self):<N><N>
        if random.randint(0, 100) > self.goal_sample_rate:
            rnd = self.Node(random.uniform(self.min_rand, self.max_rand),
                            random.uniform(self.min_rand, self.max_rand)
                            )
        else:  # goal point sampling
            rnd = self.Node(self.end.x, self.end.y)<N><N>
        return rnd<N><N>
    def generate_final_course(self, goal_index):
        print("final")
        path = [[self.end.x, self.end.y]]
        node = self.node_list[goal_index]
        while node.parent:
            for (ix, iy) in zip(reversed(node.path_x), reversed(node.path_y)):
                path.append([ix, iy])
            node = node.parent
        path.append([self.start.x, self.start.y])
        return path<N><N>
    def sample_path(self, wx, wy, step):<N><N>
        px, py, clen = [], [], []<N><N>
        for i in range(len(wx) - 1):<N><N>
            for t in np.arange(0.0, 1.0, step):
                px.append(t * wx[i + 1] + (1.0 - t) * wx[i])
                py.append(t * wy[i + 1] + (1.0 - t) * wy[i])<N><N>
        dx = np.diff(px)
        dy = np.diff(py)<N><N>
        clen = [math.sqrt(idx ** 2 + idy ** 2) for (idx, idy) in zip(dx, dy)]<N><N>
        return px, py, clen<N><N>
    def steer(self, from_node, to_node):<N><N>
        wx, wy = self.lqr_planner.lqr_planning(
            from_node.x, from_node.y, to_node.x, to_node.y, show_animation=False)<N><N>
        px, py, course_lens = self.sample_path(wx, wy, self.step_size)<N><N>
        if px is None:
            return None<N><N>
        newNode = copy.deepcopy(from_node)
        newNode.x = px[-1]
        newNode.y = py[-1]
        newNode.path_x = px
        newNode.path_y = py
        newNode.cost += sum([abs(c) for c in course_lens])
        newNode.parent = from_node<N><N>
        return newNode<N><N>

def main(maxIter=200):
    print("Start " + __file__)<N><N>
    # ====Search Path with RRT====
    obstacleList = [
        (5, 5, 1),
        (4, 6, 1),
        (4, 7.5, 1),
        (4, 9, 1),
        (6, 5, 1),
        (7, 5, 1)
    ]  # [x,y,size]<N><N>
    # Set Initial parameters
    start = [0.0, 0.0]
    goal = [6.0, 7.0]<N><N>
    lqr_rrt_star = LQRRRTStar(start, goal,
                              obstacleList,
                              [-2.0, 15.0])
    path = lqr_rrt_star.planning(animation=show_animation)<N><N>
    # Draw final path
    if show_animation:  # pragma: no cover
        lqr_rrt_star.draw_graph()
        plt.plot([x for (x, y) in path], [y for (x, y) in path], '-r')
        plt.grid(True)
        plt.pause(0.001)
        plt.show()<N><N>
    print("Done")<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Lookup Table generation for model predictive trajectory generator<N><N>
author: Atsushi Sakai<N><N>
"""
from matplotlib import pyplot as plt
import numpy as np
import math
import model_predictive_trajectory_generator as planner
import motion_model
import pandas as pd<N><N>

def calc_states_list():
    maxyaw = np.deg2rad(-30.0)<N><N>
    x = np.arange(10.0, 30.0, 5.0)
    y = np.arange(0.0, 20.0, 2.0)
    yaw = np.arange(-maxyaw, maxyaw, maxyaw)<N><N>
    states = []
    for iyaw in yaw:
        for iy in y:
            for ix in x:
                states.append([ix, iy, iyaw])
    print("nstate:", len(states))<N><N>
    return states<N><N>

def search_nearest_one_from_lookuptable(tx, ty, tyaw, lookuptable):
    mind = float("inf")
    minid = -1<N><N>
    for (i, table) in enumerate(lookuptable):<N><N>
        dx = tx - table[0]
        dy = ty - table[1]
        dyaw = tyaw - table[2]
        d = math.sqrt(dx ** 2 + dy ** 2 + dyaw ** 2)
        if d <= mind:
            minid = i
            mind = d<N><N>
    # print(minid)<N><N>
    return lookuptable[minid]<N><N>

def save_lookup_table(fname, table):
    mt = np.array(table)
    print(mt)
    # save csv
    df = pd.DataFrame()
    df["x"] = mt[:, 0]
    df["y"] = mt[:, 1]
    df["yaw"] = mt[:, 2]
    df["s"] = mt[:, 3]
    df["km"] = mt[:, 4]
    df["kf"] = mt[:, 5]
    df.to_csv(fname, index=None)<N><N>
    print("lookup table file is saved as " + fname)<N><N>

def generate_lookup_table():
    states = calc_states_list()
    k0 = 0.0<N><N>
    # x, y, yaw, s, km, kf
    lookuptable = [[1.0, 0.0, 0.0, 1.0, 0.0, 0.0]]<N><N>
    for state in states:
        bestp = search_nearest_one_from_lookuptable(
            state[0], state[1], state[2], lookuptable)<N><N>
        target = motion_model.State(x=state[0], y=state[1], yaw=state[2])
        init_p = np.array(
            [math.sqrt(state[0] ** 2 + state[1] ** 2), bestp[4], bestp[5]]).reshape(3, 1)<N><N>
        x, y, yaw, p = planner.optimize_trajectory(target, k0, init_p)<N><N>
        if x is not None:
            print("find good path")
            lookuptable.append(
                [x[-1], y[-1], yaw[-1], float(p[0]), float(p[1]), float(p[2])])<N><N>
    print("finish lookup table generation")<N><N>
    save_lookup_table("lookuptable.csv", lookuptable)<N><N>
    for table in lookuptable:
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], table[4], table[5], k0)
        plt.plot(xc, yc, "-r")
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], -table[4], -table[5], k0)
        plt.plot(xc, yc, "-r")<N><N>
    plt.grid(True)
    plt.axis("equal")
    plt.show()<N><N>
    print("Done")<N><N>

def main():
    generate_lookup_table()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Model trajectory generator<N><N>
author: Atsushi Sakai(@Atsushi_twi)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
import motion_model<N><N>
# optimization parameter
max_iter = 100
h = np.array([0.5, 0.02, 0.02]).T  # parameter sampling distance
cost_th = 0.1<N><N>
show_animation = True<N><N>

def plot_arrow(x, y, yaw, length=1.0, width=0.5, fc="r", ec="k"):  # pragma: no cover
    """
    Plot arrow
    """
    plt.arrow(x, y, length * math.cos(yaw), length * math.sin(yaw),
              fc=fc, ec=ec, head_width=width, head_length=width)
    plt.plot(x, y)
    plt.plot(0, 0)<N><N>

def calc_diff(target, x, y, yaw):
    d = np.array([target.x - x[-1],
                  target.y - y[-1],
                  motion_model.pi_2_pi(target.yaw - yaw[-1])])<N><N>
    return d<N><N>

def calc_j(target, p, h, k0):
    xp, yp, yawp = motion_model.generate_last_state(
        p[0, 0] + h[0], p[1, 0], p[2, 0], k0)
    dp = calc_diff(target, [xp], [yp], [yawp])
    xn, yn, yawn = motion_model.generate_last_state(
        p[0, 0] - h[0], p[1, 0], p[2, 0], k0)
    dn = calc_diff(target, [xn], [yn], [yawn])
    d1 = np.array((dp - dn) / (2.0 * h[0])).reshape(3, 1)<N><N>
    xp, yp, yawp = motion_model.generate_last_state(
        p[0, 0], p[1, 0] + h[1], p[2, 0], k0)
    dp = calc_diff(target, [xp], [yp], [yawp])
    xn, yn, yawn = motion_model.generate_last_state(
        p[0, 0], p[1, 0] - h[1], p[2, 0], k0)
    dn = calc_diff(target, [xn], [yn], [yawn])
    d2 = np.array((dp - dn) / (2.0 * h[1])).reshape(3, 1)<N><N>
    xp, yp, yawp = motion_model.generate_last_state(
        p[0, 0], p[1, 0], p[2, 0] + h[2], k0)
    dp = calc_diff(target, [xp], [yp], [yawp])
    xn, yn, yawn = motion_model.generate_last_state(
        p[0, 0], p[1, 0], p[2, 0] - h[2], k0)
    dn = calc_diff(target, [xn], [yn], [yawn])
    d3 = np.array((dp - dn) / (2.0 * h[2])).reshape(3, 1)<N><N>
    J = np.hstack((d1, d2, d3))<N><N>
    return J<N><N>

def selection_learning_param(dp, p, k0, target):
    mincost = float("inf")
    mina = 1.0
    maxa = 2.0
    da = 0.5<N><N>
    for a in np.arange(mina, maxa, da):
        tp = p + a * dp
        xc, yc, yawc = motion_model.generate_last_state(
            tp[0], tp[1], tp[2], k0)
        dc = calc_diff(target, [xc], [yc], [yawc])
        cost = np.linalg.norm(dc)<N><N>
        if cost <= mincost and a != 0.0:
            mina = a
            mincost = cost<N><N>
    #  print(mincost, mina)
    #  input()<N><N>
    return mina<N><N>

def show_trajectory(target, xc, yc):  # pragma: no cover
    plt.clf()
    plot_arrow(target.x, target.y, target.yaw)
    plt.plot(xc, yc, "-r")
    plt.axis("equal")
    plt.grid(True)
    plt.pause(0.1)<N><N>

def optimize_trajectory(target, k0, p):
    for i in range(max_iter):
        xc, yc, yawc = motion_model.generate_trajectory(p[0], p[1], p[2], k0)
        dc = np.array(calc_diff(target, xc, yc, yawc)).reshape(3, 1)<N><N>
        cost = np.linalg.norm(dc)
        if cost <= cost_th:
            print("path is ok cost is:" + str(cost))
            break<N><N>
        J = calc_j(target, p, h, k0)
        try:
            dp = - np.linalg.inv(J) @ dc
        except np.linalg.linalg.LinAlgError:
            print("cannot calc path LinAlgError")
            xc, yc, yawc, p = None, None, None, None
            break
        alpha = selection_learning_param(dp, p, k0, target)<N><N>
        p += alpha * np.array(dp)
        #  print(p.T)<N><N>
        if show_animation:  # pragma: no cover
            show_trajectory(target, xc, yc)
    else:
        xc, yc, yawc, p = None, None, None, None
        print("cannot calc path")<N><N>
    return xc, yc, yawc, p<N><N>

def test_optimize_trajectory():  # pragma: no cover<N><N>
    #  target = motion_model.State(x=5.0, y=2.0, yaw=np.deg2rad(00.0))
    target = motion_model.State(x=5.0, y=2.0, yaw=np.deg2rad(90.0))
    k0 = 0.0<N><N>
    init_p = np.array([6.0, 0.0, 0.0]).reshape(3, 1)<N><N>
    x, y, yaw, p = optimize_trajectory(target, k0, init_p)<N><N>
    if show_animation:
        show_trajectory(target, x, y)
        plot_arrow(target.x, target.y, target.yaw)
        plt.axis("equal")
        plt.grid(True)
        plt.show()<N><N>

def main():  # pragma: no cover
    print(__file__ + " start!!")
    test_optimize_trajectory()<N><N>

if __name__ == '__main__':
    main()
<N><N>
import math
import numpy as np
import scipy.interpolate<N><N>
# motion parameter
L = 1.0  # wheel base
ds = 0.1  # course distanse
v = 10.0 / 3.6  # velocity [m/s]<N><N>

class State:<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def update(state, v, delta, dt, L):<N><N>
    state.v = v
    state.x = state.x + state.v * math.cos(state.yaw) * dt
    state.y = state.y + state.v * math.sin(state.yaw) * dt
    state.yaw = state.yaw + state.v / L * math.tan(delta) * dt
    state.yaw = pi_2_pi(state.yaw)<N><N>
    return state<N><N>

def generate_trajectory(s, km, kf, k0):<N><N>
    n = s / ds
    time = s / v  # [s]
    
    if isinstance(time, type(np.array([]))): time = time[0]
    if isinstance(km, type(np.array([]))): km = km[0]
    if isinstance(kf, type(np.array([]))): kf = kf[0]
    
    tk = np.array([0.0, time / 2.0, time])
    kk = np.array([k0, km, kf])
    t = np.arange(0.0, time, time / n)
    fkp = scipy.interpolate.interp1d(tk, kk, kind="quadratic")
    kp = [fkp(ti) for ti in t]
    dt = float(time / n)<N><N>
    #  plt.plot(t, kp)
    #  plt.show()<N><N>
    state = State()
    x, y, yaw = [state.x], [state.y], [state.yaw]<N><N>
    for ikp in kp:
        state = update(state, v, ikp, dt, L)
        x.append(state.x)
        y.append(state.y)
        yaw.append(state.yaw)<N><N>
    return x, y, yaw<N><N>

def generate_last_state(s, km, kf, k0):<N><N>
    n = s / ds
    time = s / v  # [s]
    
    if isinstance(time,  type(np.array([]))): time = time[0]
    if isinstance(km, type(np.array([]))): km = km[0]
    if isinstance(kf, type(np.array([]))): kf = kf[0]
    
    tk = np.array([0.0, time / 2.0, time])
    kk = np.array([k0, km, kf])
    t = np.arange(0.0, time, time / n)
    fkp = scipy.interpolate.interp1d(tk, kk, kind="quadratic")
    kp = [fkp(ti) for ti in t]
    dt = time / n<N><N>
    #  plt.plot(t, kp)
    #  plt.show()<N><N>
    state = State()<N><N>
    _ = [update(state, v, ikp, dt, L) for ikp in kp]<N><N>
    return state.x, state.y, state.yaw
<N><N>
"""<N><N>Potential Field based path planner<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:
https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf<N><N>
"""<N><N>from collections import deque
import numpy as np
import matplotlib.pyplot as plt<N><N>
# Parameters
KP = 5.0  # attractive potential gain
ETA = 100.0  # repulsive potential gain
AREA_WIDTH = 30.0  # potential area width [m]
# the number of previous positions used to check oscillations
OSCILLATIONS_DETECTION_LENGTH = 3<N><N>
show_animation = True<N><N>

def calc_potential_field(gx, gy, ox, oy, reso, rr, sx, sy):
    minx = min(min(ox), sx, gx) - AREA_WIDTH / 2.0
    miny = min(min(oy), sy, gy) - AREA_WIDTH / 2.0
    maxx = max(max(ox), sx, gx) + AREA_WIDTH / 2.0
    maxy = max(max(oy), sy, gy) + AREA_WIDTH / 2.0
    xw = int(round((maxx - minx) / reso))
    yw = int(round((maxy - miny) / reso))<N><N>
    # calc each potential
    pmap = [[0.0 for i in range(yw)] for i in range(xw)]<N><N>
    for ix in range(xw):
        x = ix * reso + minx<N><N>
        for iy in range(yw):
            y = iy * reso + miny
            ug = calc_attractive_potential(x, y, gx, gy)
            uo = calc_repulsive_potential(x, y, ox, oy, rr)
            uf = ug + uo
            pmap[ix][iy] = uf<N><N>
    return pmap, minx, miny<N><N>

def calc_attractive_potential(x, y, gx, gy):
    return 0.5 * KP * np.hypot(x - gx, y - gy)<N><N>

def calc_repulsive_potential(x, y, ox, oy, rr):
    # search nearest obstacle
    minid = -1
    dmin = float("inf")
    for i, _ in enumerate(ox):
        d = np.hypot(x - ox[i], y - oy[i])
        if dmin >= d:
            dmin = d
            minid = i<N><N>
    # calc repulsive potential
    dq = np.hypot(x - ox[minid], y - oy[minid])<N><N>
    if dq <= rr:
        if dq <= 0.1:
            dq = 0.1<N><N>
        return 0.5 * ETA * (1.0 / dq - 1.0 / rr) ** 2
    else:
        return 0.0<N><N>

def get_motion_model():
    # dx, dy
    motion = [[1, 0],
              [0, 1],
              [-1, 0],
              [0, -1],
              [-1, -1],
              [-1, 1],
              [1, -1],
              [1, 1]]<N><N>
    return motion<N><N>

def oscillations_detection(previous_ids, ix, iy):
    previous_ids.append((ix, iy))<N><N>
    if (len(previous_ids) > OSCILLATIONS_DETECTION_LENGTH):
        previous_ids.popleft()<N><N>
    # check if contains any duplicates by copying into a set
    previous_ids_set = set()
    for index in previous_ids:
        if index in previous_ids_set:
            return True
        else:
            previous_ids_set.add(index)
    return False<N><N>

def potential_field_planning(sx, sy, gx, gy, ox, oy, reso, rr):<N><N>
    # calc potential field
    pmap, minx, miny = calc_potential_field(gx, gy, ox, oy, reso, rr, sx, sy)<N><N>
    # search path
    d = np.hypot(sx - gx, sy - gy)
    ix = round((sx - minx) / reso)
    iy = round((sy - miny) / reso)
    gix = round((gx - minx) / reso)
    giy = round((gy - miny) / reso)<N><N>
    if show_animation:
        draw_heatmap(pmap)
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect('key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
        plt.plot(ix, iy, "*k")
        plt.plot(gix, giy, "*m")<N><N>
    rx, ry = [sx], [sy]
    motion = get_motion_model()
    previous_ids = deque()<N><N>
"""<N><N>Probabilistic Road Map (PRM) Planner<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import random
import math
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import cKDTree<N><N>
# parameter
N_SAMPLE = 500  # number of sample_points
N_KNN = 10  # number of edge from one sampled point
MAX_EDGE_LEN = 30.0  # [m] Maximum edge length<N><N>
show_animation = True<N><N>

class Node:
    """
    Node class for dijkstra search
    """<N><N>
    def __init__(self, x, y, cost, parent_index):
        self.x = x
        self.y = y
        self.cost = cost
        self.parent_index = parent_index<N><N>
    def __str__(self):
        return str(self.x) + "," + str(self.y) + "," +\
               str(self.cost) + "," + str(self.parent_index)<N><N>

def prm_planning(sx, sy, gx, gy, ox, oy, rr):<N><N>
    obstacle_kd_tree = cKDTree(np.vstack((ox, oy)).T)<N><N>
    sample_x, sample_y = sample_points(sx, sy, gx, gy,
                                       rr, ox, oy, obstacle_kd_tree)
    if show_animation:
        plt.plot(sample_x, sample_y, ".b")<N><N>
    road_map = generate_road_map(sample_x, sample_y, rr, obstacle_kd_tree)<N><N>
    rx, ry = dijkstra_planning(
        sx, sy, gx, gy, road_map, sample_x, sample_y)<N><N>
    return rx, ry<N><N>

def is_collision(sx, sy, gx, gy, rr, obstacle_kd_tree):
    x = sx
    y = sy
    dx = gx - sx
    dy = gy - sy
    yaw = math.atan2(gy - sy, gx - sx)
    d = math.hypot(dx, dy)<N><N>
    if d >= MAX_EDGE_LEN:
        return True<N><N>
    D = rr
    n_step = round(d / D)<N><N>
    for i in range(n_step):
        dist, _ = obstacle_kd_tree.query([x, y])
        if dist <= rr:
            return True  # collision
        x += D * math.cos(yaw)
        y += D * math.sin(yaw)<N><N>
    # goal point check
    dist, _ = obstacle_kd_tree.query([gx, gy])
    if dist <= rr:
        return True  # collision<N><N>
    return False  # OK<N><N>

def generate_road_map(sample_x, sample_y, rr, obstacle_kd_tree):
    """
    Road map generation<N><N>
    sample_x: [m] x positions of sampled points
    sample_y: [m] y positions of sampled points
    rr: Robot Radius[m]
    obstacle_kd_tree: KDTree object of obstacles
    """<N><N>
    road_map = []
    n_sample = len(sample_x)
    sample_kd_tree = cKDTree(np.vstack((sample_x, sample_y)).T)<N><N>
    for (i, ix, iy) in zip(range(n_sample), sample_x, sample_y):<N><N>
        dists, indexes = sample_kd_tree.query([ix, iy], k=n_sample)
        edge_id = []<N><N>
        for ii in range(1, len(indexes)):
            nx = sample_x[indexes[ii]]
            ny = sample_y[indexes[ii]]<N><N>
            if not is_collision(ix, iy, nx, ny, rr, obstacle_kd_tree):
                edge_id.append(indexes[ii])<N><N>
            if len(edge_id) >= N_KNN:
                break<N><N>
        road_map.append(edge_id)<N><N>
    #  plot_road_map(road_map, sample_x, sample_y)<N><N>
    return road_map<N><N>

def dijkstra_planning(sx, sy, gx, gy, road_map, sample_x, sample_y):
    """
    s_x: start x position [m]
    s_y: start y position [m]
    gx: goal x position [m]
    gy: goal y position [m]
    ox: x position list of Obstacles [m]
    oy: y position list of Obstacles [m]
    rr: robot radius [m]
    road_map: ??? [m]
    sample_x: ??? [m]
    sample_y: ??? [m]<N><N>
    @return: Two lists of path coordinates ([x1, x2, ...], [y1, y2, ...]), empty list when no path was found
    """<N><N>
    start_node = Node(sx, sy, 0.0, -1)
    goal_node = Node(gx, gy, 0.0, -1)<N><N>
    open_set, closed_set = dict(), dict()
    open_set[len(road_map) - 2] = start_node<N><N>
    path_found = True<N><N>
    while True:
        if not open_set:
            print("Cannot find path")
            path_found = False
            break<N><N>
        c_id = min(open_set, key=lambda o: open_set[o].cost)
        current = open_set[c_id]<N><N>
        # show graph
        if show_animation and len(closed_set.keys()) % 2 == 0:
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
            plt.plot(current.x, current.y, "xg")
            plt.pause(0.001)<N><N>
        if c_id == (len(road_map) - 1):
            print("goal is found!")
            goal_node.parent_index = current.parent_index
            goal_node.cost = current.cost
            break<N><N>
        # Remove the item from the open set
        del open_set[c_id]
        # Add it to the closed set
        closed_set[c_id] = current<N><N>
        # expand search grid based on motion model
        for i in range(len(road_map[c_id])):
            n_id = road_map[c_id][i]
            dx = sample_x[n_id] - current.x
            dy = sample_y[n_id] - current.y
            d = math.hypot(dx, dy)
            node = Node(sample_x[n_id], sample_y[n_id],
                        current.cost + d, c_id)<N><N>
            if n_id in closed_set:
                continue
            # Otherwise if it is already in the open set
            if n_id in open_set:
                if open_set[n_id].cost > node.cost:
                    open_set[n_id].cost = node.cost
                    open_set[n_id].parent_index = c_id
            else:
                open_set[n_id] = node<N><N>
    if path_found is False:
        return [], []<N><N>
    # generate final course
    rx, ry = [goal_node.x], [goal_node.y]
    parent_index = goal_node.parent_index
    while parent_index != -1:
        n = closed_set[parent_index]
        rx.append(n.x)
        ry.append(n.y)
        parent_index = n.parent_index<N><N>
    return rx, ry<N><N>

def plot_road_map(road_map, sample_x, sample_y):  # pragma: no cover<N><N>
    for i, _ in enumerate(road_map):
        for ii in range(len(road_map[i])):
            ind = road_map[i][ii]<N><N>
            plt.plot([sample_x[i], sample_x[ind]],
                     [sample_y[i], sample_y[ind]], "-k")<N><N>

def sample_points(sx, sy, gx, gy, rr, ox, oy, obstacle_kd_tree):
    max_x = max(ox)
    max_y = max(oy)
    min_x = min(ox)
    min_y = min(oy)<N><N>
    sample_x, sample_y = [], []<N><N>
    while len(sample_x) <= N_SAMPLE:
        tx = (random.random() * (max_x - min_x)) + min_x
        ty = (random.random() * (max_y - min_y)) + min_y<N><N>
        dist, index = obstacle_kd_tree.query([tx, ty])<N><N>
        if dist >= rr:
            sample_x.append(tx)
            sample_y.append(ty)<N><N>
    sample_x.append(sx)
    sample_y.append(sy)
    sample_x.append(gx)
    sample_y.append(gy)<N><N>
    return sample_x, sample_y<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    # start and goal position
    sx = 10.0  # [m]
    sy = 10.0  # [m]
    gx = 50.0  # [m]
    gy = 50.0  # [m]
    robot_size = 5.0  # [m]<N><N>
    ox = []
    oy = []<N><N>
    for i in range(60):
        ox.append(i)
        oy.append(0.0)
    for i in range(60):
        ox.append(60.0)
        oy.append(i)
    for i in range(61):
        ox.append(i)
        oy.append(60.0)
    for i in range(61):
        ox.append(0.0)
        oy.append(i)
    for i in range(40):
        ox.append(20.0)
        oy.append(i)
    for i in range(40):
        ox.append(40.0)
        oy.append(60.0 - i)<N><N>
    if show_animation:
        plt.plot(ox, oy, ".k")
        plt.plot(sx, sy, "^r")
        plt.plot(gx, gy, "^c")
        plt.grid(True)
        plt.axis("equal")<N><N>
    rx, ry = prm_planning(sx, sy, gx, gy, ox, oy, robot_size)<N><N>
    assert rx, 'Cannot found path'<N><N>
    if show_animation:
        plt.plot(rx, ry, "-r")
        plt.pause(0.001)
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Quintic Polynomials Planner<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:<N><N>
- [Local Path planning And Motion Control For Agv In Positioning](http://ieeexplore.ieee.org/document/637936/)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
# parameter
MAX_T = 100.0  # maximum time to the goal [s]
MIN_T = 5.0  # minimum time to the goal[s]<N><N>
show_animation = True<N><N>

class QuinticPolynomial:<N><N>
    def __init__(self, xs, vxs, axs, xe, vxe, axe, time):
        # calc coefficient of quintic polynomial
        # See jupyter notebook document for derivation of this equation.
        self.a0 = xs
        self.a1 = vxs
        self.a2 = axs / 2.0<N><N>
        A = np.array([[time ** 3, time ** 4, time ** 5],
                      [3 * time ** 2, 4 * time ** 3, 5 * time ** 4],
                      [6 * time, 12 * time ** 2, 20 * time ** 3]])
        b = np.array([xe - self.a0 - self.a1 * time - self.a2 * time ** 2,
                      vxe - self.a1 - 2 * self.a2 * time,
                      axe - 2 * self.a2])
        x = np.linalg.solve(A, b)<N><N>
        self.a3 = x[0]
        self.a4 = x[1]
        self.a5 = x[2]<N><N>
    def calc_point(self, t):
        xt = self.a0 + self.a1 * t + self.a2 * t ** 2 + \
             self.a3 * t ** 3 + self.a4 * t ** 4 + self.a5 * t ** 5<N><N>
        return xt<N><N>
    def calc_first_derivative(self, t):
        xt = self.a1 + 2 * self.a2 * t + \
             3 * self.a3 * t ** 2 + 4 * self.a4 * t ** 3 + 5 * self.a5 * t ** 4<N><N>
        return xt<N><N>
    def calc_second_derivative(self, t):
        xt = 2 * self.a2 + 6 * self.a3 * t + 12 * self.a4 * t ** 2 + 20 * self.a5 * t ** 3<N><N>
        return xt<N><N>
    def calc_third_derivative(self, t):
        xt = 6 * self.a3 + 24 * self.a4 * t + 60 * self.a5 * t ** 2<N><N>
        return xt<N><N>

def quintic_polynomials_planner(sx, sy, syaw, sv, sa, gx, gy, gyaw, gv, ga, max_accel, max_jerk, dt):
    """
    quintic polynomial planner<N><N>
    input
        s_x: start x position [m]
        s_y: start y position [m]
        s_yaw: start yaw angle [rad]
        sa: start accel [m/ss]
        gx: goal x position [m]
        gy: goal y position [m]
        gyaw: goal yaw angle [rad]
        ga: goal accel [m/ss]
        max_accel: maximum accel [m/ss]
        max_jerk: maximum jerk [m/sss]
        dt: time tick [s]<N><N>
    return
        time: time result
        rx: x position result list
        ry: y position result list
        ryaw: yaw angle result list
        rv: velocity result list
        ra: accel result list<N><N>
    """<N><N>
    vxs = sv * math.cos(syaw)
    vys = sv * math.sin(syaw)
    vxg = gv * math.cos(gyaw)
    vyg = gv * math.sin(gyaw)<N><N>
    axs = sa * math.cos(syaw)
    ays = sa * math.sin(syaw)
    axg = ga * math.cos(gyaw)
    ayg = ga * math.sin(gyaw)<N><N>
    time, rx, ry, ryaw, rv, ra, rj = [], [], [], [], [], [], []<N><N>
    for T in np.arange(MIN_T, MAX_T, MIN_T):
        xqp = QuinticPolynomial(sx, vxs, axs, gx, vxg, axg, T)
        yqp = QuinticPolynomial(sy, vys, ays, gy, vyg, ayg, T)<N><N>
        time, rx, ry, ryaw, rv, ra, rj = [], [], [], [], [], [], []<N><N>
        for t in np.arange(0.0, T + dt, dt):
            time.append(t)
            rx.append(xqp.calc_point(t))
            ry.append(yqp.calc_point(t))<N><N>
            vx = xqp.calc_first_derivative(t)
            vy = yqp.calc_first_derivative(t)
            v = np.hypot(vx, vy)
            yaw = math.atan2(vy, vx)
            rv.append(v)
            ryaw.append(yaw)<N><N>
            ax = xqp.calc_second_derivative(t)
            ay = yqp.calc_second_derivative(t)
            a = np.hypot(ax, ay)
            if len(rv) >= 2 and rv[-1] - rv[-2] < 0.0:
                a *= -1
            ra.append(a)<N><N>
            jx = xqp.calc_third_derivative(t)
            jy = yqp.calc_third_derivative(t)
            j = np.hypot(jx, jy)
            if len(ra) >= 2 and ra[-1] - ra[-2] < 0.0:
                j *= -1
            rj.append(j)<N><N>
        if max([abs(i) for i in ra]) <= max_accel and max([abs(i) for i in rj]) <= max_jerk:
            print("find path!!")
            break<N><N>
"""<N><N>Reeds Shepp path planner sample code<N><N>
author Atsushi Sakai(@Atsushi_twi)<N><N>
"""
import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
show_animation = True<N><N>

class Path:<N><N>
    def __init__(self):
        self.lengths = []
        self.ctypes = []
        self.L = 0.0
        self.x = []
        self.y = []
        self.yaw = []
        self.directions = []<N><N>

def plot_arrow(x, y, yaw, length=1.0, width=0.5, fc="r", ec="k"):
    """
    Plot arrow
    """<N><N>
    if not isinstance(x, float):
        for (ix, iy, iyaw) in zip(x, y, yaw):
            plot_arrow(ix, iy, iyaw)
    else:
        plt.arrow(x, y, length * math.cos(yaw), length * math.sin(yaw),
                  fc=fc, ec=ec, head_width=width, head_length=width)
        plt.plot(x, y)<N><N>

def mod2pi(x):
    # Be consistent with fmod in cplusplus here.
    v = np.mod(x, np.copysign(2.0 * math.pi, x))
    if v < -math.pi:
        v += 2.0 * math.pi
    else:
        if v > math.pi:
            v -= 2.0 * math.pi
    return v<N><N>
"""<N><N>Path planning Sample Code with Randomized Rapidly-Exploring Random Trees (RRT)<N><N>
author: AtsushiSakai(@Atsushi_twi)<N><N>
"""<N><N>import math
import random<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
show_animation = True<N><N>

class RRT:
    """
    Class for RRT planning
    """<N><N>
    class Node:
        """
        RRT Node
        """<N><N>
        def __init__(self, x, y):
            self.x = x
            self.y = y
            self.path_x = []
            self.path_y = []
            self.parent = None<N><N>
    def __init__(self,
                 start,
                 goal,
                 obstacle_list,
                 rand_area,
                 expand_dis=3.0,
                 path_resolution=0.5,
                 goal_sample_rate=5,
                 max_iter=500):
        """
        Setting Parameter<N><N>
        start:Start Position [x,y]
        goal:Goal Position [x,y]
        obstacleList:obstacle Positions [[x,y,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
        """
        self.start = self.Node(start[0], start[1])
        self.end = self.Node(goal[0], goal[1])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.expand_dis = expand_dis
        self.path_resolution = path_resolution
        self.goal_sample_rate = goal_sample_rate
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list
        self.node_list = []<N><N>
    def planning(self, animation=True):
        """
        rrt path planning<N><N>
        animation: flag for animation on or off
        """<N><N>
        self.node_list = [self.start]
        for i in range(self.max_iter):
            rnd_node = self.get_random_node()
            nearest_ind = self.get_nearest_node_index(self.node_list, rnd_node)
            nearest_node = self.node_list[nearest_ind]<N><N>
            new_node = self.steer(nearest_node, rnd_node, self.expand_dis)<N><N>
            if self.check_collision(new_node, self.obstacle_list):
                self.node_list.append(new_node)<N><N>
            if animation and i % 5 == 0:
                self.draw_graph(rnd_node)<N><N>
            if self.calc_dist_to_goal(self.node_list[-1].x,
                                      self.node_list[-1].y) <= self.expand_dis:
                final_node = self.steer(self.node_list[-1], self.end,
                                        self.expand_dis)
                if self.check_collision(final_node, self.obstacle_list):
                    return self.generate_final_course(len(self.node_list) - 1)<N><N>
            if animation and i % 5:
                self.draw_graph(rnd_node)<N><N>
        return None  # cannot find path<N><N>
    def steer(self, from_node, to_node, extend_length=float("inf")):<N><N>
        new_node = self.Node(from_node.x, from_node.y)
        d, theta = self.calc_distance_and_angle(new_node, to_node)<N><N>
        new_node.path_x = [new_node.x]
        new_node.path_y = [new_node.y]<N><N>
        if extend_length > d:
            extend_length = d<N><N>
        n_expand = math.floor(extend_length / self.path_resolution)<N><N>
        for _ in range(n_expand):
            new_node.x += self.path_resolution * math.cos(theta)
            new_node.y += self.path_resolution * math.sin(theta)
            new_node.path_x.append(new_node.x)
            new_node.path_y.append(new_node.y)<N><N>
        d, _ = self.calc_distance_and_angle(new_node, to_node)
        if d <= self.path_resolution:
            new_node.path_x.append(to_node.x)
            new_node.path_y.append(to_node.y)
            new_node.x = to_node.x
            new_node.y = to_node.y<N><N>
        new_node.parent = from_node<N><N>
        return new_node<N><N>
    def generate_final_course(self, goal_ind):
        path = [[self.end.x, self.end.y]]
        node = self.node_list[goal_ind]
        while node.parent is not None:
            path.append([node.x, node.y])
            node = node.parent
        path.append([node.x, node.y])<N><N>
        return path<N><N>
    def calc_dist_to_goal(self, x, y):
        dx = x - self.end.x
        dy = y - self.end.y
        return math.hypot(dx, dy)<N><N>
    def get_random_node(self):
        if random.randint(0, 100) > self.goal_sample_rate:
            rnd = self.Node(
                random.uniform(self.min_rand, self.max_rand),
                random.uniform(self.min_rand, self.max_rand))
        else:  # goal point sampling
            rnd = self.Node(self.end.x, self.end.y)
        return rnd<N><N>
    def draw_graph(self, rnd=None):
        plt.clf()
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect(
            'key_release_event',
            lambda event: [exit(0) if event.key == 'escape' else None])
        if rnd is not None:
            plt.plot(rnd.x, rnd.y, "^k")
        for node in self.node_list:
            if node.parent:
                plt.plot(node.path_x, node.path_y, "-g")<N><N>
        for (ox, oy, size) in self.obstacle_list:
            self.plot_circle(ox, oy, size)<N><N>
        plt.plot(self.start.x, self.start.y, "xr")
        plt.plot(self.end.x, self.end.y, "xr")
        plt.axis("equal")
        plt.axis([-2, 15, -2, 15])
        plt.grid(True)
        plt.pause(0.01)<N><N>
    @staticmethod
    def plot_circle(x, y, size, color="-b"):  # pragma: no cover
        deg = list(range(0, 360, 5))
        deg.append(0)
        xl = [x + size * math.cos(np.deg2rad(d)) for d in deg]
        yl = [y + size * math.sin(np.deg2rad(d)) for d in deg]
        plt.plot(xl, yl, color)<N><N>
    @staticmethod
    def get_nearest_node_index(node_list, rnd_node):
        dlist = [(node.x - rnd_node.x)**2 + (node.y - rnd_node.y)**2
                 for node in node_list]
        minind = dlist.index(min(dlist))<N><N>
        return minind<N><N>
    @staticmethod
    def check_collision(node, obstacleList):<N><N>
        if node is None:
            return False<N><N>
        for (ox, oy, size) in obstacleList:
            dx_list = [ox - x for x in node.path_x]
            dy_list = [oy - y for y in node.path_y]
            d_list = [dx * dx + dy * dy for (dx, dy) in zip(dx_list, dy_list)]<N><N>
            if min(d_list) <= size**2:
                return False  # collision<N><N>
        return True  # safe<N><N>
    @staticmethod
    def calc_distance_and_angle(from_node, to_node):
        dx = to_node.x - from_node.x
        dy = to_node.y - from_node.y
        d = math.hypot(dx, dy)
        theta = math.atan2(dy, dx)
        return d, theta<N><N>

def main(gx=6.0, gy=10.0):
    print("start " + __file__)<N><N>
    # ====Search Path with RRT====
    obstacleList = [(5, 5, 1), (3, 6, 2), (3, 8, 2), (3, 10, 2), (7, 5, 2),
                    (9, 5, 2), (8, 10, 1)]  # [x, y, radius]
    # Set Initial parameters
    rrt = RRT(
        start=[0, 0],
        goal=[gx, gy],
        rand_area=[-2, 15],
        obstacle_list=obstacleList)
    path = rrt.planning(animation=show_animation)<N><N>
    if path is None:
        print("Cannot find path")
    else:
        print("found path!!")<N><N>
        # Draw final path
        if show_animation:
            rrt.draw_graph()
            plt.plot([x for (x, y) in path], [y for (x, y) in path], '-r')
            plt.grid(True)
            plt.pause(0.01)  # Need for Mac
            plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Path planning Sample Code with RRT with path smoothing<N><N>
@author: AtsushiSakai(@Atsushi_twi)<N><N>
"""<N><N>import math
import os
import random
import sys<N><N>
import matplotlib.pyplot as plt<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)))<N><N>
try:
    from rrt import RRT
except ImportError:
    raise<N><N>
show_animation = True<N><N>

def get_path_length(path):
    le = 0
    for i in range(len(path) - 1):
        dx = path[i + 1][0] - path[i][0]
        dy = path[i + 1][1] - path[i][1]
        d = math.sqrt(dx * dx + dy * dy)
        le += d<N><N>
    return le<N><N>

def get_target_point(path, targetL):
    le = 0
    ti = 0
    lastPairLen = 0
    for i in range(len(path) - 1):
        dx = path[i + 1][0] - path[i][0]
        dy = path[i + 1][1] - path[i][1]
        d = math.sqrt(dx * dx + dy * dy)
        le += d
        if le >= targetL:
            ti = i - 1
            lastPairLen = d
            break<N><N>
    partRatio = (le - targetL) / lastPairLen<N><N>
    x = path[ti][0] + (path[ti + 1][0] - path[ti][0]) * partRatio
    y = path[ti][1] + (path[ti + 1][1] - path[ti][1]) * partRatio<N><N>
    return [x, y, ti]<N><N>

def line_collision_check(first, second, obstacleList):
    # Line Equation<N><N>
    x1 = first[0]
    y1 = first[1]
    x2 = second[0]
    y2 = second[1]<N><N>
    try:
        a = y2 - y1
        b = -(x2 - x1)
        c = y2 * (x2 - x1) - x2 * (y2 - y1)
    except ZeroDivisionError:
        return False<N><N>
    for (ox, oy, size) in obstacleList:
        d = abs(a * ox + b * oy + c) / (math.sqrt(a * a + b * b))
        if d <= size:
            return False<N><N>
    return True  # OK<N><N>

def path_smoothing(path, max_iter, obstacle_list):
    le = get_path_length(path)<N><N>
    for i in range(max_iter):
        # Sample two points
        pickPoints = [random.uniform(0, le), random.uniform(0, le)]
        pickPoints.sort()
        first = get_target_point(path, pickPoints[0])
        second = get_target_point(path, pickPoints[1])<N><N>
        if first[2] <= 0 or second[2] <= 0:
            continue<N><N>
        if (second[2] + 1) > len(path):
            continue<N><N>
        if second[2] == first[2]:
            continue<N><N>
        # collision check
        if not line_collision_check(first, second, obstacle_list):
            continue<N><N>
        # Create New path
        newPath = []
        newPath.extend(path[:first[2] + 1])
        newPath.append([first[0], first[1]])
        newPath.append([second[0], second[1]])
        newPath.extend(path[second[2] + 1:])
        path = newPath
        le = get_path_length(path)<N><N>
    return path<N><N>

def main():
    # ====Search Path with RRT====
    # Parameter
    obstacleList = [
        (5, 5, 1),
        (3, 6, 2),
        (3, 8, 2),
        (3, 10, 2),
        (7, 5, 2),
        (9, 5, 2)
    ]  # [x,y,size]
    rrt = RRT(start=[0, 0], goal=[6, 10],
              rand_area=[-2, 15], obstacle_list=obstacleList)
    path = rrt.planning(animation=show_animation)<N><N>
    # Path smoothing
    maxIter = 1000
    smoothedPath = path_smoothing(path, maxIter, obstacleList)<N><N>
    # Draw final path
    if show_animation:
        rrt.draw_graph()
        plt.plot([x for (x, y) in path], [y for (x, y) in path], '-r')<N><N>
        plt.plot([x for (x, y) in smoothedPath], [
            y for (x, y) in smoothedPath], '-c')<N><N>
        plt.grid(True)
        plt.pause(0.01)  # Need for Mac
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Path planning Sample Code with Randomized Rapidly-Exploring Random
Trees with sobol low discrepancy sampler(RRTSobol).
Sobol wiki https://en.wikipedia.org/wiki/Sobol_sequence<N><N>
"""
  Licensing:
    This code is distributed under the MIT license.<N><N>
  Authors:
    Original FORTRAN77 version of i4_sobol by Bennett Fox.
    MATLAB version by John Burkardt.
    PYTHON version by Corrado Chisari<N><N>
    Original Python version of is_prime by Corrado Chisari<N><N>
    Original MATLAB versions of other functions by John Burkardt.
    PYTHON versions by Corrado Chisari<N><N>
    Original code is available at
    http://people.sc.fsu.edu/~jburkardt/py_src/sobol/sobol.html<N><N>
    Note: the i4 prefix means that the function takes a numeric argument or
          returns a number which is interpreted inside the function as a 4
          byte integer
    Note: the r4 prefix means that the function takes a numeric argument or
          returns a number which is interpreted inside the function as a 4
          byte float
"""
import math
import sys
import numpy as np<N><N>
atmost = None
dim_max = None
dim_num_save = None
initialized = None
lastq = None
log_max = None
maxcol = None
poly = None
recipd = None
seed_save = None
v = None<N><N>

def i4_bit_hi1(n):
    """
     I4_BIT_HI1 returns the position of the high 1 bit base 2 in an I4.<N><N>
      Discussion:<N><N>
        An I4 is an integer ( kind = 4 ) value.<N><N>
      Example:<N><N>
from .sobol import i4_sobol as sobol_quasirand

"""
Path planning Sample Code with RRT with Dubins path<N><N>
author: AtsushiSakai(@Atsushi_twi)<N><N>
"""
import copy
import math
import os
import random
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../DubinsPath/")
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../RRT/")<N><N>
try:
    from rrt import RRT
    import dubins_path_planning
except ImportError:
    raise<N><N>
show_animation = True<N><N>

class RRTDubins(RRT):
    """
    Class for RRT planning with Dubins path
    """<N><N>
    class Node(RRT.Node):
        """
        RRT Node
        """<N><N>
        def __init__(self, x, y, yaw):
            super().__init__(x, y)
            self.cost = 0
            self.yaw = yaw
            self.path_yaw = []<N><N>
    def __init__(self, start, goal, obstacle_list, rand_area,
                 goal_sample_rate=10,
                 max_iter=200,
                 ):
        """
        Setting Parameter<N><N>
        start:Start Position [x,y]
        goal:Goal Position [x,y]
        obstacleList:obstacle Positions [[x,y,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
        """
        self.start = self.Node(start[0], start[1], start[2])
        self.end = self.Node(goal[0], goal[1], goal[2])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.goal_sample_rate = goal_sample_rate
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list<N><N>
        self.curvature = 1.0  # for dubins path
        self.goal_yaw_th = np.deg2rad(1.0)
        self.goal_xy_th = 0.5<N><N>
    def planning(self, animation=True, search_until_max_iter=True):
        """
        execute planning<N><N>
        animation: flag for animation on or off
        """<N><N>
        self.node_list = [self.start]
        for i in range(self.max_iter):
            print("Iter:", i, ", number of nodes:", len(self.node_list))
            rnd = self.get_random_node()
            nearest_ind = self.get_nearest_node_index(self.node_list, rnd)
            new_node = self.steer(self.node_list[nearest_ind], rnd)<N><N>
            if self.check_collision(new_node, self.obstacle_list):
                self.node_list.append(new_node)<N><N>
            if animation and i % 5 == 0:
                self.plot_start_goal_arrow()
                self.draw_graph(rnd)<N><N>
            if (not search_until_max_iter) and new_node:  # check reaching the goal
                last_index = self.search_best_goal_node()
                if last_index:
                    return self.generate_final_course(last_index)<N><N>
        print("reached max iteration")<N><N>
        last_index = self.search_best_goal_node()
        if last_index:
            return self.generate_final_course(last_index)
        else:
            print("Cannot find path")<N><N>
        return None<N><N>
    def draw_graph(self, rnd=None):  # pragma: no cover
        plt.clf()
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect('key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
        if rnd is not None:
            plt.plot(rnd.x, rnd.y, "^k")
        for node in self.node_list:
            if node.parent:
                plt.plot(node.path_x, node.path_y, "-g")<N><N>
        for (ox, oy, size) in self.obstacle_list:
            plt.plot(ox, oy, "ok", ms=30 * size)<N><N>
        plt.plot(self.start.x, self.start.y, "xr")
        plt.plot(self.end.x, self.end.y, "xr")
        plt.axis([-2, 15, -2, 15])
        plt.grid(True)
        self.plot_start_goal_arrow()
        plt.pause(0.01)<N><N>
    def plot_start_goal_arrow(self):  # pragma: no cover
        dubins_path_planning.plot_arrow(
            self.start.x, self.start.y, self.start.yaw)
        dubins_path_planning.plot_arrow(
            self.end.x, self.end.y, self.end.yaw)<N><N>
    def steer(self, from_node, to_node):<N><N>
        px, py, pyaw, mode, course_length = dubins_path_planning.dubins_path_planning(
            from_node.x, from_node.y, from_node.yaw,
            to_node.x, to_node.y, to_node.yaw, self.curvature)<N><N>
        if len(px) <= 1:  # cannot find a dubins path
            return None<N><N>
        new_node = copy.deepcopy(from_node)
        new_node.x = px[-1]
        new_node.y = py[-1]
        new_node.yaw = pyaw[-1]<N><N>
        new_node.path_x = px
        new_node.path_y = py
        new_node.path_yaw = pyaw
        new_node.cost += course_length
        new_node.parent = from_node<N><N>
        return new_node<N><N>
    def calc_new_cost(self, from_node, to_node):<N><N>
        _, _, _, _, course_length = dubins_path_planning.dubins_path_planning(
            from_node.x, from_node.y, from_node.yaw,
            to_node.x, to_node.y, to_node.yaw, self.curvature)<N><N>
        return from_node.cost + course_length<N><N>
    def get_random_node(self):<N><N>
        if random.randint(0, 100) > self.goal_sample_rate:
            rnd = self.Node(random.uniform(self.min_rand, self.max_rand),
                            random.uniform(self.min_rand, self.max_rand),
                            random.uniform(-math.pi, math.pi)
                            )
        else:  # goal point sampling
            rnd = self.Node(self.end.x, self.end.y, self.end.yaw)<N><N>
        return rnd<N><N>
    def search_best_goal_node(self):<N><N>
        goal_indexes = []
        for (i, node) in enumerate(self.node_list):
            if self.calc_dist_to_goal(node.x, node.y) <= self.goal_xy_th:
                goal_indexes.append(i)<N><N>
        # angle check
        final_goal_indexes = []
        for i in goal_indexes:
            if abs(self.node_list[i].yaw - self.end.yaw) <= self.goal_yaw_th:
                final_goal_indexes.append(i)<N><N>
        if not final_goal_indexes:
            return None<N><N>
        min_cost = min([self.node_list[i].cost for i in final_goal_indexes])
        for i in final_goal_indexes:
            if self.node_list[i].cost == min_cost:
                return i<N><N>
        return None<N><N>
    def generate_final_course(self, goal_index):
        print("final")
        path = [[self.end.x, self.end.y]]
        node = self.node_list[goal_index]
        while node.parent:
            for (ix, iy) in zip(reversed(node.path_x), reversed(node.path_y)):
                path.append([ix, iy])
            node = node.parent
        path.append([self.start.x, self.start.y])
        return path<N><N>

def main():
    print("Start " + __file__)
    # ====Search Path with RRT====
    obstacleList = [
        (5, 5, 1),
        (3, 6, 2),
        (3, 8, 2),
        (3, 10, 2),
        (7, 5, 2),
        (9, 5, 2)
    ]  # [x,y,size(radius)]<N><N>
    # Set Initial parameters
    start = [0.0, 0.0, np.deg2rad(0.0)]
    goal = [10.0, 10.0, np.deg2rad(0.0)]<N><N>
    rrt_dubins = RRTDubins(start, goal, obstacleList, [-2.0, 15.0])
    path = rrt_dubins.planning(animation=show_animation)<N><N>
    # Draw final path
    if show_animation:  # pragma: no cover
        rrt_dubins.draw_graph()
        plt.plot([x for (x, y) in path], [y for (x, y) in path], '-r')
        plt.grid(True)
        plt.pause(0.001)
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Path planning Sample Code with RRT*<N><N>
author: Atsushi Sakai(@Atsushi_twi)<N><N>
"""<N><N>import math
import os
import sys<N><N>
import matplotlib.pyplot as plt<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/../RRT/")<N><N>
try:
    from rrt import RRT
except ImportError:
    raise<N><N>
show_animation = True<N><N>

class RRTStar(RRT):
    """
    Class for RRT Star planning
    """<N><N>
    class Node(RRT.Node):
        def __init__(self, x, y):
            super().__init__(x, y)
            self.cost = 0.0<N><N>
    def __init__(self,
                 start,
                 goal,
                 obstacle_list,
                 rand_area,
                 expand_dis=30.0,
                 path_resolution=1.0,
                 goal_sample_rate=20,
                 max_iter=300,
                 connect_circle_dist=50.0,
                 search_until_max_iter=False):
        """
        Setting Parameter<N><N>
        start:Start Position [x,y]
        goal:Goal Position [x,y]
        obstacleList:obstacle Positions [[x,y,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
        """
        super().__init__(start, goal, obstacle_list, rand_area, expand_dis,
                         path_resolution, goal_sample_rate, max_iter)
        self.connect_circle_dist = connect_circle_dist
        self.goal_node = self.Node(goal[0], goal[1])
        self.search_until_max_iter = search_until_max_iter<N><N>
    def planning(self, animation=True):
        """
        rrt star path planning<N><N>
        animation: flag for animation on or off .
        """<N><N>
"""
Path planning Sample Code with RRT and Dubins path<N><N>
author: AtsushiSakai(@Atsushi_twi)<N><N>
"""<N><N>import copy
import math
import os
import random
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../DubinsPath/")
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../RRTStar/")<N><N>
try:
    import dubins_path_planning
    from rrt_star import RRTStar
except ImportError:
    raise<N><N>
show_animation = True<N><N>

class RRTStarDubins(RRTStar):
    """
    Class for RRT star planning with Dubins path
    """<N><N>
    class Node(RRTStar.Node):
        """
        RRT Node
        """<N><N>
        def __init__(self, x, y, yaw):
            super().__init__(x, y)
            self.yaw = yaw
            self.path_yaw = []<N><N>
    def __init__(self, start, goal, obstacle_list, rand_area,
                 goal_sample_rate=10,
                 max_iter=200,
                 connect_circle_dist=50.0
                 ):
        """
        Setting Parameter<N><N>
        start:Start Position [x,y]
        goal:Goal Position [x,y]
        obstacleList:obstacle Positions [[x,y,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
        """
        self.start = self.Node(start[0], start[1], start[2])
        self.end = self.Node(goal[0], goal[1], goal[2])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.goal_sample_rate = goal_sample_rate
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list
        self.connect_circle_dist = connect_circle_dist<N><N>
        self.curvature = 1.0  # for dubins path
        self.goal_yaw_th = np.deg2rad(1.0)
        self.goal_xy_th = 0.5<N><N>
    def planning(self, animation=True, search_until_max_iter=True):
        """
        RRT Star planning<N><N>
        animation: flag for animation on or off
        """<N><N>
        self.node_list = [self.start]
        for i in range(self.max_iter):
            print("Iter:", i, ", number of nodes:", len(self.node_list))
            rnd = self.get_random_node()
            nearest_ind = self.get_nearest_node_index(self.node_list, rnd)
            new_node = self.steer(self.node_list[nearest_ind], rnd)<N><N>
            if self.check_collision(new_node, self.obstacle_list):
                near_indexes = self.find_near_nodes(new_node)
                new_node = self.choose_parent(new_node, near_indexes)
                if new_node:
                    self.node_list.append(new_node)
                    self.rewire(new_node, near_indexes)<N><N>
            if animation and i % 5 == 0:
                self.plot_start_goal_arrow()
                self.draw_graph(rnd)<N><N>
            if (not search_until_max_iter) and new_node:  # check reaching the goal
                last_index = self.search_best_goal_node()
                if last_index:
                    return self.generate_final_course(last_index)<N><N>
        print("reached max iteration")<N><N>
        last_index = self.search_best_goal_node()
        if last_index:
            return self.generate_final_course(last_index)
        else:
            print("Cannot find path")<N><N>
        return None<N><N>
    def draw_graph(self, rnd=None):
        plt.clf()
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect('key_release_event',
                                     lambda event: [exit(0) if event.key == 'escape' else None])
        if rnd is not None:
            plt.plot(rnd.x, rnd.y, "^k")
        for node in self.node_list:
            if node.parent:
                plt.plot(node.path_x, node.path_y, "-g")<N><N>
        for (ox, oy, size) in self.obstacle_list:
            plt.plot(ox, oy, "ok", ms=30 * size)<N><N>
        plt.plot(self.start.x, self.start.y, "xr")
        plt.plot(self.end.x, self.end.y, "xr")
        plt.axis([-2, 15, -2, 15])
        plt.grid(True)
        self.plot_start_goal_arrow()
        plt.pause(0.01)<N><N>
    def plot_start_goal_arrow(self):
        dubins_path_planning.plot_arrow(
            self.start.x, self.start.y, self.start.yaw)
        dubins_path_planning.plot_arrow(
            self.end.x, self.end.y, self.end.yaw)<N><N>
    def steer(self, from_node, to_node):<N><N>
        px, py, pyaw, mode, course_length = dubins_path_planning.dubins_path_planning(
            from_node.x, from_node.y, from_node.yaw,
            to_node.x, to_node.y, to_node.yaw, self.curvature)<N><N>
        if len(px) <= 1:  # cannot find a dubins path
            return None<N><N>
        new_node = copy.deepcopy(from_node)
        new_node.x = px[-1]
        new_node.y = py[-1]
        new_node.yaw = pyaw[-1]<N><N>
        new_node.path_x = px
        new_node.path_y = py
        new_node.path_yaw = pyaw
        new_node.cost += course_length
        new_node.parent = from_node<N><N>
        return new_node<N><N>
    def calc_new_cost(self, from_node, to_node):<N><N>
        _, _, _, _, course_length = dubins_path_planning.dubins_path_planning(
            from_node.x, from_node.y, from_node.yaw,
            to_node.x, to_node.y, to_node.yaw, self.curvature)<N><N>
        return from_node.cost + course_length<N><N>
    def get_random_node(self):<N><N>
        if random.randint(0, 100) > self.goal_sample_rate:
            rnd = self.Node(random.uniform(self.min_rand, self.max_rand),
                            random.uniform(self.min_rand, self.max_rand),
                            random.uniform(-math.pi, math.pi)
                            )
        else:  # goal point sampling
            rnd = self.Node(self.end.x, self.end.y, self.end.yaw)<N><N>
        return rnd<N><N>
    def search_best_goal_node(self):<N><N>
        goal_indexes = []
        for (i, node) in enumerate(self.node_list):
            if self.calc_dist_to_goal(node.x, node.y) <= self.goal_xy_th:
                goal_indexes.append(i)<N><N>
        # angle check
        final_goal_indexes = []
        for i in goal_indexes:
            if abs(self.node_list[i].yaw - self.end.yaw) <= self.goal_yaw_th:
                final_goal_indexes.append(i)<N><N>
        if not final_goal_indexes:
            return None<N><N>
        min_cost = min([self.node_list[i].cost for i in final_goal_indexes])
        for i in final_goal_indexes:
            if self.node_list[i].cost == min_cost:
                return i<N><N>
        return None<N><N>
    def generate_final_course(self, goal_index):
        print("final")
        path = [[self.end.x, self.end.y]]
        node = self.node_list[goal_index]
        while node.parent:
            for (ix, iy) in zip(reversed(node.path_x), reversed(node.path_y)):
                path.append([ix, iy])
            node = node.parent
        path.append([self.start.x, self.start.y])
        return path<N><N>

def main():
    print("Start rrt star with dubins planning")<N><N>
    # ====Search Path with RRT====
    obstacleList = [
        (5, 5, 1),
        (3, 6, 2),
        (3, 8, 2),
        (3, 10, 2),
        (7, 5, 2),
        (9, 5, 2)
    ]  # [x,y,size(radius)]<N><N>
    # Set Initial parameters
    start = [0.0, 0.0, np.deg2rad(0.0)]
    goal = [10.0, 10.0, np.deg2rad(0.0)]<N><N>
    rrtstar_dubins = RRTStarDubins(start, goal, rand_area=[-2.0, 15.0], obstacle_list=obstacleList)
    path = rrtstar_dubins.planning(animation=show_animation)<N><N>
    # Draw final path
    if show_animation:  # pragma: no cover
        rrtstar_dubins.draw_graph()
        plt.plot([x for (x, y) in path], [y for (x, y) in path], '-r')
        plt.grid(True)
        plt.pause(0.001)<N><N>
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Path planning Sample Code with RRT with Reeds-Shepp path<N><N>
author: AtsushiSakai(@Atsushi_twi)<N><N>
"""
import copy
import math
import os
import random
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../ReedsSheppPath/")
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../RRTStar/")<N><N>
try:
    import reeds_shepp_path_planning
    from rrt_star import RRTStar
except ImportError:
    raise<N><N>
show_animation = True<N><N>

class RRTStarReedsShepp(RRTStar):
    """
    Class for RRT star planning with Reeds Shepp path
    """<N><N>
    class Node(RRTStar.Node):
        """
        RRT Node
        """<N><N>
        def __init__(self, x, y, yaw):
            super().__init__(x, y)
            self.yaw = yaw
            self.path_yaw = []<N><N>
    def __init__(self, start, goal, obstacle_list, rand_area,
                 max_iter=200,
                 connect_circle_dist=50.0
                 ):
        """
        Setting Parameter<N><N>
        start:Start Position [x,y]
        goal:Goal Position [x,y]
        obstacleList:obstacle Positions [[x,y,size],...]
        randArea:Random Sampling Area [min,max]<N><N>
        """
        self.start = self.Node(start[0], start[1], start[2])
        self.end = self.Node(goal[0], goal[1], goal[2])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list
        self.connect_circle_dist = connect_circle_dist<N><N>
        self.curvature = 1.0
        self.goal_yaw_th = np.deg2rad(1.0)
        self.goal_xy_th = 0.5<N><N>
    def planning(self, animation=True, search_until_max_iter=True):
        """
        planning<N><N>
        animation: flag for animation on or off
        """<N><N>
        self.node_list = [self.start]
        for i in range(self.max_iter):
            print("Iter:", i, ", number of nodes:", len(self.node_list))
            rnd = self.get_random_node()
            nearest_ind = self.get_nearest_node_index(self.node_list, rnd)
            new_node = self.steer(self.node_list[nearest_ind], rnd)<N><N>
            if self.check_collision(new_node, self.obstacle_list):
                near_indexes = self.find_near_nodes(new_node)
                new_node = self.choose_parent(new_node, near_indexes)
                if new_node:
                    self.node_list.append(new_node)
                    self.rewire(new_node, near_indexes)
                    self.try_goal_path(new_node)<N><N>
            if animation and i % 5 == 0:
                self.plot_start_goal_arrow()
                self.draw_graph(rnd)<N><N>
            if (not search_until_max_iter) and new_node:  # check reaching the goal
                last_index = self.search_best_goal_node()
                if last_index:
                    return self.generate_final_course(last_index)<N><N>
        print("reached max iteration")<N><N>
        last_index = self.search_best_goal_node()
        if last_index:
            return self.generate_final_course(last_index)
        else:
            print("Cannot find path")<N><N>
        return None<N><N>
    def try_goal_path(self, node):<N><N>
        goal = self.Node(self.end.x, self.end.y, self.end.yaw)<N><N>
        new_node = self.steer(node, goal)
        if new_node is None:
            return<N><N>
        if self.check_collision(new_node, self.obstacle_list):
            self.node_list.append(new_node)<N><N>
    def draw_graph(self, rnd=None):
        plt.clf()
        # for stopping simulation with the esc key.
        plt.gcf().canvas.mpl_connect('key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
        if rnd is not None:
            plt.plot(rnd.x, rnd.y, "^k")
        for node in self.node_list:
            if node.parent:
                plt.plot(node.path_x, node.path_y, "-g")<N><N>
        for (ox, oy, size) in self.obstacle_list:
            plt.plot(ox, oy, "ok", ms=30 * size)<N><N>
        plt.plot(self.start.x, self.start.y, "xr")
        plt.plot(self.end.x, self.end.y, "xr")
        plt.axis([-2, 15, -2, 15])
        plt.grid(True)
        self.plot_start_goal_arrow()
        plt.pause(0.01)<N><N>
    def plot_start_goal_arrow(self):
        reeds_shepp_path_planning.plot_arrow(
            self.start.x, self.start.y, self.start.yaw)
        reeds_shepp_path_planning.plot_arrow(
            self.end.x, self.end.y, self.end.yaw)<N><N>
    def steer(self, from_node, to_node):<N><N>
        px, py, pyaw, mode, course_lengths = reeds_shepp_path_planning.reeds_shepp_path_planning(
            from_node.x, from_node.y, from_node.yaw,
            to_node.x, to_node.y, to_node.yaw, self.curvature)<N><N>
        if not px:
            return None<N><N>
        new_node = copy.deepcopy(from_node)
        new_node.x = px[-1]
        new_node.y = py[-1]
        new_node.yaw = pyaw[-1]<N><N>
        new_node.path_x = px
        new_node.path_y = py
        new_node.path_yaw = pyaw
        new_node.cost += sum([abs(l) for l in course_lengths])
        new_node.parent = from_node<N><N>
        return new_node<N><N>
    def calc_new_cost(self, from_node, to_node):<N><N>
        _, _, _, _, course_lengths = reeds_shepp_path_planning.reeds_shepp_path_planning(
            from_node.x, from_node.y, from_node.yaw,
            to_node.x, to_node.y, to_node.yaw, self.curvature)
        if not course_lengths:
            return float("inf")<N><N>
        return from_node.cost + sum([abs(l) for l in course_lengths])<N><N>
    def get_random_node(self):<N><N>
        rnd = self.Node(random.uniform(self.min_rand, self.max_rand),
                        random.uniform(self.min_rand, self.max_rand),
                        random.uniform(-math.pi, math.pi)
                        )<N><N>
        return rnd<N><N>
    def search_best_goal_node(self):<N><N>
        goal_indexes = []
        for (i, node) in enumerate(self.node_list):
            if self.calc_dist_to_goal(node.x, node.y) <= self.goal_xy_th:
                goal_indexes.append(i)
        print("goal_indexes:", len(goal_indexes))<N><N>
        # angle check
        final_goal_indexes = []
        for i in goal_indexes:
            if abs(self.node_list[i].yaw - self.end.yaw) <= self.goal_yaw_th:
                final_goal_indexes.append(i)<N><N>
        print("final_goal_indexes:", len(final_goal_indexes))<N><N>
        if not final_goal_indexes:
            return None<N><N>
        min_cost = min([self.node_list[i].cost for i in final_goal_indexes])
        print("min_cost:", min_cost)
        for i in final_goal_indexes:
            if self.node_list[i].cost == min_cost:
                return i<N><N>
        return None<N><N>
    def generate_final_course(self, goal_index):
        path = [[self.end.x, self.end.y, self.end.yaw]]
        node = self.node_list[goal_index]
        while node.parent:
            for (ix, iy, iyaw) in zip(reversed(node.path_x), reversed(node.path_y), reversed(node.path_yaw)):
                path.append([ix, iy, iyaw])
            node = node.parent
        path.append([self.start.x, self.start.y, self.start.yaw])
        return path<N><N>

def main(max_iter=100):
    print("Start " + __file__)<N><N>
    # ====Search Path with RRT====
    obstacleList = [
        (5, 5, 1),
        (4, 6, 1),
        (4, 8, 1),
        (4, 10, 1),
        (6, 5, 1),
        (7, 5, 1),
        (8, 6, 1),
        (8, 8, 1),
        (8, 10, 1)
    ]  # [x,y,size(radius)]<N><N>
    # Set Initial parameters
    start = [0.0, 0.0, np.deg2rad(0.0)]
    goal = [6.0, 7.0, np.deg2rad(90.0)]<N><N>
    rrt_star_reeds_shepp = RRTStarReedsShepp(start, goal,
                                             obstacleList,
                                             [-2.0, 15.0], max_iter=max_iter)
    path = rrt_star_reeds_shepp.planning(animation=show_animation)<N><N>
    # Draw final path
    if path and show_animation:  # pragma: no cover
        rrt_star_reeds_shepp.draw_graph()
        plt.plot([x for (x, y, yaw) in path], [y for (x, y, yaw) in path], '-r')
        plt.grid(True)
        plt.pause(0.001)
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""
Spiral Spanning Tree Coverage Path Planner<N><N>
author: Todd Tang
paper: Spiral-STC: An On-Line Coverage Algorithm of Grid Environments
         by a Mobile Robot - Gabriely et.al.
link: https://ieeexplore.ieee.org/abstract/document/1013479
"""<N><N>
import os
import sys
import math<N><N>
import numpy as np
import matplotlib.pyplot as plt<N><N>
do_animation = True<N><N>

class SpiralSpanningTreeCoveragePlanner:
    def __init__(self, occ_map):
        self.origin_map_height = occ_map.shape[0]
        self.origin_map_width = occ_map.shape[1]<N><N>
        # original map resolution must be even
        if self.origin_map_height % 2 == 1 or self.origin_map_width % 2 == 1:
            sys.exit('original map width/height must be even \
                in grayscale .png format')<N><N>
        self.occ_map = occ_map
        self.merged_map_height = self.origin_map_height // 2
        self.merged_map_width = self.origin_map_width // 2<N><N>
        self.edge = []<N><N>
    def plan(self, start):
        """plan<N><N>
        performing Spiral Spanning Tree Coverage path planning<N><N>
        :param start: the start node of Spiral Spanning Tree Coverage
        """<N><N>
        visit_times = np.zeros(
            (self.merged_map_height, self.merged_map_width), dtype=int)
        visit_times[start[0]][start[1]] = 1<N><N>
        # generate route by
        # recusively call perform_spanning_tree_coverage() from start node
        route = []
        self.perform_spanning_tree_coverage(start, visit_times, route)<N><N>
"""<N><N>State lattice planner with model predictive trajectory generator<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
- lookuptable.csv is generated with this script: https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathPlanning/ModelPredictiveTrajectoryGenerator/lookuptable_generator.py<N><N>
Ref:<N><N>
- State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.8210&rep=rep1&type=pdf<N><N>
"""
import sys
import os
from matplotlib import pyplot as plt
import numpy as np
import math
import pandas as pd<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__))
                + "/../ModelPredictiveTrajectoryGenerator/")<N><N>

try:
    import model_predictive_trajectory_generator as planner
    import motion_model
except ImportError:
    raise<N><N>

table_path = os.path.dirname(os.path.abspath(__file__)) + "/lookuptable.csv"<N><N>
show_animation = True<N><N>

def search_nearest_one_from_lookuptable(tx, ty, tyaw, lookup_table):
    mind = float("inf")
    minid = -1<N><N>
    for (i, table) in enumerate(lookup_table):<N><N>
        dx = tx - table[0]
        dy = ty - table[1]
        dyaw = tyaw - table[2]
        d = math.sqrt(dx ** 2 + dy ** 2 + dyaw ** 2)
        if d <= mind:
            minid = i
            mind = d<N><N>
    return lookup_table[minid]<N><N>

def get_lookup_table():
    data = pd.read_csv(table_path)<N><N>
    return np.array(data)<N><N>

def generate_path(target_states, k0):
    # x, y, yaw, s, km, kf
    lookup_table = get_lookup_table()
    result = []<N><N>
    for state in target_states:
        bestp = search_nearest_one_from_lookuptable(
            state[0], state[1], state[2], lookup_table)<N><N>
        target = motion_model.State(x=state[0], y=state[1], yaw=state[2])
        init_p = np.array(
            [math.sqrt(state[0] ** 2 + state[1] ** 2), bestp[4], bestp[5]]).reshape(3, 1)<N><N>
        x, y, yaw, p = planner.optimize_trajectory(target, k0, init_p)<N><N>
        if x is not None:
            print("find good path")
            result.append(
                [x[-1], y[-1], yaw[-1], float(p[0]), float(p[1]), float(p[2])])<N><N>
    print("finish path generation")
    return result<N><N>

def calc_uniform_polar_states(nxy, nh, d, a_min, a_max, p_min, p_max):
    """
    calc uniform state<N><N>
    :param nxy: number of position sampling
    :param nh: number of heading sampleing
    :param d: distance of terminal state
    :param a_min: position sampling min angle
    :param a_max: position sampling max angle
    :param p_min: heading sampling min angle
    :param p_max: heading sampling max angle
    :return: states list
    """<N><N>
    angle_samples = [i / (nxy - 1) for i in range(nxy)]
    states = sample_states(angle_samples, a_min, a_max, d, p_max, p_min, nh)<N><N>
    return states<N><N>

def calc_biased_polar_states(goal_angle, ns, nxy, nh, d, a_min, a_max, p_min, p_max):
    """
    calc biased state<N><N>
    :param goal_angle: goal orientation for biased sampling
    :param ns: number of biased sampling
    :param nxy: number of position sampling
    :param nxy: number of position sampling
    :param nh: number of heading sampleing
    :param d: distance of terminal state
    :param a_min: position sampling min angle
    :param a_max: position sampling max angle
    :param p_min: heading sampling min angle
    :param p_max: heading sampling max angle
    :return: states list
    """<N><N>
    asi = [a_min + (a_max - a_min) * i / (ns - 1) for i in range(ns - 1)]
    cnav = [math.pi - abs(i - goal_angle) for i in asi]<N><N>
    cnav_sum = sum(cnav)
    cnav_max = max(cnav)<N><N>
    # normalize
    cnav = [(cnav_max - cnav[i]) / (cnav_max * ns - cnav_sum)
            for i in range(ns - 1)]<N><N>
    csumnav = np.cumsum(cnav)
    di = []
    li = 0
    for i in range(nxy):
        for ii in range(li, ns - 1):
            if ii / ns >= i / (nxy - 1):
                di.append(csumnav[ii])
                li = ii - 1
                break<N><N>
    states = sample_states(di, a_min, a_max, d, p_max, p_min, nh)<N><N>
    return states<N><N>

def calc_lane_states(l_center, l_heading, l_width, v_width, d, nxy):
    """<N><N>
    calc lane states<N><N>
    :param l_center: lane lateral position
    :param l_heading:  lane heading
    :param l_width:  lane width
    :param v_width: vehicle width
    :param d: longitudinal position
    :param nxy: sampling number
    :return: state list
    """
    xc = d
    yc = l_center<N><N>
    states = []
    for i in range(nxy):
        delta = -0.5 * (l_width - v_width) + \
            (l_width - v_width) * i / (nxy - 1)
        xf = xc - delta * math.sin(l_heading)
        yf = yc + delta * math.cos(l_heading)
        yawf = l_heading
        states.append([xf, yf, yawf])<N><N>
    return states<N><N>

def sample_states(angle_samples, a_min, a_max, d, p_max, p_min, nh):
    states = []
    for i in angle_samples:
        a = a_min + (a_max - a_min) * i<N><N>
        for j in range(nh):
            xf = d * math.cos(a)
            yf = d * math.sin(a)
            if nh == 1:
                yawf = (p_max - p_min) / 2 + a
            else:
                yawf = p_min + (p_max - p_min) * j / (nh - 1) + a
            states.append([xf, yf, yawf])<N><N>
    return states<N><N>

def uniform_terminal_state_sampling_test1():
    k0 = 0.0
    nxy = 5
    nh = 3
    d = 20
    a_min = - np.deg2rad(45.0)
    a_max = np.deg2rad(45.0)
    p_min = - np.deg2rad(45.0)
    p_max = np.deg2rad(45.0)
    states = calc_uniform_polar_states(nxy, nh, d, a_min, a_max, p_min, p_max)
    result = generate_path(states, k0)<N><N>
    for table in result:
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], table[4], table[5], k0)<N><N>
        if show_animation:
            plt.plot(xc, yc, "-r")<N><N>
    if show_animation:
        plt.grid(True)
        plt.axis("equal")
        plt.show()<N><N>
    print("Done")<N><N>

def uniform_terminal_state_sampling_test2():
    k0 = 0.1
    nxy = 6
    nh = 3
    d = 20
    a_min = - np.deg2rad(-10.0)
    a_max = np.deg2rad(45.0)
    p_min = - np.deg2rad(20.0)
    p_max = np.deg2rad(20.0)
    states = calc_uniform_polar_states(nxy, nh, d, a_min, a_max, p_min, p_max)
    result = generate_path(states, k0)<N><N>
    for table in result:
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], table[4], table[5], k0)<N><N>
        if show_animation:
            plt.plot(xc, yc, "-r")<N><N>
    if show_animation:
        plt.grid(True)
        plt.axis("equal")
        plt.show()<N><N>
    print("Done")<N><N>

def biased_terminal_state_sampling_test1():
    k0 = 0.0
    nxy = 30
    nh = 2
    d = 20
    a_min = np.deg2rad(-45.0)
    a_max = np.deg2rad(45.0)
    p_min = - np.deg2rad(20.0)
    p_max = np.deg2rad(20.0)
    ns = 100
    goal_angle = np.deg2rad(0.0)
    states = calc_biased_polar_states(
        goal_angle, ns, nxy, nh, d, a_min, a_max, p_min, p_max)
    result = generate_path(states, k0)<N><N>
    for table in result:
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], table[4], table[5], k0)
        if show_animation:
            plt.plot(xc, yc, "-r")<N><N>
    if show_animation:
        plt.grid(True)
        plt.axis("equal")
        plt.show()<N><N>

def biased_terminal_state_sampling_test2():
    k0 = 0.0
    nxy = 30
    nh = 1
    d = 20
    a_min = np.deg2rad(0.0)
    a_max = np.deg2rad(45.0)
    p_min = - np.deg2rad(20.0)
    p_max = np.deg2rad(20.0)
    ns = 100
    goal_angle = np.deg2rad(30.0)
    states = calc_biased_polar_states(
        goal_angle, ns, nxy, nh, d, a_min, a_max, p_min, p_max)
    result = generate_path(states, k0)<N><N>
    for table in result:
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], table[4], table[5], k0)<N><N>
        if show_animation:
            plt.plot(xc, yc, "-r")<N><N>
    if show_animation:
        plt.grid(True)
        plt.axis("equal")
        plt.show()<N><N>

def lane_state_sampling_test1():
    k0 = 0.0<N><N>
    l_center = 10.0
    l_heading = np.deg2rad(0.0)
    l_width = 3.0
    v_width = 1.0
    d = 10
    nxy = 5
    states = calc_lane_states(l_center, l_heading, l_width, v_width, d, nxy)
    result = generate_path(states, k0)<N><N>
    if show_animation:
        plt.close("all")<N><N>
    for table in result:
        xc, yc, yawc = motion_model.generate_trajectory(
            table[3], table[4], table[5], k0)<N><N>
        if show_animation:
            plt.plot(xc, yc, "-r")<N><N>
    if show_animation:
        plt.grid(True)
        plt.axis("equal")
        plt.show()<N><N>

def main():
    planner.show_animation = show_animation
    uniform_terminal_state_sampling_test1()
    uniform_terminal_state_sampling_test2()
    biased_terminal_state_sampling_test1()
    biased_terminal_state_sampling_test2()
    lane_state_sampling_test1()<N><N>

if __name__ == '__main__':
    main()
<N><N>
class Geometry:
    class Point:
        def __init__(self, x, y):
            self.x = x
            self.y = y<N><N>
    @staticmethod
    def is_seg_intersect(p1, q1, p2, q2):<N><N>
        def on_segment(p, q, r):
            if ((q.x <= max(p.x, r.x)) and (q.x >= min(p.x, r.x)) and
                    (q.y <= max(p.y, r.y)) and (q.y >= min(p.y, r.y))):
                return True
            return False<N><N>
        def orientation(p, q, r):
            val = (float(q.y - p.y) * (r.x - q.x)) - (
                    float(q.x - p.x) * (r.y - q.y))
            if val > 0:
                return 1
            if val < 0:
                return 2
            return 0<N><N>
        # Find the 4 orientations required for
        # the general and special cases
        o1 = orientation(p1, q1, p2)
        o2 = orientation(p1, q1, q2)
        o3 = orientation(p2, q2, p1)
        o4 = orientation(p2, q2, q1)<N><N>
        if (o1 != o2) and (o3 != o4):
            return True
        if (o1 == 0) and on_segment(p1, p2, q1):
            return True
        if (o2 == 0) and on_segment(p1, q2, q1):
            return True
        if (o3 == 0) and on_segment(p2, p1, q2):
            return True
        if (o4 == 0) and on_segment(p2, q1, q2):
            return True<N><N>
        return False<N><N>
"""<N><N>Visibility Road Map Planner<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import os
import sys
import math
import numpy as np
import matplotlib.pyplot as plt<N><N>
from PathPlanning.VisibilityRoadMap.geometry import Geometry<N><N>
sys.path.append(os.path.dirname(os.path.abspath(__file__)) +
                "/../VoronoiRoadMap/")
from dijkstra_search import DijkstraSearch<N><N>
show_animation = True<N><N>

class VisibilityRoadMap:<N><N>
    def __init__(self, robot_radius, do_plot=False):
        self.robot_radius = robot_radius
        self.do_plot = do_plot<N><N>
    def planning(self, start_x, start_y, goal_x, goal_y, obstacles):<N><N>
        nodes = self.generate_graph_node(start_x, start_y, goal_x, goal_y,
                                         obstacles)<N><N>
        road_map_info = self.generate_road_map_info(nodes, obstacles)<N><N>
        if self.do_plot:
            self.plot_road_map(nodes, road_map_info)
            plt.pause(1.0)<N><N>
        rx, ry = DijkstraSearch(show_animation).search(
            start_x, start_y,
            goal_x, goal_y,
            [node.x for node in nodes],
            [node.y for node in nodes],
            road_map_info
        )<N><N>
        return rx, ry<N><N>
    def generate_graph_node(self, start_x, start_y, goal_x, goal_y, obstacles):<N><N>
        # add start and goal as nodes
        nodes = [DijkstraSearch.Node(start_x, start_y),
                 DijkstraSearch.Node(goal_x, goal_y, 0, None)]<N><N>
        # add vertexes in configuration space as nodes
        for obstacle in obstacles:<N><N>
            cvx_list, cvy_list = self.calc_vertexes_in_configuration_space(
                obstacle.x_list, obstacle.y_list)<N><N>
            for (vx, vy) in zip(cvx_list, cvy_list):
                nodes.append(DijkstraSearch.Node(vx, vy))<N><N>
        for node in nodes:
            plt.plot(node.x, node.y, "xr")<N><N>
        return nodes<N><N>
    def calc_vertexes_in_configuration_space(self, x_list, y_list):
        x_list = x_list[0:-1]
        y_list = y_list[0:-1]
        cvx_list, cvy_list = [], []<N><N>
        n_data = len(x_list)<N><N>
        for index in range(n_data):
            offset_x, offset_y = self.calc_offset_xy(
                x_list[index - 1], y_list[index - 1],
                x_list[index], y_list[index],
                x_list[(index + 1) % n_data], y_list[(index + 1) % n_data],
            )
            cvx_list.append(offset_x)
            cvy_list.append(offset_y)<N><N>
        return cvx_list, cvy_list<N><N>
    def generate_road_map_info(self, nodes, obstacles):<N><N>
        road_map_info_list = []<N><N>
        for target_node in nodes:
            road_map_info = []
            for node_id, node in enumerate(nodes):
                if np.hypot(target_node.x - node.x,
                            target_node.y - node.y) <= 0.1:
                    continue<N><N>
                is_valid = True
                for obstacle in obstacles:
                    if not self.is_edge_valid(target_node, node, obstacle):
                        is_valid = False
                        break
                if is_valid:
                    road_map_info.append(node_id)<N><N>
            road_map_info_list.append(road_map_info)<N><N>
        return road_map_info_list<N><N>
    @staticmethod
    def is_edge_valid(target_node, node, obstacle):<N><N>
        for i in range(len(obstacle.x_list) - 1):
            p1 = Geometry.Point(target_node.x, target_node.y)
            p2 = Geometry.Point(node.x, node.y)
            p3 = Geometry.Point(obstacle.x_list[i], obstacle.y_list[i])
            p4 = Geometry.Point(obstacle.x_list[i + 1], obstacle.y_list[i + 1])<N><N>
            if Geometry.is_seg_intersect(p1, p2, p3, p4):
                return False<N><N>
        return True<N><N>
    def calc_offset_xy(self, px, py, x, y, nx, ny):
        p_vec = math.atan2(y - py, x - px)
        n_vec = math.atan2(ny - y, nx - x)
        offset_vec = math.atan2(math.sin(p_vec) + math.sin(n_vec),
                                math.cos(p_vec) + math.cos(
                                    n_vec)) + math.pi / 2.0
        offset_x = x + self.robot_radius * math.cos(offset_vec)
        offset_y = y + self.robot_radius * math.sin(offset_vec)
        return offset_x, offset_y<N><N>
    @staticmethod
    def plot_road_map(nodes, road_map_info_list):
        for i, node in enumerate(nodes):
            for index in road_map_info_list[i]:
                plt.plot([node.x, nodes[index].x],
                         [node.y, nodes[index].y], "-b")<N><N>

class ObstaclePolygon:<N><N>
    def __init__(self, x_list, y_list):
        self.x_list = x_list
        self.y_list = y_list<N><N>
        self.close_polygon()
        self.make_clockwise()<N><N>
    def make_clockwise(self):
        if not self.is_clockwise():
            self.x_list = list(reversed(self.x_list))
            self.y_list = list(reversed(self.y_list))<N><N>
    def is_clockwise(self):
        n_data = len(self.x_list)
        eval_sum = sum([(self.x_list[i + 1] - self.x_list[i]) *
                        (self.y_list[i + 1] + self.y_list[i])
                        for i in range(n_data - 1)])
        eval_sum += (self.x_list[0] - self.x_list[n_data - 1]) * \
                    (self.y_list[0] + self.y_list[n_data - 1])
        return eval_sum >= 0<N><N>
    def close_polygon(self):
        is_x_same = self.x_list[0] == self.x_list[-1]
        is_y_same = self.y_list[0] == self.y_list[-1]
        if is_x_same and is_y_same:
            return  # no need to close<N><N>
        self.x_list.append(self.x_list[0])
        self.y_list.append(self.y_list[0])<N><N>
    def plot(self):
        plt.plot(self.x_list, self.y_list, "-k")<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    # start and goal position
    sx, sy = 10.0, 10.0  # [m]
    gx, gy = 50.0, 50.0  # [m]<N><N>
    robot_radius = 5.0  # [m]<N><N>
    obstacles = [
        ObstaclePolygon(
            [20.0, 30.0, 15.0],
            [20.0, 20.0, 30.0],
        ),
        ObstaclePolygon(
            [40.0, 45.0, 50.0, 40.0],
            [50.0, 40.0, 20.0, 40.0],
        ),
        ObstaclePolygon(
            [20.0, 30.0, 30.0, 20.0],
            [40.0, 45.0, 60.0, 50.0],
        )
    ]<N><N>
    if show_animation:  # pragma: no cover
        plt.plot(sx, sy, "or")
        plt.plot(gx, gy, "ob")
        for ob in obstacles:
            ob.plot()
        plt.axis("equal")
        plt.pause(1.0)<N><N>
    rx, ry = VisibilityRoadMap(robot_radius, do_plot=show_animation).planning(
        sx, sy, gx, gy, obstacles)<N><N>
    if show_animation:  # pragma: no cover
        plt.plot(rx, ry, "-r")
        plt.pause(0.1)
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Dijkstra Search library<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import matplotlib.pyplot as plt
import math
import numpy as np<N><N>

class DijkstraSearch:
    class Node:
        """
        Node class for dijkstra search
        """<N><N>
        def __init__(self, x, y, cost=None, parent=None, edge_ids=None):
            self.x = x
            self.y = y
            self.cost = cost
            self.parent = parent
            self.edge_ids = edge_ids<N><N>
        def __str__(self):
            return str(self.x) + "," + str(self.y) + "," + str(
                self.cost) + "," + str(self.parent)<N><N>
    def __init__(self, show_animation):
        self.show_animation = show_animation<N><N>
    def search(self, sx, sy, gx, gy, node_x, node_y, edge_ids_list):
        """
        Search shortest path<N><N>
        s_x: start x positions [m]
        s_y: start y positions [m]
        gx: goal x position [m]
        gx: goal x position [m]
        node_x: node x position
        node_y: node y position
        edge_ids_list: edge_list each item includes a list of edge ids
        """<N><N>
        start_node = self.Node(sx, sy, 0.0, -1)
        goal_node = self.Node(gx, gy, 0.0, -1)
        current_node = None<N><N>
        open_set, close_set = dict(), dict()
        open_set[self.find_id(node_x, node_y, start_node)] = start_node<N><N>
        while True:
            if self.has_node_in_set(close_set, goal_node):
                print("goal is found!")
                goal_node.parent = current_node.parent
                goal_node.cost = current_node.cost
                break
            elif not open_set:
                print("Cannot find path")
                break<N><N>
            current_id = min(open_set, key=lambda o: open_set[o].cost)
            current_node = open_set[current_id]<N><N>
            # show graph
            if self.show_animation and len(
                    close_set.keys()) % 2 == 0:  # pragma: no cover
                plt.plot(current_node.x, current_node.y, "xg")
                # for stopping simulation with the esc key.
                plt.gcf().canvas.mpl_connect(
                    'key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])
                plt.pause(0.1)<N><N>
            # Remove the item from the open set
            del open_set[current_id]
            # Add it to the closed set
            close_set[current_id] = current_node<N><N>
            # expand search grid based on motion model
            for i in range(len(edge_ids_list[current_id])):
                n_id = edge_ids_list[current_id][i]
                dx = node_x[n_id] - current_node.x
                dy = node_y[n_id] - current_node.y
                d = math.hypot(dx, dy)
                node = self.Node(node_x[n_id], node_y[n_id],
                                 current_node.cost + d, current_id)<N><N>
                if n_id in close_set:
                    continue
                # Otherwise if it is already in the open set
                if n_id in open_set:
                    if open_set[n_id].cost > node.cost:
                        open_set[n_id] = node
                else:
                    open_set[n_id] = node<N><N>
        # generate final course
        rx, ry = self.generate_final_path(close_set, goal_node)<N><N>
        return rx, ry<N><N>
    @staticmethod
    def generate_final_path(close_set, goal_node):
        rx, ry = [goal_node.x], [goal_node.y]
        parent = goal_node.parent
        while parent != -1:
            n = close_set[parent]
            rx.append(n.x)
            ry.append(n.y)
            parent = n.parent
        rx, ry = rx[::-1], ry[::-1]  # reverse it
        return rx, ry<N><N>
    def has_node_in_set(self, target_set, node):
        for key in target_set:
            if self.is_same_node(target_set[key], node):
                return True
        return False<N><N>
    def find_id(self, node_x_list, node_y_list, target_node):
        for i, _ in enumerate(node_x_list):
            if self.is_same_node_with_xy(node_x_list[i], node_y_list[i],
                                         target_node):
                return i
        return None<N><N>
    @staticmethod
    def is_same_node_with_xy(node_x, node_y, node_b):
        dist = np.hypot(node_x - node_b.x,
                        node_y - node_b.y)
        return dist <= 0.1<N><N>
    @staticmethod
    def is_same_node(node_a, node_b):
        dist = np.hypot(node_a.x - node_b.x,
                        node_b.y - node_b.y)
        return dist <= 0.1
<N><N>
"""<N><N>Voronoi Road Map Planner<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math
import numpy as np
import matplotlib.pyplot as plt
from dijkstra_search import DijkstraSearch
from scipy.spatial import cKDTree, Voronoi<N><N>
show_animation = True<N><N>

class VoronoiRoadMapPlanner:<N><N>
    def __init__(self):
        # parameter
        self.N_KNN = 10  # number of edge from one sampled point
        self.MAX_EDGE_LEN = 30.0  # [m] Maximum edge length<N><N>
    def planning(self, sx, sy, gx, gy, ox, oy, robot_radius):
        obstacle_tree = cKDTree(np.vstack((ox, oy)).T)<N><N>
        sample_x, sample_y = self.voronoi_sampling(sx, sy, gx, gy, ox, oy)
        if show_animation:  # pragma: no cover
            plt.plot(sample_x, sample_y, ".b")<N><N>
        road_map_info = self.generate_road_map_info(
            sample_x, sample_y, robot_radius, obstacle_tree)<N><N>
        rx, ry = DijkstraSearch(show_animation).search(sx, sy, gx, gy,
                                                       sample_x, sample_y,
                                                       road_map_info)
        return rx, ry<N><N>
    def is_collision(self, sx, sy, gx, gy, rr, obstacle_kd_tree):
        x = sx
        y = sy
        dx = gx - sx
        dy = gy - sy
        yaw = math.atan2(gy - sy, gx - sx)
        d = math.hypot(dx, dy)<N><N>
        if d >= self.MAX_EDGE_LEN:
            return True<N><N>
        D = rr
        n_step = round(d / D)<N><N>
        for i in range(n_step):
            dist, _ = obstacle_kd_tree.query([x, y])
            if dist <= rr:
                return True  # collision
            x += D * math.cos(yaw)
            y += D * math.sin(yaw)<N><N>
        # goal point check
        dist, _ = obstacle_kd_tree.query([gx, gy])
        if dist <= rr:
            return True  # collision<N><N>
        return False  # OK<N><N>
    def generate_road_map_info(self, node_x, node_y, rr, obstacle_tree):
        """
        Road map generation<N><N>
        node_x: [m] x positions of sampled points
        node_y: [m] y positions of sampled points
        rr: Robot Radius[m]
        obstacle_tree: KDTree object of obstacles
        """<N><N>
        road_map = []
        n_sample = len(node_x)
        node_tree = cKDTree(np.vstack((node_x, node_y)).T)<N><N>
        for (i, ix, iy) in zip(range(n_sample), node_x, node_y):<N><N>
            dists, indexes = node_tree.query([ix, iy], k=n_sample)<N><N>
            edge_id = []<N><N>
            for ii in range(1, len(indexes)):
                nx = node_x[indexes[ii]]
                ny = node_y[indexes[ii]]<N><N>
                if not self.is_collision(ix, iy, nx, ny, rr, obstacle_tree):
                    edge_id.append(indexes[ii])<N><N>
                if len(edge_id) >= self.N_KNN:
                    break<N><N>
            road_map.append(edge_id)<N><N>
        #  plot_road_map(road_map, sample_x, sample_y)<N><N>
        return road_map<N><N>
    @staticmethod
    def plot_road_map(road_map, sample_x, sample_y):  # pragma: no cover<N><N>
        for i, _ in enumerate(road_map):
            for ii in range(len(road_map[i])):
                ind = road_map[i][ii]<N><N>
                plt.plot([sample_x[i], sample_x[ind]],
                         [sample_y[i], sample_y[ind]], "-k")<N><N>
    @staticmethod
    def voronoi_sampling(sx, sy, gx, gy, ox, oy):
        oxy = np.vstack((ox, oy)).T<N><N>
        # generate voronoi point
        vor = Voronoi(oxy)
        sample_x = [ix for [ix, _] in vor.vertices]
        sample_y = [iy for [_, iy] in vor.vertices]<N><N>
        sample_x.append(sx)
        sample_y.append(sy)
        sample_x.append(gx)
        sample_y.append(gy)<N><N>
        return sample_x, sample_y<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    # start and goal position
    sx = 10.0  # [m]
    sy = 10.0  # [m]
    gx = 50.0  # [m]
    gy = 50.0  # [m]
    robot_size = 5.0  # [m]<N><N>
    ox = []
    oy = []<N><N>
    for i in range(60):
        ox.append(i)
        oy.append(0.0)
    for i in range(60):
        ox.append(60.0)
        oy.append(i)
    for i in range(61):
        ox.append(i)
        oy.append(60.0)
    for i in range(61):
        ox.append(0.0)
        oy.append(i)
    for i in range(40):
        ox.append(20.0)
        oy.append(i)
    for i in range(40):
        ox.append(40.0)
        oy.append(60.0 - i)<N><N>
    if show_animation:  # pragma: no cover
        plt.plot(ox, oy, ".k")
        plt.plot(sx, sy, "^r")
        plt.plot(gx, gy, "^c")
        plt.grid(True)
        plt.axis("equal")<N><N>
    rx, ry = VoronoiRoadMapPlanner().planning(sx, sy, gx, gy, ox, oy,
                                              robot_size)<N><N>
    assert rx, 'Cannot found path'<N><N>
    if show_animation:  # pragma: no cover
        plt.plot(rx, ry, "-r")
        plt.pause(0.1)
        plt.show()<N><N>

if __name__ == '__main__':
    main()
<N><N>
import os
import sys<N><N>sys.path.append(os.path.dirname(os.path.abspath(__file__)))

"""
Distance/Path Transform Wavefront Coverage Path Planner<N><N>
author: Todd Tang
paper: Planning paths of complete coverage of an unstructured environment
         by a mobile robot - Zelinsky et.al.
link: http://pinkwink.kr/attachment/cfile3.uf@1354654A4E8945BD13FE77.pdf
"""<N><N>
import os
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage<N><N>
do_animation = True<N><N>

def transform(
        grid_map, src, distance_type='chessboard',
        transform_type='path', alpha=0.01
):
    """transform<N><N>
    calculating transform of transform_type from src
    in given distance_type<N><N>
    :param grid_map: 2d binary map
    :param src: distance transform source
    :param distance_type: type of distance used
    :param transform_type: type of transform used
    :param alpha: weight of Obstacle Transform used when using path_transform
    """<N><N>
    n_rows, n_cols = grid_map.shape<N><N>
    if n_rows == 0 or n_cols == 0:
        sys.exit('Empty grid_map.')<N><N>
    inc_order = [[0, 1], [1, 1], [1, 0], [1, -1],
                 [0, -1], [-1, -1], [-1, 0], [-1, 1]]
    if distance_type == 'chessboard':
        cost = [1, 1, 1, 1, 1, 1, 1, 1]
    elif distance_type == 'eculidean':
        cost = [1, np.sqrt(2), 1, np.sqrt(2), 1, np.sqrt(2), 1, np.sqrt(2)]
    else:
        sys.exit('Unsupported distance type.')<N><N>
    transform_matrix = float('inf') * np.ones_like(grid_map, dtype=float)
    transform_matrix[src[0], src[1]] = 0
    if transform_type == 'distance':
        eT = np.zeros_like(grid_map)
    elif transform_type == 'path':
        eT = ndimage.distance_transform_cdt(1 - grid_map, distance_type)
    else:
        sys.exit('Unsupported transform type.')<N><N>
    # set obstacle transform_matrix value to infinity
    for i in range(n_rows):
        for j in range(n_cols):
            if grid_map[i][j] == 1.0:
                transform_matrix[i][j] = float('inf')
    is_visited = np.zeros_like(transform_matrix, dtype=bool)
    is_visited[src[0], src[1]] = True
    traversal_queue = [src]
    calculated = [(src[0] - 1) * n_cols + src[1]]<N><N>
    def is_valid_neighbor(g_i, g_j):
        return 0 <= g_i < n_rows and 0 <= g_j < n_cols \
               and not grid_map[g_i][g_j]<N><N>
    while traversal_queue:
        i, j = traversal_queue.pop(0)
        for k, inc in enumerate(inc_order):
            ni = i + inc[0]
            nj = j + inc[1]
            if is_valid_neighbor(ni, nj):
                is_visited[i][j] = True<N><N>
                # update transform_matrix
                transform_matrix[i][j] = min(
                    transform_matrix[i][j],
                    transform_matrix[ni][nj] + cost[k] + alpha * eT[ni][nj])<N><N>
                if not is_visited[ni][nj] \
                        and ((ni - 1) * n_cols + nj) not in calculated:
                    traversal_queue.append((ni, nj))
                    calculated.append((ni - 1) * n_cols + nj)<N><N>
    return transform_matrix<N><N>
"""<N><N>Nonlinear MPC simulation with CGMRES<N><N>
author Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:
Shunichi09/nonlinear_control: Implementing the nonlinear model predictive
control, sliding mode control https://github.com/Shunichi09/nonlinear_control<N><N>
"""<N><N>from math import cos, sin, radians, atan2<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
U_A_MAX = 1.0
U_OMEGA_MAX = radians(45.0)
PHI_V = 0.01
PHI_OMEGA = 0.01
WB = 0.25  # [m] wheel base<N><N>
show_animation = True<N><N>

def differential_model(v, yaw, u_1, u_2):
    dx = cos(yaw) * v
    dy = sin(yaw) * v
    dv = u_1
    # tangent is not good for nonlinear optimization
    d_yaw = v / WB * sin(u_2)<N><N>
    return dx, dy, d_yaw, dv<N><N>

class TwoWheeledSystem:<N><N>
    def __init__(self, init_x, init_y, init_yaw, init_v):
        self.x = init_x
        self.y = init_y
        self.yaw = init_yaw
        self.v = init_v
        self.history_x = [init_x]
        self.history_y = [init_y]
        self.history_yaw = [init_yaw]
        self.history_v = [init_v]<N><N>
    def update_state(self, u_1, u_2, dt=0.01):
        dx, dy, d_yaw, dv = differential_model(self.v, self.yaw, u_1, u_2)<N><N>
        self.x += dt * dx
        self.y += dt * dy
        self.yaw += dt * d_yaw
        self.v += dt * dv<N><N>
        # save
        self.history_x.append(self.x)
        self.history_y.append(self.y)
        self.history_yaw.append(self.yaw)
        self.history_v.append(self.v)<N><N>

class NMPCSimulatorSystem:<N><N>
    def calc_predict_and_adjoint_state(self, x, y, yaw, v, u_1s, u_2s, N, dt):
        # by using state equation
        x_s, y_s, yaw_s, v_s = self._calc_predict_states(
            x, y, yaw, v, u_1s, u_2s, N, dt)
        # by using adjoint equation
        lam_1s, lam_2s, lam_3s, lam_4s = self._calc_adjoint_states(
            x_s, y_s, yaw_s, v_s, u_2s, N, dt)<N><N>
        return x_s, y_s, yaw_s, v_s, lam_1s, lam_2s, lam_3s, lam_4s<N><N>
    def _calc_predict_states(self, x, y, yaw, v, u_1s, u_2s, N, dt):
        x_s = [x]
        y_s = [y]
        yaw_s = [yaw]
        v_s = [v]<N><N>
        for i in range(N):
            temp_x_1, temp_x_2, temp_x_3, temp_x_4 = self._predict_state_with_oylar(
                x_s[i], y_s[i], yaw_s[i], v_s[i], u_1s[i], u_2s[i], dt)
            x_s.append(temp_x_1)
            y_s.append(temp_x_2)
            yaw_s.append(temp_x_3)
            v_s.append(temp_x_4)<N><N>
        return x_s, y_s, yaw_s, v_s<N><N>
    def _calc_adjoint_states(self, x_s, y_s, yaw_s, v_s, u_2s, N, dt):
        lam_1s = [x_s[-1]]
        lam_2s = [y_s[-1]]
        lam_3s = [yaw_s[-1]]
        lam_4s = [v_s[-1]]<N><N>
        # backward adjoint state calc
        for i in range(N - 1, 0, -1):
            temp_lam_1, temp_lam_2, temp_lam_3, temp_lam_4 = self._adjoint_state_with_oylar(
                yaw_s[i], v_s[i], lam_1s[0], lam_2s[0], lam_3s[0], lam_4s[0],
                u_2s[i], dt)
            lam_1s.insert(0, temp_lam_1)
            lam_2s.insert(0, temp_lam_2)
            lam_3s.insert(0, temp_lam_3)
            lam_4s.insert(0, temp_lam_4)<N><N>
        return lam_1s, lam_2s, lam_3s, lam_4s<N><N>
    @staticmethod
    def _predict_state_with_oylar(x, y, yaw, v, u_1, u_2, dt):<N><N>
        dx, dy, dyaw, dv = differential_model(
            v, yaw, u_1, u_2)<N><N>
        next_x_1 = x + dt * dx
        next_x_2 = y + dt * dy
        next_x_3 = yaw + dt * dyaw
        next_x_4 = v + dt * dv<N><N>
        return next_x_1, next_x_2, next_x_3, next_x_4<N><N>
    @staticmethod
    def _adjoint_state_with_oylar(yaw, v, lam_1, lam_2, lam_3, lam_4, u_2, dt):<N><N>
        # ∂H/∂x
        pre_lam_1 = lam_1 + dt * 0.0
        pre_lam_2 = lam_2 + dt * 0.0
        tmp1 = - lam_1 * sin(yaw) * v + lam_2 * cos(yaw) * v
        pre_lam_3 = lam_3 + dt * tmp1
        tmp2 = lam_1 * cos(yaw) + lam_2 * sin(yaw) + lam_3 * sin(u_2) / WB
        pre_lam_4 = lam_4 + dt * tmp2<N><N>
        return pre_lam_1, pre_lam_2, pre_lam_3, pre_lam_4<N><N>
"""<N><N>Path tracking simulation with LQR speed and steering control<N><N>
author Atsushi Sakai (@Atsushi_twi)<N><N>
"""
import math
import sys<N><N>
import matplotlib.pyplot as plt
import numpy as np
import scipy.linalg as la<N><N>
sys.path.append("../../PathPlanning/CubicSpline/")<N><N>
try:
    import cubic_spline_planner
except ImportError:
    raise<N><N>
# === Parameters =====<N><N>
# LQR parameter
lqr_Q = np.eye(5)
lqr_R = np.eye(2)
dt = 0.1  # time tick[s]
L = 0.5  # Wheel base of the vehicle [m]
max_steer = np.deg2rad(45.0)  # maximum steering angle[rad]<N><N>
show_animation = True<N><N>

class State:<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v<N><N>

def update(state, a, delta):<N><N>
    if delta >= max_steer:
        delta = max_steer
    if delta <= - max_steer:
        delta = - max_steer<N><N>
    state.x = state.x + state.v * math.cos(state.yaw) * dt
    state.y = state.y + state.v * math.sin(state.yaw) * dt
    state.yaw = state.yaw + state.v / L * math.tan(delta) * dt
    state.v = state.v + a * dt<N><N>
    return state<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def solve_dare(A, B, Q, R):
    """
    solve a discrete time_Algebraic Riccati equation (DARE)
    """
    x = Q
    x_next = Q
    max_iter = 150
    eps = 0.01<N><N>
    for i in range(max_iter):
        x_next = A.T @ x @ A - A.T @ x @ B @ \
                 la.inv(R + B.T @ x @ B) @ B.T @ x @ A + Q
        if (abs(x_next - x)).max() < eps:
            break
        x = x_next<N><N>
    return x_next<N><N>

def dlqr(A, B, Q, R):
    """Solve the discrete time lqr controller.
    x[k+1] = A x[k] + B u[k]
    cost = sum x[k].T*Q*x[k] + u[k].T*R*u[k]
    # ref Bertsekas, p.151
    """<N><N>
    # first, try to solve the ricatti equation
    X = solve_dare(A, B, Q, R)<N><N>
    # compute the LQR gain
    K = la.inv(B.T @ X @ B + R) @ (B.T @ X @ A)<N><N>
    eig_result = la.eig(A - B @ K)<N><N>
    return K, X, eig_result[0]<N><N>

def lqr_speed_steering_control(state, cx, cy, cyaw, ck, pe, pth_e, sp, Q, R):
    ind, e = calc_nearest_index(state, cx, cy, cyaw)<N><N>
    tv = sp[ind]<N><N>
    k = ck[ind]
    v = state.v
    th_e = pi_2_pi(state.yaw - cyaw[ind])<N><N>
    # A = [1.0, dt, 0.0, 0.0, 0.0
    #      0.0, 0.0, v, 0.0, 0.0]
    #      0.0, 0.0, 1.0, dt, 0.0]
    #      0.0, 0.0, 0.0, 0.0, 0.0]
    #      0.0, 0.0, 0.0, 0.0, 1.0]
    A = np.zeros((5, 5))
    A[0, 0] = 1.0
    A[0, 1] = dt
    A[1, 2] = v
    A[2, 2] = 1.0
    A[2, 3] = dt
    A[4, 4] = 1.0<N><N>
    # B = [0.0, 0.0
    #     0.0, 0.0
    #     0.0, 0.0
    #     v/L, 0.0
    #     0.0, dt]
    B = np.zeros((5, 2))
    B[3, 0] = v / L
    B[4, 1] = dt<N><N>
    K, _, _ = dlqr(A, B, Q, R)<N><N>
    # state vector
    # x = [e, dot_e, th_e, dot_th_e, delta_v]
    # e: lateral distance to the path
    # dot_e: derivative of e
    # th_e: angle difference to the path
    # dot_th_e: derivative of th_e
    # delta_v: difference between current speed and target speed
    x = np.zeros((5, 1))
    x[0, 0] = e
    x[1, 0] = (e - pe) / dt
    x[2, 0] = th_e
    x[3, 0] = (th_e - pth_e) / dt
    x[4, 0] = v - tv<N><N>
    # input vector
    # u = [delta, accel]
    # delta: steering angle
    # accel: acceleration
    ustar = -K @ x<N><N>
    # calc steering input
    ff = math.atan2(L * k, 1)  # feedforward steering angle
    fb = pi_2_pi(ustar[0, 0])  # feedback steering angle
    delta = ff + fb<N><N>
    # calc accel input
    accel = ustar[1, 0]<N><N>
    return delta, ind, e, th_e, accel<N><N>

def calc_nearest_index(state, cx, cy, cyaw):
    dx = [state.x - icx for icx in cx]
    dy = [state.y - icy for icy in cy]<N><N>
    d = [idx ** 2 + idy ** 2 for (idx, idy) in zip(dx, dy)]<N><N>
    mind = min(d)<N><N>
    ind = d.index(mind)<N><N>
    mind = math.sqrt(mind)<N><N>
    dxl = cx[ind] - state.x
    dyl = cy[ind] - state.y<N><N>
    angle = pi_2_pi(cyaw[ind] - math.atan2(dyl, dxl))
    if angle < 0:
        mind *= -1<N><N>
    return ind, mind<N><N>

def do_simulation(cx, cy, cyaw, ck, speed_profile, goal):
    T = 500.0  # max simulation time
    goal_dis = 0.3
    stop_speed = 0.05<N><N>
    state = State(x=-0.0, y=-0.0, yaw=0.0, v=0.0)<N><N>
    time = 0.0
    x = [state.x]
    y = [state.y]
    yaw = [state.yaw]
    v = [state.v]
    t = [0.0]<N><N>
    e, e_th = 0.0, 0.0<N><N>
    while T >= time:
        dl, target_ind, e, e_th, ai = lqr_speed_steering_control(
            state, cx, cy, cyaw, ck, e, e_th, speed_profile, lqr_Q, lqr_R)<N><N>
        state = update(state, ai, dl)<N><N>
        if abs(state.v) <= stop_speed:
            target_ind += 1<N><N>
        time = time + dt<N><N>
        # check goal
        dx = state.x - goal[0]
        dy = state.y - goal[1]
        if math.hypot(dx, dy) <= goal_dis:
            print("Goal")
            break<N><N>
        x.append(state.x)
        y.append(state.y)
        yaw.append(state.yaw)
        v.append(state.v)
        t.append(time)<N><N>
"""<N><N>Path tracking simulation with LQR steering control and PID speed control.<N><N>
author Atsushi Sakai (@Atsushi_twi)<N><N>
"""
import scipy.linalg as la
import matplotlib.pyplot as plt
import math
import numpy as np
import sys
sys.path.append("../../PathPlanning/CubicSpline/")<N><N>
try:
    import cubic_spline_planner
except:
    raise<N><N>

Kp = 1.0  # speed proportional gain<N><N>
# LQR parameter
Q = np.eye(4)
R = np.eye(1)<N><N>
# parameters
dt = 0.1  # time tick[s]
L = 0.5  # Wheel base of the vehicle [m]
max_steer = np.deg2rad(45.0)  # maximum steering angle[rad]<N><N>
show_animation = True
#  show_animation = False<N><N>

class State:<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v<N><N>

def update(state, a, delta):<N><N>
    if delta >= max_steer:
        delta = max_steer
    if delta <= - max_steer:
        delta = - max_steer<N><N>
    state.x = state.x + state.v * math.cos(state.yaw) * dt
    state.y = state.y + state.v * math.sin(state.yaw) * dt
    state.yaw = state.yaw + state.v / L * math.tan(delta) * dt
    state.v = state.v + a * dt<N><N>
    return state<N><N>

def PIDControl(target, current):
    a = Kp * (target - current)<N><N>
    return a<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def solve_DARE(A, B, Q, R):
    """
    solve a discrete time_Algebraic Riccati equation (DARE)
    """
    X = Q
    maxiter = 150
    eps = 0.01<N><N>
    for i in range(maxiter):
        Xn = A.T @ X @ A - A.T @ X @ B @ \
            la.inv(R + B.T @ X @ B) @ B.T @ X @ A + Q
        if (abs(Xn - X)).max() < eps:
            break
        X = Xn<N><N>
    return Xn<N><N>

def dlqr(A, B, Q, R):
    """Solve the discrete time lqr controller.
    x[k+1] = A x[k] + B u[k]
    cost = sum x[k].T*Q*x[k] + u[k].T*R*u[k]
    # ref Bertsekas, p.151
    """<N><N>
    # first, try to solve the ricatti equation
    X = solve_DARE(A, B, Q, R)<N><N>
    # compute the LQR gain
    K = la.inv(B.T @ X @ B + R) @ (B.T @ X @ A)<N><N>
    eigVals, eigVecs = la.eig(A - B @ K)<N><N>
    return K, X, eigVals<N><N>

def lqr_steering_control(state, cx, cy, cyaw, ck, pe, pth_e):
    ind, e = calc_nearest_index(state, cx, cy, cyaw)<N><N>
    k = ck[ind]
    v = state.v
    th_e = pi_2_pi(state.yaw - cyaw[ind])<N><N>
    A = np.zeros((4, 4))
    A[0, 0] = 1.0
    A[0, 1] = dt
    A[1, 2] = v
    A[2, 2] = 1.0
    A[2, 3] = dt
    # print(A)<N><N>
    B = np.zeros((4, 1))
    B[3, 0] = v / L<N><N>
    K, _, _ = dlqr(A, B, Q, R)<N><N>
    x = np.zeros((4, 1))<N><N>
    x[0, 0] = e
    x[1, 0] = (e - pe) / dt
    x[2, 0] = th_e
    x[3, 0] = (th_e - pth_e) / dt<N><N>
    ff = math.atan2(L * k, 1)
    fb = pi_2_pi((-K @ x)[0, 0])<N><N>
    delta = ff + fb<N><N>
    return delta, ind, e, th_e<N><N>

def calc_nearest_index(state, cx, cy, cyaw):
    dx = [state.x - icx for icx in cx]
    dy = [state.y - icy for icy in cy]<N><N>
    d = [idx ** 2 + idy ** 2 for (idx, idy) in zip(dx, dy)]<N><N>
    mind = min(d)<N><N>
    ind = d.index(mind)<N><N>
    mind = math.sqrt(mind)<N><N>
    dxl = cx[ind] - state.x
    dyl = cy[ind] - state.y<N><N>
    angle = pi_2_pi(cyaw[ind] - math.atan2(dyl, dxl))
    if angle < 0:
        mind *= -1<N><N>
    return ind, mind<N><N>

def closed_loop_prediction(cx, cy, cyaw, ck, speed_profile, goal):
    T = 500.0  # max simulation time
    goal_dis = 0.3
    stop_speed = 0.05<N><N>
    state = State(x=-0.0, y=-0.0, yaw=0.0, v=0.0)<N><N>
    time = 0.0
    x = [state.x]
    y = [state.y]
    yaw = [state.yaw]
    v = [state.v]
    t = [0.0]<N><N>
    e, e_th = 0.0, 0.0<N><N>
    while T >= time:
        dl, target_ind, e, e_th = lqr_steering_control(
            state, cx, cy, cyaw, ck, e, e_th)<N><N>
        ai = PIDControl(speed_profile[target_ind], state.v)
        state = update(state, ai, dl)<N><N>
        if abs(state.v) <= stop_speed:
            target_ind += 1<N><N>
        time = time + dt<N><N>
        # check goal
        dx = state.x - goal[0]
        dy = state.y - goal[1]
        if math.hypot(dx, dy) <= goal_dis:
            print("Goal")
            break<N><N>
        x.append(state.x)
        y.append(state.y)
        yaw.append(state.yaw)
        v.append(state.v)
        t.append(time)<N><N>
"""<N><N>Path tracking simulation with iterative linear model predictive control for speed and steer control<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""
import matplotlib.pyplot as plt
import cvxpy
import math
import numpy as np
import sys
sys.path.append("../../PathPlanning/CubicSpline/")<N><N>
try:
    import cubic_spline_planner
except:
    raise<N><N>

NX = 4  # x = x, y, v, yaw
NU = 2  # a = [accel, steer]
T = 5  # horizon length<N><N>
# mpc parameters
R = np.diag([0.01, 0.01])  # input cost matrix
Rd = np.diag([0.01, 1.0])  # input difference cost matrix
Q = np.diag([1.0, 1.0, 0.5, 0.5])  # state cost matrix
Qf = Q  # state final matrix
GOAL_DIS = 1.5  # goal distance
STOP_SPEED = 0.5 / 3.6  # stop speed
MAX_TIME = 500.0  # max simulation time<N><N>
# iterative paramter
MAX_ITER = 3  # Max iteration
DU_TH = 0.1  # iteration finish param<N><N>
TARGET_SPEED = 10.0 / 3.6  # [m/s] target speed
N_IND_SEARCH = 10  # Search index number<N><N>
DT = 0.2  # [s] time tick<N><N>
# Vehicle parameters
LENGTH = 4.5  # [m]
WIDTH = 2.0  # [m]
BACKTOWHEEL = 1.0  # [m]
WHEEL_LEN = 0.3  # [m]
WHEEL_WIDTH = 0.2  # [m]
TREAD = 0.7  # [m]
WB = 2.5  # [m]<N><N>
MAX_STEER = np.deg2rad(45.0)  # maximum steering angle [rad]
MAX_DSTEER = np.deg2rad(30.0)  # maximum steering speed [rad/s]
MAX_SPEED = 55.0 / 3.6  # maximum speed [m/s]
MIN_SPEED = -20.0 / 3.6  # minimum speed [m/s]
MAX_ACCEL = 1.0  # maximum accel [m/ss]<N><N>
show_animation = True<N><N>

class State:
    """
    vehicle state class
    """<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v
        self.predelta = None<N><N>

def pi_2_pi(angle):
    while(angle > math.pi):
        angle = angle - 2.0 * math.pi<N><N>
    while(angle < -math.pi):
        angle = angle + 2.0 * math.pi<N><N>
    return angle<N><N>

def get_linear_model_matrix(v, phi, delta):<N><N>
    A = np.zeros((NX, NX))
    A[0, 0] = 1.0
    A[1, 1] = 1.0
    A[2, 2] = 1.0
    A[3, 3] = 1.0
    A[0, 2] = DT * math.cos(phi)
    A[0, 3] = - DT * v * math.sin(phi)
    A[1, 2] = DT * math.sin(phi)
    A[1, 3] = DT * v * math.cos(phi)
    A[3, 2] = DT * math.tan(delta) / WB<N><N>
    B = np.zeros((NX, NU))
    B[2, 0] = DT
    B[3, 1] = DT * v / (WB * math.cos(delta) ** 2)<N><N>
    C = np.zeros(NX)
    C[0] = DT * v * math.sin(phi) * phi
    C[1] = - DT * v * math.cos(phi) * phi
    C[3] = - DT * v * delta / (WB * math.cos(delta) ** 2)<N><N>
    return A, B, C<N><N>

def plot_car(x, y, yaw, steer=0.0, cabcolor="-r", truckcolor="-k"):  # pragma: no cover<N><N>
    outline = np.array([[-BACKTOWHEEL, (LENGTH - BACKTOWHEEL), (LENGTH - BACKTOWHEEL), -BACKTOWHEEL, -BACKTOWHEEL],
                        [WIDTH / 2, WIDTH / 2, - WIDTH / 2, -WIDTH / 2, WIDTH / 2]])<N><N>
    fr_wheel = np.array([[WHEEL_LEN, -WHEEL_LEN, -WHEEL_LEN, WHEEL_LEN, WHEEL_LEN],
                         [-WHEEL_WIDTH - TREAD, -WHEEL_WIDTH - TREAD, WHEEL_WIDTH - TREAD, WHEEL_WIDTH - TREAD, -WHEEL_WIDTH - TREAD]])<N><N>
    rr_wheel = np.copy(fr_wheel)<N><N>
    fl_wheel = np.copy(fr_wheel)
    fl_wheel[1, :] *= -1
    rl_wheel = np.copy(rr_wheel)
    rl_wheel[1, :] *= -1<N><N>
    Rot1 = np.array([[math.cos(yaw), math.sin(yaw)],
                     [-math.sin(yaw), math.cos(yaw)]])
    Rot2 = np.array([[math.cos(steer), math.sin(steer)],
                     [-math.sin(steer), math.cos(steer)]])<N><N>
    fr_wheel = (fr_wheel.T.dot(Rot2)).T
    fl_wheel = (fl_wheel.T.dot(Rot2)).T
    fr_wheel[0, :] += WB
    fl_wheel[0, :] += WB<N><N>
    fr_wheel = (fr_wheel.T.dot(Rot1)).T
    fl_wheel = (fl_wheel.T.dot(Rot1)).T<N><N>
    outline = (outline.T.dot(Rot1)).T
    rr_wheel = (rr_wheel.T.dot(Rot1)).T
    rl_wheel = (rl_wheel.T.dot(Rot1)).T<N><N>
    outline[0, :] += x
    outline[1, :] += y
    fr_wheel[0, :] += x
    fr_wheel[1, :] += y
    rr_wheel[0, :] += x
    rr_wheel[1, :] += y
    fl_wheel[0, :] += x
    fl_wheel[1, :] += y
    rl_wheel[0, :] += x
    rl_wheel[1, :] += y<N><N>
"""<N><N>Move to specified pose<N><N>
Author: Daniel Ingram (daniel-s-ingram)
        Atsushi Sakai(@Atsushi_twi)<N><N>
P. I. Corke, "Robotics, Vision & Control", Springer 2017, ISBN 978-3-319-54413-7<N><N>
"""<N><N>import matplotlib.pyplot as plt
import numpy as np
from random import random<N><N>
# simulation parameters
Kp_rho = 9
Kp_alpha = 15
Kp_beta = -3
dt = 0.01<N><N>
show_animation = True<N><N>

def move_to_pose(x_start, y_start, theta_start, x_goal, y_goal, theta_goal):
    """
    rho is the distance between the robot and the goal position
    alpha is the angle to the goal relative to the heading of the robot
    beta is the angle between the robot's position and the goal position plus the goal angle<N><N>
    Kp_rho*rho and Kp_alpha*alpha drive the robot along a line towards the goal
    Kp_beta*beta rotates the line so that it is parallel to the goal angle
    """
    x = x_start
    y = y_start
    theta = theta_start<N><N>
    x_diff = x_goal - x
    y_diff = y_goal - y<N><N>
    x_traj, y_traj = [], []<N><N>
    rho = np.hypot(x_diff, y_diff)
    while rho > 0.001:
        x_traj.append(x)
        y_traj.append(y)<N><N>
        x_diff = x_goal - x
        y_diff = y_goal - y<N><N>
        # Restrict alpha and beta (angle differences) to the range
        # [-pi, pi] to prevent unstable behavior e.g. difference going
        # from 0 rad to 2*pi rad with slight turn<N><N>
        rho = np.hypot(x_diff, y_diff)
        alpha = (np.arctan2(y_diff, x_diff)
                 - theta + np.pi) % (2 * np.pi) - np.pi
        beta = (theta_goal - theta - alpha + np.pi) % (2 * np.pi) - np.pi<N><N>
        v = Kp_rho * rho
        w = Kp_alpha * alpha + Kp_beta * beta<N><N>
        if alpha > np.pi / 2 or alpha < -np.pi / 2:
            v = -v<N><N>
        theta = theta + w * dt
        x = x + v * np.cos(theta) * dt
        y = y + v * np.sin(theta) * dt<N><N>
        if show_animation:  # pragma: no cover
            plt.cla()
            plt.arrow(x_start, y_start, np.cos(theta_start),
                      np.sin(theta_start), color='r', width=0.1)
            plt.arrow(x_goal, y_goal, np.cos(theta_goal),
                      np.sin(theta_goal), color='g', width=0.1)
            plot_vehicle(x, y, theta, x_traj, y_traj)<N><N>

def plot_vehicle(x, y, theta, x_traj, y_traj):  # pragma: no cover
    # Corners of triangular vehicle when pointing to the right (0 radians)
    p1_i = np.array([0.5, 0, 1]).T
    p2_i = np.array([-0.5, 0.25, 1]).T
    p3_i = np.array([-0.5, -0.25, 1]).T<N><N>
    T = transformation_matrix(x, y, theta)
    p1 = np.matmul(T, p1_i)
    p2 = np.matmul(T, p2_i)
    p3 = np.matmul(T, p3_i)<N><N>
    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], 'k-')
    plt.plot([p2[0], p3[0]], [p2[1], p3[1]], 'k-')
    plt.plot([p3[0], p1[0]], [p3[1], p1[1]], 'k-')<N><N>
    plt.plot(x_traj, y_traj, 'b--')<N><N>
    # for stopping simulation with the esc key.
    plt.gcf().canvas.mpl_connect('key_release_event',
            lambda event: [exit(0) if event.key == 'escape' else None])<N><N>
    plt.xlim(0, 20)
    plt.ylim(0, 20)<N><N>
    plt.pause(dt)<N><N>

def transformation_matrix(x, y, theta):
    return np.array([
        [np.cos(theta), -np.sin(theta), x],
        [np.sin(theta), np.cos(theta), y],
        [0, 0, 1]
    ])<N><N>

def main():<N><N>
"""<N><N>Path tracking simulation with pure pursuit steering and PID speed control.<N><N>
author: Atsushi Sakai (@Atsushi_twi)
        Guillaume Jacquenot (@Gjacquenot)<N><N>
"""
import numpy as np
import math
import matplotlib.pyplot as plt<N><N>
# Parameters
k = 0.1  # look forward gain
Lfc = 2.0  # [m] look-ahead distance
Kp = 1.0  # speed proportional gain
dt = 0.1  # [s] time tick
WB = 2.9  # [m] wheel base of vehicle<N><N>
show_animation = True<N><N>

class State:<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v
        self.rear_x = self.x - ((WB / 2) * math.cos(self.yaw))
        self.rear_y = self.y - ((WB / 2) * math.sin(self.yaw))<N><N>
    def update(self, a, delta):
        self.x += self.v * math.cos(self.yaw) * dt
        self.y += self.v * math.sin(self.yaw) * dt
        self.yaw += self.v / WB * math.tan(delta) * dt
        self.v += a * dt
        self.rear_x = self.x - ((WB / 2) * math.cos(self.yaw))
        self.rear_y = self.y - ((WB / 2) * math.sin(self.yaw))<N><N>
    def calc_distance(self, point_x, point_y):
        dx = self.rear_x - point_x
        dy = self.rear_y - point_y
        return math.hypot(dx, dy)<N><N>

class States:<N><N>
    def __init__(self):
        self.x = []
        self.y = []
        self.yaw = []
        self.v = []
        self.t = []<N><N>
    def append(self, t, state):
        self.x.append(state.x)
        self.y.append(state.y)
        self.yaw.append(state.yaw)
        self.v.append(state.v)
        self.t.append(t)<N><N>

def proportional_control(target, current):
    a = Kp * (target - current)<N><N>
    return a<N><N>

class TargetCourse:<N><N>
    def __init__(self, cx, cy):
        self.cx = cx
        self.cy = cy
        self.old_nearest_point_index = None<N><N>
    def search_target_index(self, state):<N><N>
"""<N><N>Path tracking simulation with rear wheel feedback steering control and PID speed control.<N><N>
author: Atsushi Sakai(@Atsushi_twi)<N><N>
"""
import matplotlib.pyplot as plt
import math
import numpy as np<N><N>
from scipy import interpolate
from scipy import optimize<N><N>
Kp = 1.0  # speed propotional gain
# steering control parameter
KTH = 1.0
KE = 0.5<N><N>
dt = 0.1  # [s]
L = 2.9  # [m]<N><N>
show_animation = True<N><N>
class State:
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0, direction=1):
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v
        self.direction = direction<N><N>
    def update(self, a, delta, dt):
        self.x   = self.x + self.v * math.cos(self.yaw) * dt
        self.y   = self.y + self.v * math.sin(self.yaw) * dt
        self.yaw = self.yaw + self.v / L * math.tan(delta) * dt
        self.v   = self.v + a * dt<N><N>
class CubicSplinePath:
    def __init__(self, x, y):
        x, y = map(np.asarray, (x, y))
        s = np.append([0],(np.cumsum(np.diff(x)**2) + np.cumsum(np.diff(y)**2))**0.5)<N><N>
        self.X = interpolate.CubicSpline(s, x)
        self.Y = interpolate.CubicSpline(s, y)<N><N>
        self.dX = self.X.derivative(1)
        self.ddX = self.X.derivative(2)<N><N>
        self.dY = self.Y.derivative(1)
        self.ddY = self.Y.derivative(2)<N><N>
"""<N><N>Path tracking simulation with Stanley steering control and PID speed control.<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
Ref:
    - [Stanley: The robot that won the DARPA grand challenge](http://isl.ecst.csuchico.edu/DOCS/darpa2005/DARPA%202005%20Stanley.pdf)
    - [Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)<N><N>
"""
import numpy as np
import matplotlib.pyplot as plt
import sys
sys.path.append("../../PathPlanning/CubicSpline/")<N><N>
try:
    import cubic_spline_planner
except:
    raise<N><N>

k = 0.5  # control gain
Kp = 1.0  # speed proportional gain
dt = 0.1  # [s] time difference
L = 2.9  # [m] Wheel base of vehicle
max_steer = np.radians(30.0)  # [rad] max steering angle<N><N>
show_animation = True<N><N>

class State(object):
    """
    Class representing the state of a vehicle.<N><N>
    :param x: (float) x-coordinate
    :param y: (float) y-coordinate
    :param yaw: (float) yaw angle
    :param v: (float) speed
    """<N><N>
    def __init__(self, x=0.0, y=0.0, yaw=0.0, v=0.0):
        """Instantiate the object."""
        super(State, self).__init__()
        self.x = x
        self.y = y
        self.yaw = yaw
        self.v = v<N><N>
    def update(self, acceleration, delta):
        """
        Update the state of the vehicle.<N><N>
        Stanley Control uses bicycle model.<N><N>
        :param acceleration: (float) Acceleration
        :param delta: (float) Steering
        """
        delta = np.clip(delta, -max_steer, max_steer)<N><N>
        self.x += self.v * np.cos(self.yaw) * dt
        self.y += self.v * np.sin(self.yaw) * dt
        self.yaw += self.v / L * np.tan(delta) * dt
        self.yaw = normalize_angle(self.yaw)
        self.v += acceleration * dt<N><N>

def pid_control(target, current):
    """
    Proportional control for the speed.<N><N>
    :param target: (float)
    :param current: (float)
    :return: (float)
    """
    return Kp * (target - current)<N><N>

def stanley_control(state, cx, cy, cyaw, last_target_idx):
    """
    Stanley steering control.<N><N>
    :param state: (State object)
    :param cx: ([float])
    :param cy: ([float])
    :param cyaw: ([float])
    :param last_target_idx: (int)
    :return: (float, int)
    """
    current_target_idx, error_front_axle = calc_target_index(state, cx, cy)<N><N>
    if last_target_idx >= current_target_idx:
        current_target_idx = last_target_idx<N><N>
    # theta_e corrects the heading error
    theta_e = normalize_angle(cyaw[current_target_idx] - state.yaw)
    # theta_d corrects the cross track error
    theta_d = np.arctan2(k * error_front_axle, state.v)
    # Steering control
    delta = theta_e + theta_d<N><N>
    return delta, current_target_idx<N><N>

def normalize_angle(angle):
    """
    Normalize an angle to [-pi, pi].<N><N>
    :param angle: (float)
    :return: (float) Angle in radian in [-pi, pi]
    """
    while angle > np.pi:
        angle -= 2.0 * np.pi<N><N>
    while angle < -np.pi:
        angle += 2.0 * np.pi<N><N>
    return angle<N><N>

def calc_target_index(state, cx, cy):
    """
    Compute index in the trajectory list of the target.<N><N>
    :param state: (State object)
    :param cx: [float]
    :param cy: [float]
    :return: (int, float)
    """
    # Calc front axle position
    fx = state.x + L * np.cos(state.yaw)
    fy = state.y + L * np.sin(state.yaw)<N><N>
    # Search nearest point index
    dx = [fx - icx for icx in cx]
    dy = [fy - icy for icy in cy]
    d = np.hypot(dx, dy)
    target_idx = np.argmin(d)<N><N>
    # Project RMS error onto front axle vector
    front_axle_vec = [-np.cos(state.yaw + np.pi / 2),
                      -np.sin(state.yaw + np.pi / 2)]
    error_front_axle = np.dot([dx[target_idx], dy[target_idx]], front_axle_vec)<N><N>
    return target_idx, error_front_axle<N><N>

def main():
    """Plot an example of Stanley steering control on a cubic spline."""
    #  target course
    ax = [0.0, 100.0, 100.0, 50.0, 60.0]
    ay = [0.0, 0.0, -30.0, -20.0, 0.0]<N><N>
    cx, cy, cyaw, ck, s = cubic_spline_planner.calc_spline_course(
        ax, ay, ds=0.1)<N><N>
    target_speed = 30.0 / 3.6  # [m/s]<N><N>
    max_simulation_time = 100.0<N><N>
    # Initial state
    state = State(x=-0.0, y=5.0, yaw=np.radians(20.0), v=0.0)<N><N>
    last_idx = len(cx) - 1
    time = 0.0
    x = [state.x]
    y = [state.y]
    yaw = [state.yaw]
    v = [state.v]
    t = [0.0]
    target_idx, _ = calc_target_index(state, cx, cy)<N><N>
    while max_simulation_time >= time and last_idx > target_idx:
        ai = pid_control(target_speed, state.v)
        di, target_idx = stanley_control(state, cx, cy, cyaw, target_idx)
        state.update(ai, di)<N><N>
        time += dt<N><N>
        x.append(state.x)
        y.append(state.y)
        yaw.append(state.yaw)
        v.append(state.v)
        t.append(time)<N><N>
"""
Extended Kalman Filter SLAM example<N><N>
author: Atsushi Sakai (@Atsushi_twi)
"""<N><N>
import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
# EKF state covariance
Cx = np.diag([0.5, 0.5, np.deg2rad(30.0)]) ** 2<N><N>
#  Simulation parameter
Q_sim = np.diag([0.2, np.deg2rad(1.0)]) ** 2
R_sim = np.diag([1.0, np.deg2rad(10.0)]) ** 2<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]
MAX_RANGE = 20.0  # maximum observation range
M_DIST_TH = 2.0  # Threshold of Mahalanobis distance for data association.
STATE_SIZE = 3  # State size [x,y,yaw]
LM_SIZE = 2  # LM state size [x,y]<N><N>
show_animation = True<N><N>

def ekf_slam(xEst, PEst, u, z):
    # Predict
    S = STATE_SIZE
    G, Fx = jacob_motion(xEst[0:S], u)
    xEst[0:S] = motion_model(xEst[0:S], u)
    PEst[0:S, 0:S] = G.T @ PEst[0:S, 0:S] @ G + Fx.T @ Cx @ Fx
    initP = np.eye(2)<N><N>
    # Update
    for iz in range(len(z[:, 0])):  # for each observation
        min_id = search_correspond_landmark_id(xEst, PEst, z[iz, 0:2])<N><N>
"""<N><N>FastSLAM 1.0 example<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
# Fast SLAM covariance
Q = np.diag([3.0, np.deg2rad(10.0)]) ** 2
R = np.diag([1.0, np.deg2rad(20.0)]) ** 2<N><N>
#  Simulation parameter
Q_sim = np.diag([0.3, np.deg2rad(2.0)]) ** 2
R_sim = np.diag([0.5, np.deg2rad(10.0)]) ** 2
OFFSET_YAW_RATE_NOISE = 0.01<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]
MAX_RANGE = 20.0  # maximum observation range
M_DIST_TH = 2.0  # Threshold of Mahalanobis distance for data association.
STATE_SIZE = 3  # State size [x,y,yaw]
LM_SIZE = 2  # LM state size [x,y]
N_PARTICLE = 100  # number of particle
NTH = N_PARTICLE / 1.5  # Number of particle for re-sampling<N><N>
show_animation = True<N><N>

class Particle:<N><N>
    def __init__(self, n_landmark):
        self.w = 1.0 / N_PARTICLE
        self.x = 0.0
        self.y = 0.0
        self.yaw = 0.0
        # landmark x-y positions
        self.lm = np.zeros((n_landmark, LM_SIZE))
        # landmark position covariance
        self.lmP = np.zeros((n_landmark * LM_SIZE, LM_SIZE))<N><N>

def fast_slam1(particles, u, z):
    particles = predict_particles(particles, u)<N><N>
    particles = update_with_observation(particles, z)<N><N>
    particles = resampling(particles)<N><N>
    return particles<N><N>

def normalize_weight(particles):
    sum_w = sum([p.w for p in particles])<N><N>
    try:
        for i in range(N_PARTICLE):
            particles[i].w /= sum_w
    except ZeroDivisionError:
        for i in range(N_PARTICLE):
            particles[i].w = 1.0 / N_PARTICLE<N><N>
        return particles<N><N>
    return particles<N><N>

def calc_final_state(particles):
    xEst = np.zeros((STATE_SIZE, 1))<N><N>
    particles = normalize_weight(particles)<N><N>
    for i in range(N_PARTICLE):
        xEst[0, 0] += particles[i].w * particles[i].x
        xEst[1, 0] += particles[i].w * particles[i].y
        xEst[2, 0] += particles[i].w * particles[i].yaw<N><N>
    xEst[2, 0] = pi_2_pi(xEst[2, 0])
    #  print(xEst)<N><N>
    return xEst<N><N>

def predict_particles(particles, u):
    for i in range(N_PARTICLE):
        px = np.zeros((STATE_SIZE, 1))
        px[0, 0] = particles[i].x
        px[1, 0] = particles[i].y
        px[2, 0] = particles[i].yaw
        ud = u + (np.random.randn(1, 2) @ R ** 0.5).T  # add noise
        px = motion_model(px, ud)
        particles[i].x = px[0, 0]
        particles[i].y = px[1, 0]
        particles[i].yaw = px[2, 0]<N><N>
    return particles<N><N>

def add_new_landmark(particle, z, Q_cov):
    r = z[0]
    b = z[1]
    lm_id = int(z[2])<N><N>
    s = math.sin(pi_2_pi(particle.yaw + b))
    c = math.cos(pi_2_pi(particle.yaw + b))<N><N>
    particle.lm[lm_id, 0] = particle.x + r * c
    particle.lm[lm_id, 1] = particle.y + r * s<N><N>
    # covariance
    dx = r * c
    dy = r * s
    d2 = dx**2 + dy**2
    d = math.sqrt(d2)
    Gz = np.array([[dx / d, dy / d],
                   [-dy / d2, dx / d2]])
    particle.lmP[2 * lm_id:2 * lm_id + 2] = np.linalg.inv(
        Gz) @ Q_cov @ np.linalg.inv(Gz.T)<N><N>
    return particle<N><N>

def compute_jacobians(particle, xf, Pf, Q_cov):
    dx = xf[0, 0] - particle.x
    dy = xf[1, 0] - particle.y
    d2 = dx ** 2 + dy ** 2
    d = math.sqrt(d2)<N><N>
    zp = np.array(
        [d, pi_2_pi(math.atan2(dy, dx) - particle.yaw)]).reshape(2, 1)<N><N>
    Hv = np.array([[-dx / d, -dy / d, 0.0],
                   [dy / d2, -dx / d2, -1.0]])<N><N>
    Hf = np.array([[dx / d, dy / d],
                   [-dy / d2, dx / d2]])<N><N>
    Sf = Hf @ Pf @ Hf.T + Q_cov<N><N>
    return zp, Hv, Hf, Sf<N><N>

def update_kf_with_cholesky(xf, Pf, v, Q_cov, Hf):
    PHt = Pf @ Hf.T
    S = Hf @ PHt + Q_cov<N><N>
    S = (S + S.T) * 0.5
    s_chol = np.linalg.cholesky(S).T
    s_chol_inv = np.linalg.inv(s_chol)
    W1 = PHt @ s_chol_inv
    W = W1 @ s_chol_inv.T<N><N>
    x = xf + W @ v
    P = Pf - W1 @ W1.T<N><N>
    return x, P<N><N>

def update_landmark(particle, z, Q_cov):
    lm_id = int(z[2])
    xf = np.array(particle.lm[lm_id, :]).reshape(2, 1)
    Pf = np.array(particle.lmP[2 * lm_id:2 * lm_id + 2, :])<N><N>
    zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q)<N><N>
    dz = z[0:2].reshape(2, 1) - zp
    dz[1, 0] = pi_2_pi(dz[1, 0])<N><N>
    xf, Pf = update_kf_with_cholesky(xf, Pf, dz, Q_cov, Hf)<N><N>
    particle.lm[lm_id, :] = xf.T
    particle.lmP[2 * lm_id:2 * lm_id + 2, :] = Pf<N><N>
    return particle<N><N>

def compute_weight(particle, z, Q_cov):
    lm_id = int(z[2])
    xf = np.array(particle.lm[lm_id, :]).reshape(2, 1)
    Pf = np.array(particle.lmP[2 * lm_id:2 * lm_id + 2])
    zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)<N><N>
    dx = z[0:2].reshape(2, 1) - zp
    dx[1, 0] = pi_2_pi(dx[1, 0])<N><N>
    try:
        invS = np.linalg.inv(Sf)
    except np.linalg.linalg.LinAlgError:
        print("singular")
        return 1.0<N><N>
    num = math.exp(-0.5 * dx.T @ invS @ dx)
    den = 2.0 * math.pi * math.sqrt(np.linalg.det(Sf))<N><N>
    w = num / den<N><N>
    return w<N><N>

def update_with_observation(particles, z):
    for iz in range(len(z[0, :])):<N><N>
        landmark_id = int(z[2, iz])<N><N>
        for ip in range(N_PARTICLE):
            # new landmark
            if abs(particles[ip].lm[landmark_id, 0]) <= 0.01:
                particles[ip] = add_new_landmark(particles[ip], z[:, iz], Q)
            # known landmark
            else:
                w = compute_weight(particles[ip], z[:, iz], Q)
                particles[ip].w *= w
                particles[ip] = update_landmark(particles[ip], z[:, iz], Q)<N><N>
    return particles<N><N>

def resampling(particles):
    """
    low variance re-sampling
    """<N><N>
    particles = normalize_weight(particles)<N><N>
    pw = []
    for i in range(N_PARTICLE):
        pw.append(particles[i].w)<N><N>
    pw = np.array(pw)<N><N>
    n_eff = 1.0 / (pw @ pw.T)  # Effective particle number
    # print(n_eff)<N><N>
    if n_eff < NTH:  # resampling
        w_cum = np.cumsum(pw)
        base = np.cumsum(pw * 0.0 + 1 / N_PARTICLE) - 1 / N_PARTICLE
        resample_id = base + np.random.rand(base.shape[0]) / N_PARTICLE<N><N>
        inds = []
        ind = 0
        for ip in range(N_PARTICLE):
            while (ind < w_cum.shape[0] - 1) \
                    and (resample_id[ip] > w_cum[ind]):
                ind += 1
            inds.append(ind)<N><N>
        tmp_particles = particles[:]
        for i in range(len(inds)):
            particles[i].x = tmp_particles[inds[i]].x
            particles[i].y = tmp_particles[inds[i]].y
            particles[i].yaw = tmp_particles[inds[i]].yaw
            particles[i].lm = tmp_particles[inds[i]].lm[:, :]
            particles[i].lmP = tmp_particles[inds[i]].lmP[:, :]
            particles[i].w = 1.0 / N_PARTICLE<N><N>
    return particles<N><N>

def calc_input(time):
    if time <= 3.0:  # wait at first
        v = 0.0
        yaw_rate = 0.0
    else:
        v = 1.0  # [m/s]
        yaw_rate = 0.1  # [rad/s]<N><N>
    u = np.array([v, yaw_rate]).reshape(2, 1)<N><N>
    return u<N><N>

def observation(xTrue, xd, u, rfid):
    # calc true state
    xTrue = motion_model(xTrue, u)<N><N>
    # add noise to range observation
    z = np.zeros((3, 0))
    for i in range(len(rfid[:, 0])):<N><N>
        dx = rfid[i, 0] - xTrue[0, 0]
        dy = rfid[i, 1] - xTrue[1, 0]
        d = math.hypot(dx, dy)
        angle = pi_2_pi(math.atan2(dy, dx) - xTrue[2, 0])
        if d <= MAX_RANGE:
            dn = d + np.random.randn() * Q_sim[0, 0] ** 0.5  # add noise
            angle_with_noize = angle + np.random.randn() * Q_sim[
                1, 1] ** 0.5  # add noise
            zi = np.array([dn, pi_2_pi(angle_with_noize), i]).reshape(3, 1)
            z = np.hstack((z, zi))<N><N>
    # add noise to input
    ud1 = u[0, 0] + np.random.randn() * R_sim[0, 0] ** 0.5
    ud2 = u[1, 0] + np.random.randn() * R_sim[
        1, 1] ** 0.5 + OFFSET_YAW_RATE_NOISE
    ud = np.array([ud1, ud2]).reshape(2, 1)<N><N>
    xd = motion_model(xd, ud)<N><N>
    return xTrue, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0],
                  [0, 1.0, 0],
                  [0, 0, 1.0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT]])<N><N>
    x = F @ x + B @ u<N><N>
    x[2, 0] = pi_2_pi(x[2, 0])<N><N>
    return x<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    time = 0.0<N><N>
    # RFID positions [x, y]
    RFID = np.array([[10.0, -2.0],
                     [15.0, 10.0],
                     [15.0, 15.0],
                     [10.0, 20.0],
                     [3.0, 15.0],
                     [-5.0, 20.0],
                     [-5.0, 5.0],
                     [-10.0, 15.0]
                     ])
    n_landmark = RFID.shape[0]<N><N>
    # State Vector [x y yaw v]'
    xEst = np.zeros((STATE_SIZE, 1))  # SLAM estimation
    xTrue = np.zeros((STATE_SIZE, 1))  # True state
    xDR = np.zeros((STATE_SIZE, 1))  # Dead reckoning<N><N>
    # history
    hxEst = xEst
    hxTrue = xTrue
    hxDR = xTrue<N><N>
    particles = [Particle(n_landmark) for _ in range(N_PARTICLE)]<N><N>
    while SIM_TIME >= time:
        time += DT
        u = calc_input(time)<N><N>
        xTrue, z, xDR, ud = observation(xTrue, xDR, u, RFID)<N><N>
        particles = fast_slam1(particles, ud, z)<N><N>
        xEst = calc_final_state(particles)<N><N>
        x_state = xEst[0: STATE_SIZE]<N><N>
        # store data history
        hxEst = np.hstack((hxEst, x_state))
        hxDR = np.hstack((hxDR, xDR))
        hxTrue = np.hstack((hxTrue, xTrue))<N><N>
        if show_animation:  # pragma: no cover
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event', lambda event:
                [exit(0) if event.key == 'escape' else None])
            plt.plot(RFID[:, 0], RFID[:, 1], "*k")<N><N>
            for i in range(N_PARTICLE):
                plt.plot(particles[i].x, particles[i].y, ".r")
                plt.plot(particles[i].lm[:, 0], particles[i].lm[:, 1], "xb")<N><N>
            plt.plot(hxTrue[0, :], hxTrue[1, :], "-b")
            plt.plot(hxDR[0, :], hxDR[1, :], "-k")
            plt.plot(hxEst[0, :], hxEst[1, :], "-r")
            plt.plot(xEst[0], xEst[1], "xk")
            plt.axis("equal")
            plt.grid(True)
            plt.pause(0.001)<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>FastSLAM 2.0 example<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
"""<N><N>import math<N><N>
import matplotlib.pyplot as plt
import numpy as np<N><N>
# Fast SLAM covariance
Q = np.diag([3.0, np.deg2rad(10.0)]) ** 2
R = np.diag([1.0, np.deg2rad(20.0)]) ** 2<N><N>
#  Simulation parameter
Q_sim = np.diag([0.3, np.deg2rad(2.0)]) ** 2
R_sim = np.diag([0.5, np.deg2rad(10.0)]) ** 2
OFFSET_YAW_RATE_NOISE = 0.01<N><N>
DT = 0.1  # time tick [s]
SIM_TIME = 50.0  # simulation time [s]
MAX_RANGE = 20.0  # maximum observation range
M_DIST_TH = 2.0  # Threshold of Mahalanobis distance for data association.
STATE_SIZE = 3  # State size [x,y,yaw]
LM_SIZE = 2  # LM state size [x,y]
N_PARTICLE = 100  # number of particle
NTH = N_PARTICLE / 1.5  # Number of particle for re-sampling<N><N>
show_animation = True<N><N>

class Particle:<N><N>
    def __init__(self, N_LM):
        self.w = 1.0 / N_PARTICLE
        self.x = 0.0
        self.y = 0.0
        self.yaw = 0.0
        self.P = np.eye(3)
        # landmark x-y positions
        self.lm = np.zeros((N_LM, LM_SIZE))
        # landmark position covariance
        self.lmP = np.zeros((N_LM * LM_SIZE, LM_SIZE))<N><N>

def fast_slam2(particles, u, z):
    particles = predict_particles(particles, u)<N><N>
    particles = update_with_observation(particles, z)<N><N>
    particles = resampling(particles)<N><N>
    return particles<N><N>

def normalize_weight(particles):
    sum_w = sum([p.w for p in particles])<N><N>
    try:
        for i in range(N_PARTICLE):
            particles[i].w /= sum_w
    except ZeroDivisionError:
        for i in range(N_PARTICLE):
            particles[i].w = 1.0 / N_PARTICLE<N><N>
        return particles<N><N>
    return particles<N><N>

def calc_final_state(particles):
    xEst = np.zeros((STATE_SIZE, 1))<N><N>
    particles = normalize_weight(particles)<N><N>
    for i in range(N_PARTICLE):
        xEst[0, 0] += particles[i].w * particles[i].x
        xEst[1, 0] += particles[i].w * particles[i].y
        xEst[2, 0] += particles[i].w * particles[i].yaw<N><N>
    xEst[2, 0] = pi_2_pi(xEst[2, 0])<N><N>
    return xEst<N><N>

def predict_particles(particles, u):
    for i in range(N_PARTICLE):
        px = np.zeros((STATE_SIZE, 1))
        px[0, 0] = particles[i].x
        px[1, 0] = particles[i].y
        px[2, 0] = particles[i].yaw
        ud = u + (np.random.randn(1, 2) @ R ** 0.5).T  # add noise
        px = motion_model(px, ud)
        particles[i].x = px[0, 0]
        particles[i].y = px[1, 0]
        particles[i].yaw = px[2, 0]<N><N>
    return particles<N><N>

def add_new_lm(particle, z, Q_cov):
    r = z[0]
    b = z[1]
    lm_id = int(z[2])<N><N>
    s = math.sin(pi_2_pi(particle.yaw + b))
    c = math.cos(pi_2_pi(particle.yaw + b))<N><N>
    particle.lm[lm_id, 0] = particle.x + r * c
    particle.lm[lm_id, 1] = particle.y + r * s<N><N>
    # covariance
    dx = r * c
    dy = r * s
    d2 = dx ** 2 + dy ** 2
    d = math.sqrt(d2)
    Gz = np.array([[dx / d, dy / d],
                   [-dy / d2, dx / d2]])
    particle.lmP[2 * lm_id:2 * lm_id + 2] = np.linalg.inv(
        Gz) @ Q_cov @ np.linalg.inv(Gz.T)<N><N>
    return particle<N><N>

def compute_jacobians(particle, xf, Pf, Q_cov):
    dx = xf[0, 0] - particle.x
    dy = xf[1, 0] - particle.y
    d2 = dx ** 2 + dy ** 2
    d = math.sqrt(d2)<N><N>
    zp = np.array(
        [d, pi_2_pi(math.atan2(dy, dx) - particle.yaw)]).reshape(2, 1)<N><N>
    Hv = np.array([[-dx / d, -dy / d, 0.0],
                   [dy / d2, -dx / d2, -1.0]])<N><N>
    Hf = np.array([[dx / d, dy / d],
                   [-dy / d2, dx / d2]])<N><N>
    Sf = Hf @ Pf @ Hf.T + Q_cov<N><N>
    return zp, Hv, Hf, Sf<N><N>

def update_kf_with_cholesky(xf, Pf, v, Q_cov, Hf):
    PHt = Pf @ Hf.T
    S = Hf @ PHt + Q_cov<N><N>
    S = (S + S.T) * 0.5
    SChol = np.linalg.cholesky(S).T
    SCholInv = np.linalg.inv(SChol)
    W1 = PHt @ SCholInv
    W = W1 @ SCholInv.T<N><N>
    x = xf + W @ v
    P = Pf - W1 @ W1.T<N><N>
    return x, P<N><N>

def update_landmark(particle, z, Q_cov):
    lm_id = int(z[2])
    xf = np.array(particle.lm[lm_id, :]).reshape(2, 1)
    Pf = np.array(particle.lmP[2 * lm_id:2 * lm_id + 2])<N><N>
    zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)<N><N>
    dz = z[0:2].reshape(2, 1) - zp
    dz[1, 0] = pi_2_pi(dz[1, 0])<N><N>
    xf, Pf = update_kf_with_cholesky(xf, Pf, dz, Q, Hf)<N><N>
    particle.lm[lm_id, :] = xf.T
    particle.lmP[2 * lm_id:2 * lm_id + 2, :] = Pf<N><N>
    return particle<N><N>

def compute_weight(particle, z, Q_cov):
    lm_id = int(z[2])
    xf = np.array(particle.lm[lm_id, :]).reshape(2, 1)
    Pf = np.array(particle.lmP[2 * lm_id:2 * lm_id + 2])
    zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)<N><N>
    dz = z[0:2].reshape(2, 1) - zp
    dz[1, 0] = pi_2_pi(dz[1, 0])<N><N>
    try:
        invS = np.linalg.inv(Sf)
    except np.linalg.linalg.LinAlgError:
        return 1.0<N><N>
    num = math.exp(-0.5 * dz.T @ invS @ dz)
    den = 2.0 * math.pi * math.sqrt(np.linalg.det(Sf))<N><N>
    w = num / den<N><N>
    return w<N><N>

def proposal_sampling(particle, z, Q_cov):
    lm_id = int(z[2])
    xf = particle.lm[lm_id, :].reshape(2, 1)
    Pf = particle.lmP[2 * lm_id:2 * lm_id + 2]
    # State
    x = np.array([particle.x, particle.y, particle.yaw]).reshape(3, 1)
    P = particle.P
    zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)<N><N>
    Sfi = np.linalg.inv(Sf)
    dz = z[0:2].reshape(2, 1) - zp
    dz[1] = pi_2_pi(dz[1])<N><N>
    Pi = np.linalg.inv(P)<N><N>
    particle.P = np.linalg.inv(Hv.T @ Sfi @ Hv + Pi)  # proposal covariance
    x += particle.P @ Hv.T @ Sfi @ dz  # proposal mean<N><N>
    particle.x = x[0, 0]
    particle.y = x[1, 0]
    particle.yaw = x[2, 0]<N><N>
    return particle<N><N>

def update_with_observation(particles, z):
    for iz in range(len(z[0, :])):
        landmark_id = int(z[2, iz])<N><N>
        for ip in range(N_PARTICLE):
            # new landmark
            if abs(particles[ip].lm[landmark_id, 0]) <= 0.01:
                particles[ip] = add_new_lm(particles[ip], z[:, iz], Q)
            # known landmark
            else:
                w = compute_weight(particles[ip], z[:, iz], Q)
                particles[ip].w *= w<N><N>
                particles[ip] = update_landmark(particles[ip], z[:, iz], Q)
                particles[ip] = proposal_sampling(particles[ip], z[:, iz], Q)<N><N>
    return particles<N><N>

def resampling(particles):
    """
    low variance re-sampling
    """<N><N>
    particles = normalize_weight(particles)<N><N>
    pw = []
    for i in range(N_PARTICLE):
        pw.append(particles[i].w)<N><N>
    pw = np.array(pw)<N><N>
    n_eff = 1.0 / (pw @ pw.T)  # Effective particle number<N><N>
    if n_eff < NTH:  # resampling
        w_cum = np.cumsum(pw)
        base = np.cumsum(pw * 0.0 + 1 / N_PARTICLE) - 1 / N_PARTICLE
        resample_id = base + np.random.rand(base.shape[0]) / N_PARTICLE<N><N>
        inds = []
        ind = 0
        for ip in range(N_PARTICLE):
            while (ind < w_cum.shape[0] - 1) \
                    and (resample_id[ip] > w_cum[ind]):
                ind += 1
            inds.append(ind)<N><N>
        tmp_particles = particles[:]
        for i in range(len(inds)):
            particles[i].x = tmp_particles[inds[i]].x
            particles[i].y = tmp_particles[inds[i]].y
            particles[i].yaw = tmp_particles[inds[i]].yaw
            particles[i].lm = tmp_particles[inds[i]].lm[:, :]
            particles[i].lmP = tmp_particles[inds[i]].lmP[:, :]
            particles[i].w = 1.0 / N_PARTICLE<N><N>
    return particles<N><N>

def calc_input(time):
    if time <= 3.0:  # wait at first
        v = 0.0
        yaw_rate = 0.0
    else:
        v = 1.0  # [m/s]
        yaw_rate = 0.1  # [rad/s]<N><N>
    u = np.array([v, yaw_rate]).reshape(2, 1)<N><N>
    return u<N><N>

def observation(xTrue, xd, u, RFID):
    # calc true state
    xTrue = motion_model(xTrue, u)<N><N>
    # add noise to range observation
    z = np.zeros((3, 0))<N><N>
    for i in range(len(RFID[:, 0])):<N><N>
        dx = RFID[i, 0] - xTrue[0, 0]
        dy = RFID[i, 1] - xTrue[1, 0]
        d = math.hypot(dx, dy)
        angle = pi_2_pi(math.atan2(dy, dx) - xTrue[2, 0])
        if d <= MAX_RANGE:
            dn = d + np.random.randn() * Q_sim[0, 0] ** 0.5  # add noise
            angle_noise = np.random.randn() * Q_sim[1, 1] ** 0.5
            angle_with_noise = angle + angle_noise  # add noise
            zi = np.array([dn, pi_2_pi(angle_with_noise), i]).reshape(3, 1)
            z = np.hstack((z, zi))<N><N>
    # add noise to input
    ud1 = u[0, 0] + np.random.randn() * R_sim[0, 0] ** 0.5
    ud2 = u[1, 0] + np.random.randn() * R_sim[
        1, 1] ** 0.5 + OFFSET_YAW_RATE_NOISE
    ud = np.array([ud1, ud2]).reshape(2, 1)<N><N>
    xd = motion_model(xd, ud)<N><N>
    return xTrue, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0],
                  [0, 1.0, 0],
                  [0, 0, 1.0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT]])<N><N>
    x = F @ x + B @ u<N><N>
    x[2, 0] = pi_2_pi(x[2, 0])<N><N>
    return x<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    time = 0.0<N><N>
    # RFID positions [x, y]
    RFID = np.array([[10.0, -2.0],
                     [15.0, 10.0],
                     [15.0, 15.0],
                     [10.0, 20.0],
                     [3.0, 15.0],
                     [-5.0, 20.0],
                     [-5.0, 5.0],
                     [-10.0, 15.0]
                     ])
    n_landmark = RFID.shape[0]<N><N>
    # State Vector [x y yaw v]'
    xEst = np.zeros((STATE_SIZE, 1))  # SLAM estimation
    xTrue = np.zeros((STATE_SIZE, 1))  # True state
    xDR = np.zeros((STATE_SIZE, 1))  # Dead reckoning<N><N>
    # history
    hxEst = xEst
    hxTrue = xTrue
    hxDR = xTrue<N><N>
    particles = [Particle(n_landmark) for _ in range(N_PARTICLE)]<N><N>
    while SIM_TIME >= time:
        time += DT
        u = calc_input(time)<N><N>
        xTrue, z, xDR, ud = observation(xTrue, xDR, u, RFID)<N><N>
        particles = fast_slam2(particles, ud, z)<N><N>
        xEst = calc_final_state(particles)<N><N>
        x_state = xEst[0: STATE_SIZE]<N><N>
        # store data history
        hxEst = np.hstack((hxEst, x_state))
        hxDR = np.hstack((hxDR, xDR))
        hxTrue = np.hstack((hxTrue, xTrue))<N><N>
        if show_animation:  # pragma: no cover
            plt.cla()
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
            plt.plot(RFID[:, 0], RFID[:, 1], "*k")<N><N>
            for iz in range(len(z[:, 0])):
                landmark_id = int(z[2, iz])
                plt.plot([xEst[0], RFID[landmark_id, 0]], [
                    xEst[1], RFID[landmark_id, 1]], "-k")<N><N>
            for i in range(N_PARTICLE):
                plt.plot(particles[i].x, particles[i].y, ".r")
                plt.plot(particles[i].lm[:, 0], particles[i].lm[:, 1], "xb")<N><N>
            plt.plot(hxTrue[0, :], hxTrue[1, :], "-b")
            plt.plot(hxDR[0, :], hxDR[1, :], "-k")
            plt.plot(hxEst[0, :], hxEst[1, :], "-r")
            plt.plot(xEst[0], xEst[1], "xk")
            plt.axis("equal")
            plt.grid(True)
            plt.pause(0.001)<N><N>

if __name__ == '__main__':
    main()
<N><N>
"""<N><N>Graph based SLAM example<N><N>
author: Atsushi Sakai (@Atsushi_twi)<N><N>
Ref<N><N>[A Tutorial on Graph-Based SLAM]
(http://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf)<N><N>
"""<N><N>import copy
import itertools
import math<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial.transform import Rotation as Rot<N><N>
#  Simulation parameter
Q_sim = np.diag([0.2, np.deg2rad(1.0)]) ** 2
R_sim = np.diag([0.1, np.deg2rad(10.0)]) ** 2<N><N>
DT = 2.0  # time tick [s]
SIM_TIME = 100.0  # simulation time [s]
MAX_RANGE = 30.0  # maximum observation range
STATE_SIZE = 3  # State size [x,y,yaw]<N><N>
# Covariance parameter of Graph Based SLAM
C_SIGMA1 = 0.1
C_SIGMA2 = 0.1
C_SIGMA3 = np.deg2rad(1.0)<N><N>
MAX_ITR = 20  # Maximum iteration<N><N>
show_graph_d_time = 20.0  # [s]
show_animation = True<N><N>

class Edge:<N><N>
    def __init__(self):
        self.e = np.zeros((3, 1))
        self.omega = np.zeros((3, 3))  # information matrix
        self.d1 = 0.0
        self.d2 = 0.0
        self.yaw1 = 0.0
        self.yaw2 = 0.0
        self.angle1 = 0.0
        self.angle2 = 0.0
        self.id1 = 0
        self.id2 = 0<N><N>

def cal_observation_sigma():
    sigma = np.zeros((3, 3))
    sigma[0, 0] = C_SIGMA1 ** 2
    sigma[1, 1] = C_SIGMA2 ** 2
    sigma[2, 2] = C_SIGMA3 ** 2<N><N>
    return sigma<N><N>

def calc_rotational_matrix(angle):
    return Rot.from_euler('z', angle).as_matrix()<N><N>

def calc_edge(x1, y1, yaw1, x2, y2, yaw2, d1,
              angle1, d2, angle2, t1, t2):
    edge = Edge()<N><N>
    tangle1 = pi_2_pi(yaw1 + angle1)
    tangle2 = pi_2_pi(yaw2 + angle2)
    tmp1 = d1 * math.cos(tangle1)
    tmp2 = d2 * math.cos(tangle2)
    tmp3 = d1 * math.sin(tangle1)
    tmp4 = d2 * math.sin(tangle2)<N><N>
    edge.e[0, 0] = x2 - x1 - tmp1 + tmp2
    edge.e[1, 0] = y2 - y1 - tmp3 + tmp4
    edge.e[2, 0] = 0<N><N>
    Rt1 = calc_rotational_matrix(tangle1)
    Rt2 = calc_rotational_matrix(tangle2)<N><N>
    sig1 = cal_observation_sigma()
    sig2 = cal_observation_sigma()<N><N>
    edge.omega = np.linalg.inv(Rt1 @ sig1 @ Rt1.T + Rt2 @ sig2 @ Rt2.T)<N><N>
    edge.d1, edge.d2 = d1, d2
    edge.yaw1, edge.yaw2 = yaw1, yaw2
    edge.angle1, edge.angle2 = angle1, angle2
    edge.id1, edge.id2 = t1, t2<N><N>
    return edge<N><N>

def calc_edges(x_list, z_list):
    edges = []
    cost = 0.0
    z_ids = list(itertools.combinations(range(len(z_list)), 2))<N><N>
    for (t1, t2) in z_ids:
        x1, y1, yaw1 = x_list[0, t1], x_list[1, t1], x_list[2, t1]
        x2, y2, yaw2 = x_list[0, t2], x_list[1, t2], x_list[2, t2]<N><N>
        if z_list[t1] is None or z_list[t2] is None:
            continue  # No observation<N><N>
        for iz1 in range(len(z_list[t1][:, 0])):
            for iz2 in range(len(z_list[t2][:, 0])):
                if z_list[t1][iz1, 3] == z_list[t2][iz2, 3]:
                    d1 = z_list[t1][iz1, 0]
                    angle1, phi1 = z_list[t1][iz1, 1], z_list[t1][iz1, 2]
                    d2 = z_list[t2][iz2, 0]
                    angle2, phi2 = z_list[t2][iz2, 1], z_list[t2][iz2, 2]<N><N>
                    edge = calc_edge(x1, y1, yaw1, x2, y2, yaw2, d1,
                                     angle1, d2, angle2, t1, t2)<N><N>
                    edges.append(edge)
                    cost += (edge.e.T @ edge.omega @ edge.e)[0, 0]<N><N>
    print("cost:", cost, ",n_edge:", len(edges))
    return edges<N><N>

def calc_jacobian(edge):
    t1 = edge.yaw1 + edge.angle1
    A = np.array([[-1.0, 0, edge.d1 * math.sin(t1)],
                  [0, -1.0, -edge.d1 * math.cos(t1)],
                  [0, 0, 0]])<N><N>
    t2 = edge.yaw2 + edge.angle2
    B = np.array([[1.0, 0, -edge.d2 * math.sin(t2)],
                  [0, 1.0, edge.d2 * math.cos(t2)],
                  [0, 0, 0]])<N><N>
    return A, B<N><N>

def fill_H_and_b(H, b, edge):
    A, B = calc_jacobian(edge)<N><N>
    id1 = edge.id1 * STATE_SIZE
    id2 = edge.id2 * STATE_SIZE<N><N>
    H[id1:id1 + STATE_SIZE, id1:id1 + STATE_SIZE] += A.T @ edge.omega @ A
    H[id1:id1 + STATE_SIZE, id2:id2 + STATE_SIZE] += A.T @ edge.omega @ B
    H[id2:id2 + STATE_SIZE, id1:id1 + STATE_SIZE] += B.T @ edge.omega @ A
    H[id2:id2 + STATE_SIZE, id2:id2 + STATE_SIZE] += B.T @ edge.omega @ B<N><N>
    b[id1:id1 + STATE_SIZE] += (A.T @ edge.omega @ edge.e)
    b[id2:id2 + STATE_SIZE] += (B.T @ edge.omega @ edge.e)<N><N>
    return H, b<N><N>

def graph_based_slam(x_init, hz):
    print("start graph based slam")<N><N>
    z_list = copy.deepcopy(hz)<N><N>
    x_opt = copy.deepcopy(x_init)
    nt = x_opt.shape[1]
    n = nt * STATE_SIZE<N><N>
    for itr in range(MAX_ITR):
        edges = calc_edges(x_opt, z_list)<N><N>
        H = np.zeros((n, n))
        b = np.zeros((n, 1))<N><N>
        for edge in edges:
            H, b = fill_H_and_b(H, b, edge)<N><N>
        # to fix origin
        H[0:STATE_SIZE, 0:STATE_SIZE] += np.identity(STATE_SIZE)<N><N>
        dx = - np.linalg.inv(H) @ b<N><N>
        for i in range(nt):
            x_opt[0:3, i] += dx[i * 3:i * 3 + 3, 0]<N><N>
        diff = dx.T @ dx
        print("iteration: %d, diff: %f" % (itr + 1, diff))
        if diff < 1.0e-5:
            break<N><N>
    return x_opt<N><N>

def calc_input():
    v = 1.0  # [m/s]
    yaw_rate = 0.1  # [rad/s]
    u = np.array([[v, yaw_rate]]).T
    return u<N><N>

def observation(xTrue, xd, u, RFID):
    xTrue = motion_model(xTrue, u)<N><N>
    # add noise to gps x-y
    z = np.zeros((0, 4))<N><N>
    for i in range(len(RFID[:, 0])):<N><N>
        dx = RFID[i, 0] - xTrue[0, 0]
        dy = RFID[i, 1] - xTrue[1, 0]
        d = math.hypot(dx, dy)
        angle = pi_2_pi(math.atan2(dy, dx)) - xTrue[2, 0]
        phi = pi_2_pi(math.atan2(dy, dx))
        if d <= MAX_RANGE:
            dn = d + np.random.randn() * Q_sim[0, 0]  # add noise
            angle_noise = np.random.randn() * Q_sim[1, 1]
            angle += angle_noise
            phi += angle_noise
            zi = np.array([dn, angle, phi, i])
            z = np.vstack((z, zi))<N><N>
    # add noise to input
    ud1 = u[0, 0] + np.random.randn() * R_sim[0, 0]
    ud2 = u[1, 0] + np.random.randn() * R_sim[1, 1]
    ud = np.array([[ud1, ud2]]).T<N><N>
    xd = motion_model(xd, ud)<N><N>
    return xTrue, z, xd, ud<N><N>

def motion_model(x, u):
    F = np.array([[1.0, 0, 0],
                  [0, 1.0, 0],
                  [0, 0, 1.0]])<N><N>
    B = np.array([[DT * math.cos(x[2, 0]), 0],
                  [DT * math.sin(x[2, 0]), 0],
                  [0.0, DT]])<N><N>
    x = F @ x + B @ u<N><N>
    return x<N><N>

def pi_2_pi(angle):
    return (angle + math.pi) % (2 * math.pi) - math.pi<N><N>

def main():
    print(__file__ + " start!!")<N><N>
    time = 0.0<N><N>
    # RFID positions [x, y, yaw]
    RFID = np.array([[10.0, -2.0, 0.0],
                     [15.0, 10.0, 0.0],
                     [3.0, 15.0, 0.0],
                     [-5.0, 20.0, 0.0],
                     [-5.0, 5.0, 0.0]
                     ])<N><N>
    # State Vector [x y yaw v]'
    xTrue = np.zeros((STATE_SIZE, 1))
    xDR = np.zeros((STATE_SIZE, 1))  # Dead reckoning<N><N>
    # history
    hxTrue = []
    hxDR = []
    hz = []
    d_time = 0.0
    init = False
    while SIM_TIME >= time:<N><N>
        if not init:
            hxTrue = xTrue
            hxDR = xTrue
            init = True
        else:
            hxDR = np.hstack((hxDR, xDR))
            hxTrue = np.hstack((hxTrue, xTrue))<N><N>
        time += DT
        d_time += DT
        u = calc_input()<N><N>
        xTrue, z, xDR, ud = observation(xTrue, xDR, u, RFID)<N><N>
        hz.append(z)<N><N>
        if d_time >= show_graph_d_time:
            x_opt = graph_based_slam(hxDR, hz)
            d_time = 0.0<N><N>
            if show_animation:  # pragma: no cover
                plt.cla()
                # for stopping simulation with the esc key.
                plt.gcf().canvas.mpl_connect(
                    'key_release_event',
                    lambda event: [exit(0) if event.key == 'escape' else None])
                plt.plot(RFID[:, 0], RFID[:, 1], "*k")<N><N>
                plt.plot(hxTrue[0, :].flatten(),
                         hxTrue[1, :].flatten(), "-b")
                plt.plot(hxDR[0, :].flatten(),
                         hxDR[1, :].flatten(), "-k")
                plt.plot(x_opt[0, :].flatten(),
                         x_opt[1, :].flatten(), "-r")
                plt.axis("equal")
                plt.grid(True)
                plt.title("Time" + str(time)[0:5])
                plt.pause(1.0)<N><N>

if __name__ == '__main__':
    main()
<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>
r"""A ``Graph`` class that stores the edges and vertices required for Graph SLAM.<N><N>
"""<N><N>import warnings
from collections import defaultdict
from functools import reduce<N><N>
import matplotlib.pyplot as plt
import numpy as np
from scipy.sparse import SparseEfficiencyWarning, lil_matrix
from scipy.sparse.linalg import spsolve<N><N>
warnings.simplefilter("ignore", SparseEfficiencyWarning)
warnings.filterwarnings("ignore", category=SparseEfficiencyWarning)<N><N>

# pylint: disable=too-few-public-methods
class _Chi2GradientHessian:
    r"""A class that is used to aggregate the :math:`\chi^2` error, gradient, and Hessian.<N><N>
    Parameters
    ----------
    dim : int
        The compact dimensionality of the poses<N><N>
    Attributes
    ----------
    chi2 : float
        The :math:`\chi^2` error
    dim : int
        The compact dimensionality of the poses
    gradient : defaultdict
        The contributions to the gradient vector
    hessian : defaultdict
        The contributions to the Hessian matrix<N><N>
    """<N><N>
    def __init__(self, dim):
        self.chi2 = 0.
        self.dim = dim
        self.gradient = defaultdict(lambda: np.zeros(dim))
        self.hessian = defaultdict(lambda: np.zeros((dim, dim)))<N><N>
    @staticmethod
    def update(chi2_grad_hess, incoming):
        r"""Update the :math:`\chi^2` error and the gradient and Hessian dictionaries.<N><N>
        Parameters
        ----------
        chi2_grad_hess : _Chi2GradientHessian
            The ``_Chi2GradientHessian`` that will be updated
        incoming : tuple<N><N>
        """
        chi2_grad_hess.chi2 += incoming[0]<N><N>
        for idx, contrib in incoming[1].items():
            chi2_grad_hess.gradient[idx] += contrib<N><N>
        for (idx1, idx2), contrib in incoming[2].items():
            if idx1 <= idx2:
                chi2_grad_hess.hessian[idx1, idx2] += contrib
            else:
                chi2_grad_hess.hessian[idx2, idx1] += np.transpose(contrib)<N><N>
        return chi2_grad_hess<N><N>

class Graph(object):
    r"""A graph that will be optimized via Graph SLAM.<N><N>
    Parameters
    ----------
    edges : list[graphslam.edge.edge_odometry.EdgeOdometry]
        A list of the vertices in the graph
    vertices : list[graphslam.vertex.Vertex]
        A list of the vertices in the graph<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>
"""Functions for loading graphs.<N><N>
"""<N><N>import logging<N><N>
import numpy as np<N><N>
from .edge.edge_odometry import EdgeOdometry
from .graph import Graph
from .pose.se2 import PoseSE2
from .util import upper_triangular_matrix_to_full_matrix
from .vertex import Vertex<N><N>
_LOGGER = logging.getLogger(__name__)<N><N>

def load_g2o_se2(infile):
    """Load an :math:`SE(2)` graph from a .g2o file.<N><N>
    Parameters
    ----------
    infile : str
        The path to the .g2o file<N><N>
    Returns
    -------
    Graph
        The loaded graph<N><N>
    """
    edges = []
    vertices = []<N><N>
    with open(infile) as f:
        for line in f.readlines():
            if line.startswith("VERTEX_SE2"):
                numbers = line[10:].split()
                arr = np.array([float(number) for number in numbers[1:]],
                               dtype=float)
                p = PoseSE2(arr[:2], arr[2])
                v = Vertex(int(numbers[0]), p)
                vertices.append(v)
                continue<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>
"""Utility functions used throughout the package.<N><N>
"""<N><N>import numpy as np<N><N>

TWO_PI = 2 * np.pi<N><N>

def neg_pi_to_pi(angle):
    r"""Normalize ``angle`` to be in :math:`[-\pi, \pi)`.<N><N>
    Parameters
    ----------
    angle : float
        An angle (in radians)<N><N>
    Returns
    -------
    float
        The angle normalized to :math:`[-\pi, \pi)`<N><N>
    """
    return (angle + np.pi) % (TWO_PI) - np.pi<N><N>

def solve_for_edge_dimensionality(n):
    r"""Solve for the dimensionality of an edge.<N><N>
    In a .g2o file, an edge is specified as ``<estimate> <information matrix>``, where only the upper triangular portion of the matrix is provided.<N><N>
    This solves the problem:<N><N>
    .. math::<N><N>
       d + \frac{d (d + 1)}{2} = n<N><N>
    Returns
    -------
    int
        The dimensionality of the edge<N><N>
    """
    return int(round(np.sqrt(2 * n + 2.25) - 1.5))<N><N>

def upper_triangular_matrix_to_full_matrix(arr, n):
    """Given an upper triangular matrix, return the full matrix.<N><N>
    Parameters
    ----------
    arr : np.ndarray
        The upper triangular portion of the matrix
    n : int
        The size of the matrix<N><N>
    Returns
    -------
    mat : np.ndarray
        The full matrix<N><N>
    """
    triu0 = np.triu_indices(n, 0)
    triu1 = np.triu_indices(n, 1)
    tril1 = np.tril_indices(n, -1)<N><N>
    mat = np.zeros((n, n), dtype=float)
    mat[triu0] = arr
    mat[tril1] = mat[triu1]<N><N>
    return mat
<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>
"""A ``Vertex`` class.<N><N>
"""<N><N>import matplotlib.pyplot as plt<N><N>

# pylint: disable=too-few-public-methods
class Vertex:
    """A class for representing a vertex in Graph SLAM.<N><N>
    Parameters
    ----------
    vertex_id : int
        The vertex's unique ID
    pose : graphslam.pose.se2.PoseSE2
        The pose associated with the vertex
    vertex_index : int, None
        The vertex's index in the graph's ``vertices`` list<N><N>
    Attributes
    ----------
    id : int
        The vertex's unique ID
    index : int, None
        The vertex's index in the graph's ``vertices`` list
    pose : graphslam.pose.se2.PoseSE2
        The pose associated with the vertex<N><N>
    """
    def __init__(self, vertex_id, pose, vertex_index=None):
        self.id = vertex_id
        self.pose = pose
        self.index = vertex_index<N><N>
    def to_g2o(self):
        """Export the vertex to the .g2o format.<N><N>
        Returns
        -------
        str
            The vertex in .g2o format<N><N>
        """
        return "VERTEX_SE2 {} {} {} {}\n".format(self.id, self.pose[0], self.pose[1], self.pose[2])<N><N>
    def plot(self, color='r', marker='o', markersize=3):
        """Plot the vertex.<N><N>
        Parameters
        ----------
        color : str
            The color that will be used to plot the vertex
        marker : str
            The marker that will be used to plot the vertex
        markersize : int
            The size of the plotted vertex<N><N>
        """
        x, y = self.pose.position
        plt.plot(x, y, color=color, marker=marker, markersize=markersize)
<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>"""Graph SLAM solver in Python.<N><N>"""

# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>
r"""A class for odometry edges.<N><N>
"""<N><N>
import numpy as np
import matplotlib.pyplot as plt<N><N>

#: The difference that will be used for numerical differentiation
EPSILON = 1e-6<N><N>

class EdgeOdometry:
    r"""A class for representing odometry edges in Graph SLAM.<N><N>
    Parameters
    ----------
    vertices : list[graphslam.vertex.Vertex]
        A list of the vertices constrained by the edge
    information : np.ndarray
        The information matrix :math:`\Omega_j` associated with the edge
    estimate : graphslam.pose.se2.PoseSE2
        The expected measurement :math:`\mathbf{z}_j`<N><N>
    Attributes
    ----------
    vertices : list[graphslam.vertex.Vertex]
        A list of the vertices constrained by the edge
    information : np.ndarray
        The information matrix :math:`\Omega_j` associated with the edge
    estimate : PoseSE2
        The expected measurement :math:`\mathbf{z}_j`<N><N>
    """
    def __init__(self, vertex_ids, information, estimate, vertices=None):
        self.vertex_ids = vertex_ids
        self.information = information
        self.estimate = estimate
        self.vertices = vertices<N><N>
    def calc_error(self):
        r"""Calculate the error for the edge: :math:`\mathbf{e}_j \in \mathbb{R}^\bullet`.<N><N>
        .. math::<N><N>
           \mathbf{e}_j = \mathbf{z}_j - (p_2 \ominus p_1)<N><N>

        Returns
        -------
        np.ndarray
            The error for the edge<N><N>
        """
        return (self.estimate - (self.vertices[1].pose - self.vertices[0].pose)).to_compact()<N><N>
    def calc_chi2(self):
        r"""Calculate the :math:`\chi^2` error for the edge.<N><N>
        .. math::<N><N>
           \mathbf{e}_j^T \Omega_j \mathbf{e}_j<N><N>

        Returns
        -------
        float
            The :math:`\chi^2` error for the edge<N><N>
        """
        err = self.calc_error()<N><N>
        return np.dot(np.dot(np.transpose(err), self.information), err)<N><N>
    def calc_chi2_gradient_hessian(self):
        r"""Calculate the edge's contributions to the graph's :math:`\chi^2` error, gradient (:math:`\mathbf{b}`), and Hessian (:math:`H`).<N><N>
        Returns
        -------
        float
            The :math:`\chi^2` error for the edge
        dict
            The edge's contribution(s) to the gradient
        dict
            The edge's contribution(s) to the Hessian<N><N>
        """
        chi2 = self.calc_chi2()<N><N>
        err = self.calc_error()<N><N>
        jacobians = self.calc_jacobians()<N><N>
        return chi2, {v.index: np.dot(np.dot(np.transpose(err), self.information), jacobian) for v, jacobian in zip(self.vertices, jacobians)}, {(self.vertices[i].index, self.vertices[j].index): np.dot(np.dot(np.transpose(jacobians[i]), self.information), jacobians[j]) for i in range(len(jacobians)) for j in range(i, len(jacobians))}<N><N>
    def calc_jacobians(self):
        r"""Calculate the Jacobian of the edge's error with respect to each constrained pose.<N><N>
        .. math::<N><N>
           \frac{\partial}{\partial \Delta \mathbf{x}^k} \left[ \mathbf{e}_j(\mathbf{x}^k \boxplus \Delta \mathbf{x}^k) \right]<N><N>

        Returns
        -------
        list[np.ndarray]
            The Jacobian matrices for the edge with respect to each constrained pose<N><N>
        """
        err = self.calc_error()<N><N>
        # The dimensionality of the compact pose representation
        dim = len(self.vertices[0].pose.to_compact())<N><N>
        return [self._calc_jacobian(err, dim, i) for i in range(len(self.vertices))]<N><N>
    def _calc_jacobian(self, err, dim, vertex_index):
        r"""Calculate the Jacobian of the edge with respect to the specified vertex's pose.<N><N>
        Parameters
        ----------
        err : np.ndarray
            The current error for the edge (see :meth:`EdgeOdometry.calc_error`)
        dim : int
            The dimensionality of the compact pose representation
        vertex_index : int
            The index of the vertex (pose) for which we are computing the Jacobian<N><N>
        Returns
        -------
        np.ndarray
            The Jacobian of the edge with respect to the specified vertex's pose<N><N>
        """
        jacobian = np.zeros(err.shape + (dim,))
        p0 = self.vertices[vertex_index].pose.copy()<N><N>
        for d in range(dim):
            # update the pose
            delta_pose = np.zeros(dim)
            delta_pose[d] = EPSILON
            self.vertices[vertex_index].pose += delta_pose<N><N>
            # compute the numerical derivative
            jacobian[:, d] = (self.calc_error() - err) / EPSILON<N><N>
            # restore the pose
            self.vertices[vertex_index].pose = p0.copy()<N><N>
        return jacobian<N><N>
    def to_g2o(self):
        """Export the edge to the .g2o format.<N><N>
        Returns
        -------
        str
            The edge in .g2o format<N><N>
        """
        return "EDGE_SE2 {} {} {} {} {} ".format(self.vertex_ids[0], self.vertex_ids[1], self.estimate[0], self.estimate[1], self.estimate[2]) + " ".join([str(x) for x in self.information[np.triu_indices(3, 0)]]) + "\n"<N><N>
    def plot(self, color='b'):
        """Plot the edge.<N><N>
        Parameters
        ----------
        color : str
            The color that will be used to plot the edge<N><N>
        """
        xy = np.array([v.pose.position for v in self.vertices])
        plt.plot(xy[:, 0], xy[:, 1], color=color)
<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam

# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam<N><N>
r"""Representation of a pose in :math:`SE(2)`.<N><N>
"""<N><N>import math
import numpy as np<N><N>
from ..util import neg_pi_to_pi<N><N>

class PoseSE2(np.ndarray):
    r"""A representation of a pose in :math:`SE(2)`.<N><N>
    Parameters
    ----------
    position : np.ndarray, list
        The position in :math:`\mathbb{R}^2`
    orientation : float
        The angle of the pose (in radians)<N><N>
    """<N><N>
    def __new__(cls, position, orientation):
        obj = np.array([position[0], position[1], neg_pi_to_pi(orientation)],
                       dtype=float).view(cls)
        return obj<N><N>
    # pylint: disable=arguments-differ
    def copy(self):
        """Return a copy of the pose.<N><N>
        Returns
        -------
        PoseSE2
            A copy of the pose<N><N>
        """
        return PoseSE2(self[:2], self[2])<N><N>
    def to_array(self):
        """Return the pose as a numpy array.<N><N>
        Returns
        -------
        np.ndarray
            The pose as a numpy array<N><N>
        """
        return np.array(self)<N><N>
    def to_compact(self):
        """Return the pose as a compact numpy array.<N><N>
        Returns
        -------
        np.ndarray
            The pose as a compact numpy array<N><N>
        """
        return np.array(self)<N><N>
    def to_matrix(self):
        """Return the pose as an :math:`SE(2)` matrix.<N><N>
        Returns
        -------
        np.ndarray
            The pose as an :math:`SE(2)` matrix<N><N>
        """
        return np.array([[np.cos(self[2]), -np.sin(self[2]), self[0]],
                         [np.sin(self[2]), np.cos(self[2]), self[1]],
                         [0., 0., 1.]], dtype=float)<N><N>
    @classmethod
    def from_matrix(cls, matrix):
        """Return the pose as an :math:`SE(2)` matrix.<N><N>
        Parameters
        ----------
        matrix : np.ndarray
            The :math:`SE(2)` matrix that will be converted to a `PoseSE2` instance<N><N>
        Returns
        -------
        PoseSE2
            The matrix as a `PoseSE2` object<N><N>
        """
        return cls([matrix[0, 2], matrix[1, 2]],
                   math.atan2(matrix[1, 0], matrix[0, 0]))<N><N>
    # ======================================================================= #
    #                                                                         #
    #                                Properties                               #
    #                                                                         #
    # ======================================================================= #
    @property
    def position(self):
        """Return the pose's position.<N><N>
        Returns
        -------
        np.ndarray
            The position portion of the pose<N><N>
        """
        return np.array(self[:2])<N><N>
    @property
    def orientation(self):
        """Return the pose's orientation.<N><N>
        Returns
        -------
        float
            The angle of the pose<N><N>
        """
        return self[2]<N><N>
    @property
    def inverse(self):
        """Return the pose's inverse.<N><N>
        Returns
        -------
        PoseSE2
            The pose's inverse<N><N>
        """
        return PoseSE2([-self[0] * np.cos(self[2]) - self[1] * np.sin(self[2]),
                        self[0] * np.sin(self[2]) - self[1] * np.cos(
                            [self[2]])], -self[2])<N><N>
    # ======================================================================= #
    #                                                                         #
    #                              Magic Methods                              #
    #                                                                         #
    # ======================================================================= #
    def __add__(self, other):
        r"""Add poses (i.e., pose composition): :math:`p_1 \oplus p_2`.<N><N>
        Parameters
        ----------
        other : PoseSE2
            The other pose<N><N>
        Returns
        -------
        PoseSE2
            The result of pose composition<N><N>
        """
        return PoseSE2(
            [self[0] + other[0] * np.cos(self[2]) - other[1] * np.sin(self[2]),
             self[1] + other[0] * np.sin(self[2]) + other[1] * np.cos(self[2])
             ], neg_pi_to_pi(self[2] + other[2]))<N><N>
    def __sub__(self, other):
        r"""Subtract poses (i.e., inverse pose composition): :math:`p_1 \ominus p_2`.<N><N>
        Parameters
        ----------
        other : PoseSE2
            The other pose<N><N>
        Returns
        -------
        PoseSE2
            The result of inverse pose composition<N><N>
        """
        return PoseSE2([(self[0] - other[0]) * np.cos(other[2]) + (
                    self[1] - other[1]) * np.sin(other[2]),
                        (other[0] - self[0]) * np.sin(other[2]) + (
                                    self[1] - other[1]) * np.cos(other[2])],
                       neg_pi_to_pi(self[2] - other[2]))
<N><N>
# Copyright (c) 2020 Jeff Irion and contributors
#
# This file originated from the `graphslam` package:
#
#   https://github.com/JeffLIrion/python-graphslam

"""
Iterative Closest Point (ICP) SLAM example
author: Atsushi Sakai (@Atsushi_twi), Göktuğ Karakaşlı, Shamil Gemuev
"""<N><N>
import math<N><N>
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
import matplotlib.pyplot as plt
import numpy as np<N><N>
#  ICP parameters
EPS = 0.0001
MAX_ITER = 100<N><N>
show_animation = True<N><N>

def icp_matching(previous_points, current_points):
    """
    Iterative Closest Point matching
    - input
    previous_points: 2D or 3D points in the previous frame
    current_points: 2D or 3D points in the current frame
    - output
    R: Rotation matrix
    T: Translation vector
    """
    H = None  # homogeneous transformation matrix<N><N>
    dError = np.inf
    preError = np.inf
    count = 0<N><N>
    if show_animation:
        fig = plt.figure()
        if previous_points.shape[0] == 3:
           fig.add_subplot(111, projection='3d')<N><N>
    while dError >= EPS:
        count += 1<N><N>
        if show_animation:  # pragma: no cover
            plot_points(previous_points, current_points, fig)
            plt.pause(0.1)<N><N>
        indexes, error = nearest_neighbor_association(previous_points, current_points)
        Rt, Tt = svd_motion_estimation(previous_points[:, indexes], current_points)
        # update current points
        current_points = (Rt @ current_points) + Tt[:, np.newaxis]<N><N>
        dError = preError - error
        print("Residual:", error)<N><N>
        if dError < 0:  # prevent matrix H changing, exit loop
            print("Not Converge...", preError, dError, count)
            break<N><N>
        preError = error
        H = update_homogeneous_matrix(H, Rt, Tt)<N><N>
        if dError <= EPS:
            print("Converge", error, dError, count)
            break
        elif MAX_ITER <= count:
            print("Not Converge...", error, dError, count)
            break<N><N>
    R = np.array(H[0:-1, 0:-1])
    T = np.array(H[0:-1, -1])<N><N>
    return R, T<N><N>

def update_homogeneous_matrix(Hin, R, T):<N><N>
    r_size = R.shape[0]
    H = np.zeros((r_size + 1, r_size + 1))<N><N>
    H[0:r_size, 0:r_size] = R
    H[0:r_size, r_size] = T
    H[r_size, r_size] = 1.0<N><N>
    if Hin is None:
        return H
    else:
        return Hin @ H<N><N>

def nearest_neighbor_association(previous_points, current_points):<N><N>
    # calc the sum of residual errors
    delta_points = previous_points - current_points
    d = np.linalg.norm(delta_points, axis=0)
    error = sum(d)<N><N>
    # calc index with nearest neighbor assosiation
    d = np.linalg.norm(np.repeat(current_points, previous_points.shape[1], axis=1)
                       - np.tile(previous_points, (1, current_points.shape[1])), axis=0)
    indexes = np.argmin(d.reshape(current_points.shape[1], previous_points.shape[1]), axis=1)<N><N>
    return indexes, error<N><N>

def svd_motion_estimation(previous_points, current_points):
    pm = np.mean(previous_points, axis=1)
    cm = np.mean(current_points, axis=1)<N><N>
    p_shift = previous_points - pm[:, np.newaxis]
    c_shift = current_points - cm[:, np.newaxis]<N><N>
    W = c_shift @ p_shift.T
    u, s, vh = np.linalg.svd(W)<N><N>
    R = (u @ vh).T
    t = pm - (R @ cm)<N><N>
    return R, t<N><N>
"""Path hack to make tests work."""
import sys
import os
import pytest<N><N>TEST_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(TEST_DIR)  # to import this file from test code.
ROOT_DIR = os.path.dirname(TEST_DIR)
sys.path.append(ROOT_DIR)<N><N>
def run_this_test(file):
    pytest.main([os.path.abspath(file)])

import conftest
from PathPlanning.AStar import a_star as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.AStar import a_star_searching_from_two_side as m<N><N>
def test1():
    m.show_animation = False
    m.main(800)<N><N>
def test2():
    m.show_animation = False
    m.main(5000)  # increase obstacle number, block path<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import PathPlanning.AStar.a_star_variants as a_star
import conftest<N><N>

def test_1():
    # A* with beam search
    a_star.show_animation = False<N><N>
    a_star.use_beam_search = True
    a_star.main()
    reset_all()<N><N>
    # A* with iterative deepening
    a_star.use_iterative_deepening = True
    a_star.main()
    reset_all()<N><N>
    # A* with dynamic weighting
    a_star.use_dynamic_weighting = True
    a_star.main()
    reset_all()<N><N>
    # theta*
    a_star.use_theta_star = True
    a_star.main()
    reset_all()<N><N>
    # A* with jump point
    a_star.use_jump_point = True
    a_star.main()
    reset_all()<N><N>

def reset_all():
    a_star.show_animation = False
    a_star.use_beam_search = False
    a_star.use_iterative_deepening = False
    a_star.use_dynamic_weighting = False
    a_star.use_theta_star = False
    a_star.use_jump_point = False<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
import random<N><N>import conftest
from PathPlanning.BatchInformedRRTStar import batch_informed_rrtstar as m<N><N>
def test_1():
    m.show_animation = False
    random.seed(12345)
    m.main(maxIter=10)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.BezierPath import bezier_path as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
def test_2():
    m.show_animation = False
    m.main2()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from Bipedal.bipedal_planner import bipedal_planner as m<N><N>
def test_1():
    bipedal_planner = m.BipedalPlanner()<N><N>    footsteps = [[0.0, 0.2, 0.0],
                 [0.3, 0.2, 0.0],
                 [0.3, 0.2, 0.2],
                 [0.3, 0.2, 0.2],
                 [0.0, 0.2, 0.2]]
    bipedal_planner.set_ref_footsteps(footsteps)
    bipedal_planner.walk(plot=False)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.BreadthFirstSearch import breadth_first_search as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.BugPlanning import bug as m<N><N>
def test_1():
    m.show_animation = False
    m.main(bug_0=True, bug_1=True, bug_2=True)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathTracking.cgmres_nmpc import cgmres_nmpc as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from Mapping.circle_fitting import circle_fitting as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.ClosedLoopRRTStar import closed_loop_rrt_star_car as m
import random<N><N>
def test_1():
    random.seed(12345)
    m.show_animation = False
    m.main(gx=1.0, gy=0.0, gyaw=0.0, max_iter=5)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from Localization.cubature_kalman_filter import cubature_kalman_filter as m<N><N>
def test1():
    m.show_final = False
    m.show_animation = False
    m.show_ellipse = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.DepthFirstSearch import depth_first_search as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.Dijkstra import dijkstra as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from AerialNavigation.drone_3d_trajectory_following \
    import drone_3d_trajectory_following as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.DStar import dstar as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
import numpy as np
from PathPlanning.DubinsPath import dubins_path_planning<N><N>
np.random.seed(12345)<N><N>

def check_edge_condition(px, py, pyaw, start_x, start_y, start_yaw,
                         end_x, end_y, end_yaw):
    assert (abs(px[0] - start_x) <= 0.01)
    assert (abs(py[0] - start_y) <= 0.01)
    assert (abs(pyaw[0] - start_yaw) <= 0.01)
    assert (abs(px[-1] - end_x) <= 0.01)
    assert (abs(py[-1] - end_y) <= 0.01)
    assert (abs(pyaw[-1] - end_yaw) <= 0.01)<N><N>

def test_1():
    start_x = 1.0  # [m]
    start_y = 1.0  # [m]
    start_yaw = np.deg2rad(45.0)  # [rad]<N><N>
    end_x = -3.0  # [m]
    end_y = -3.0  # [m]
    end_yaw = np.deg2rad(-45.0)  # [rad]<N><N>
    curvature = 1.0<N><N>
    px, py, pyaw, mode, clen = dubins_path_planning.dubins_path_planning(
        start_x, start_y, start_yaw, end_x, end_y, end_yaw, curvature)<N><N>
    check_edge_condition(px, py, pyaw,
                         start_x, start_y, start_yaw,
                         end_x, end_y, end_yaw)<N><N>

def test_2():
    dubins_path_planning.show_animation = False
    dubins_path_planning.main()<N><N>

def test_3():
    N_TEST = 10<N><N>
    for i in range(N_TEST):
        start_x = (np.random.rand() - 0.5) * 10.0  # [m]
        start_y = (np.random.rand() - 0.5) * 10.0  # [m]
        start_yaw = np.deg2rad((np.random.rand() - 0.5) * 180.0)  # [rad]<N><N>
        end_x = (np.random.rand() - 0.5) * 10.0  # [m]
        end_y = (np.random.rand() - 0.5) * 10.0  # [m]
        end_yaw = np.deg2rad((np.random.rand() - 0.5) * 180.0)  # [rad]<N><N>
        curvature = 1.0 / (np.random.rand() * 5.0)<N><N>
        px, py, pyaw, mode, clen = dubins_path_planning.dubins_path_planning(
            start_x, start_y, start_yaw, end_x, end_y, end_yaw, curvature)<N><N>
        check_edge_condition(px, py, pyaw,
                             start_x, start_y, start_yaw,
                             end_x, end_y, end_yaw)<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
import conftest
import numpy as np<N><N>
from PathPlanning.DynamicWindowApproach import dynamic_window_approach as m<N><N>

def test_main1():
    m.show_animation = False
    m.main(gx=1.0, gy=1.0)<N><N>

def test_main2():
    m.show_animation = False
    m.main(gx=1.0, gy=1.0, robot_type=m.RobotType.rectangle)<N><N>
import conftest
from PathPlanning.DStarLite import d_star_lite as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from SLAM.EKFSLAM import ekf_slam as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.Eta3SplinePath import eta3_spline_path as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from Localization.extended_kalman_filter import extended_kalman_filter as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from SLAM.FastSLAM1 import fast_slam1 as m<N><N>
def test1():
    m.show_animation = False
    m.SIM_TIME = 3.0
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from SLAM.FastSLAM2 import fast_slam2 as m<N><N>
def test1():
    m.show_animation = False
    m.SIM_TIME = 3.0
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
import PathPlanning.FlowField.flowfield as flow_field<N><N>
def test():
    flow_field.show_animation = False
    flow_field.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.FrenetOptimalTrajectory import frenet_optimal_trajectory as m<N><N>
def test1():
    m.show_animation = False
    m.SIM_LOOP = 5
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from Mapping.gaussian_grid_map import gaussian_grid_map as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from SLAM.GraphBasedSLAM import graph_based_slam as m<N><N>
def test_1():
    m.show_animation = False
    m.SIM_TIME = 20.0
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.GreedyBestFirstSearch import greedy_best_first_search as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.GridBasedSweepCPP \
    import grid_based_sweep_coverage_path_planner<N><N>
grid_based_sweep_coverage_path_planner.do_animation = False
RIGHT = grid_based_sweep_coverage_path_planner. \
    SweepSearcher.MovingDirection.RIGHT
LEFT = grid_based_sweep_coverage_path_planner. \
    SweepSearcher.MovingDirection.LEFT
UP = grid_based_sweep_coverage_path_planner. \
    SweepSearcher.SweepDirection.UP
DOWN = grid_based_sweep_coverage_path_planner. \
    SweepSearcher.SweepDirection.DOWN<N><N>

def test_planning1():
    ox = [0.0, 20.0, 50.0, 100.0, 130.0, 40.0, 0.0]
    oy = [0.0, -20.0, 0.0, 30.0, 60.0, 80.0, 0.0]
    resolution = 5.0<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=LEFT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=UP,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=UP,
    )
    assert len(px) >= 5<N><N>

def test_planning2():
    ox = [0.0, 50.0, 50.0, 0.0, 0.0]
    oy = [0.0, 0.0, 30.0, 30.0, 0.0]
    resolution = 1.3<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=LEFT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=UP,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>

def test_planning3():
    ox = [0.0, 20.0, 50.0, 200.0, 130.0, 40.0, 0.0]
    oy = [0.0, -80.0, 0.0, 30.0, 60.0, 80.0, 0.0]
    resolution = 5.1
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=LEFT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=UP,
    )
    assert len(px) >= 5<N><N>
    px, py = grid_based_sweep_coverage_path_planner.planning(
        ox, oy, resolution,
        moving_direction=RIGHT,
        sweeping_direction=DOWN,
    )
    assert len(px) >= 5<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
from Mapping.grid_map_lib.grid_map_lib import GridMap<N><N>

def test_position_set():
    grid_map = GridMap(100, 120, 0.5, 10.0, -0.5)<N><N>
    grid_map.set_value_from_xy_pos(10.1, -1.1, 1.0)
    grid_map.set_value_from_xy_pos(10.1, -0.1, 1.0)
    grid_map.set_value_from_xy_pos(10.1, 1.1, 1.0)
    grid_map.set_value_from_xy_pos(11.1, 0.1, 1.0)
    grid_map.set_value_from_xy_pos(10.1, 0.1, 1.0)
    grid_map.set_value_from_xy_pos(9.1, 0.1, 1.0)<N><N>

def test_polygon_set():
    ox = [0.0, 20.0, 50.0, 100.0, 130.0, 40.0]
    oy = [0.0, -20.0, 0.0, 30.0, 60.0, 80.0]<N><N>
    grid_map = GridMap(600, 290, 0.7, 60.0, 30.5)<N><N>
    grid_map.set_value_from_polygon(ox, oy, 1.0, inside=False)<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
import conftest
from Localization.histogram_filter import histogram_filter as m<N><N>
def test1():
    m.show_animation = False
    m.SIM_TIME = 1.0
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.HybridAStar import hybrid_a_star as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from PathPlanning.InformedRRTStar import informed_rrt_star as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
import sys
if 'cvxpy' in sys.modules:  # pragma: no cover<N><N>    from InvertedPendulumCart.InvertedPendulumMPCControl \
        import inverted_pendulum_mpc_control as m<N><N>    def test1():
        m.show_animation = False
        m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from SLAM.iterative_closest_point import iterative_closest_point as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
def test_2():
    m.show_animation = False
    m.main_3d_points()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
from Mapping.kmeans_clustering import kmeans_clustering as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.LQRPlanner import LQRplanner as m<N><N>
def test_1():
    m.SHOW_ANIMATION = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.LQRRRTStar import lqr_rrt_star as m
import random<N><N>random.seed(12345)<N><N>
def test1():
    m.show_animation = False
    m.main(maxIter=5)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathTracking.lqr_speed_steer_control import lqr_speed_steer_control as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathTracking.lqr_steer_control import lqr_steer_control as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import sys<N><N>if 'cvxpy' in sys.modules:  # pragma: no cover<N><N>    from PathTracking.model_predictive_speed_and_steer_control \
        import model_predictive_speed_and_steer_control as m<N><N>    def test_1():
        m.show_animation = False
        m.main()<N><N>    def test_2():
        m.show_animation = False
        m.main2()<N><N>if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathTracking.move_to_pose import move_to_pose as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from ArmNavigation.n_joint_arm_to_point_control\
    import n_joint_arm_to_point_control as m
import random<N><N>random.seed(12345)<N><N>
def test1():
    m.show_animation = False
    m.animation()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from Localization.particle_filter import particle_filter as m<N><N>
def test_1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.PotentialFieldPlanning import potential_field_planning as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.ProbabilisticRoadMap import probabilistic_road_map<N><N>
def test1():
    probabilistic_road_map.show_animation = False
    probabilistic_road_map.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathTracking.pure_pursuit import pure_pursuit as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.QuinticPolynomialsPlanner import quintic_polynomials_planner as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from Mapping.raycasting_grid_map import raycasting_grid_map as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathTracking.rear_wheel_feedback import rear_wheel_feedback as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from Mapping.rectangle_fitting import rectangle_fitting as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.ReedsSheppPath import reeds_shepp_path_planning as m
import numpy as np<N><N>

def check_edge_condition(px, py, pyaw, start_x, start_y, start_yaw, end_x,
                         end_y, end_yaw):
    assert (abs(px[0] - start_x) <= 0.01)
    assert (abs(py[0] - start_y) <= 0.01)
    assert (abs(pyaw[0] - start_yaw) <= 0.01)
    print("x", px[-1], end_x)
    assert (abs(px[-1] - end_x) <= 0.01)
    print("y", py[-1], end_y)
    assert (abs(py[-1] - end_y) <= 0.01)
    print("yaw", pyaw[-1], end_yaw)
    assert (abs(pyaw[-1] - end_yaw) <= 0.01)<N><N>

def test1():
    m.show_animation = False
    m.main()<N><N>

def test2():
    N_TEST = 10<N><N>
    for i in range(N_TEST):
        start_x = (np.random.rand() - 0.5) * 10.0  # [m]
        start_y = (np.random.rand() - 0.5) * 10.0  # [m]
        start_yaw = np.deg2rad((np.random.rand() - 0.5) * 180.0)  # [rad]<N><N>
        end_x = (np.random.rand() - 0.5) * 10.0  # [m]
        end_y = (np.random.rand() - 0.5) * 10.0  # [m]
        end_yaw = np.deg2rad((np.random.rand() - 0.5) * 180.0)  # [rad]<N><N>
        curvature = 1.0 / (np.random.rand() * 5.0)<N><N>
        px, py, pyaw, mode, clen = m.reeds_shepp_path_planning(
            start_x, start_y, start_yaw, end_x, end_y, end_yaw, curvature)<N><N>
        check_edge_condition(px, py, pyaw, start_x, start_y, start_yaw,
                             end_x, end_y, end_yaw)<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
import conftest  # Add root path to sys.path
import sys<N><N>if 'cvxpy' in sys.modules:  # pragma: no cover<N><N>    from AerialNavigation.rocket_powered_landing import rocket_powered_landing as m<N><N>    def test1():
        m.show_animation = False
        m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest
import random<N><N>from PathPlanning.RRT import rrt as m
from PathPlanning.RRT import rrt_with_pathsmoothing as m1<N><N>random.seed(12345)<N><N>
def test1():
    m.show_animation = False
    m.main(gx=1.0, gy=1.0)<N><N>
def test2():
    m1.show_animation = False
    m1.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.RRTDubins import rrt_dubins as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.RRTStar import rrt_star as m<N><N>

def test1():
    m.show_animation = False
    m.main()<N><N>

def test_no_obstacle():
    obstacle_list = []<N><N>
    # Set Initial parameters
    rrt_star = m.RRTStar(start=[0, 0],
                         goal=[6, 10],
                         rand_area=[-2, 15],
                         obstacle_list=obstacle_list)
    path = rrt_star.planning(animation=False)
    assert path is not None<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
import conftest  # Add root path to sys.path
from PathPlanning.RRTStarDubins import rrt_star_dubins as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
import rrt_star_reeds_shepp as m<N><N>
def test1():
    m.show_animation = False
    m.main(max_iter=5)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from ArmNavigation.rrt_star_seven_joint_arm_control \
    import rrt_star_seven_joint_arm_control as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.RRT import rrt_with_sobol_sampler as m
import random<N><N>random.seed(12345)<N><N>
def test1():
    m.show_animation = False
    m.main(gx=1.0, gy=1.0)<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
import os
import matplotlib.pyplot as plt
from PathPlanning.SpiralSpanningTreeCPP \
    import spiral_spanning_tree_coverage_path_planner<N><N>
spiral_spanning_tree_coverage_path_planner.do_animation = True<N><N>

def spiral_stc_cpp(img, start):
    num_free = 0
    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            num_free += img[i][j]<N><N>
    STC_planner = spiral_spanning_tree_coverage_path_planner.\
        SpiralSpanningTreeCoveragePlanner(img)<N><N>
    edge, route, path = STC_planner.plan(start)<N><N>
    covered_nodes = set()
    for p, q in edge:
        covered_nodes.add(p)
        covered_nodes.add(q)<N><N>
    # assert complete coverage
    assert len(covered_nodes) == num_free / 4<N><N>

def test_spiral_stc_cpp_1():
    img_dir = os.path.dirname(
        os.path.abspath(__file__)) + \
        "/../PathPlanning/SpiralSpanningTreeCPP"
    img = plt.imread(os.path.join(img_dir, 'map', 'test.png'))
    start = (0, 0)
    spiral_stc_cpp(img, start)<N><N>

def test_spiral_stc_cpp_2():
    img_dir = os.path.dirname(
        os.path.abspath(__file__)) + \
        "/../PathPlanning/SpiralSpanningTreeCPP"
    img = plt.imread(os.path.join(img_dir, 'map', 'test_2.png'))
    start = (10, 0)
    spiral_stc_cpp(img, start)<N><N>

def test_spiral_stc_cpp_3():
    img_dir = os.path.dirname(
        os.path.abspath(__file__)) + \
        "/../PathPlanning/SpiralSpanningTreeCPP"
    img = plt.imread(os.path.join(img_dir, 'map', 'test_3.png'))
    start = (0, 0)
    spiral_stc_cpp(img, start)<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
import conftest  # Add root path to sys.path
from PathTracking.stanley_controller import stanley_controller as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.StateLatticePlanner import state_lattice_planner as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from ArmNavigation.two_joint_arm_to_point_control \
    import two_joint_arm_to_point_control as m<N><N>
def test1():
    m.show_animation = False
    m.animation()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from Localization.unscented_kalman_filter import unscented_kalman_filter as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.VoronoiRoadMap import voronoi_road_map as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
from PathPlanning.VisibilityRoadMap import visibility_road_map as m<N><N>
def test1():
    m.show_animation = False
    m.main()<N><N>
if __name__ == '__main__':
    conftest.run_this_test(__file__)

import conftest  # Add root path to sys.path
import os
import matplotlib.pyplot as plt
from PathPlanning.WavefrontCPP import wavefront_coverage_path_planner<N><N>
wavefront_coverage_path_planner.do_animation = False<N><N>

def wavefront_cpp(img, start, goal):
    num_free = 0
    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            num_free += 1 - img[i][j]<N><N>
    DT = wavefront_coverage_path_planner.transform(
        img, goal, transform_type='distance')
    DT_path = wavefront_coverage_path_planner.wavefront(DT, start, goal)
    assert len(DT_path) == num_free  # assert complete coverage<N><N>
    PT = wavefront_coverage_path_planner.transform(
        img, goal, transform_type='path', alpha=0.01)
    PT_path = wavefront_coverage_path_planner.wavefront(PT, start, goal)
    assert len(PT_path) == num_free  # assert complete coverage<N><N>

def test_wavefront_CPP_1():
    img_dir = os.path.dirname(
        os.path.abspath(__file__)) + "/../PathPlanning/WavefrontCPP"
    img = plt.imread(os.path.join(img_dir, 'map', 'test.png'))
    img = 1 - img<N><N>
    start = (43, 0)
    goal = (0, 0)<N><N>
    wavefront_cpp(img, start, goal)<N><N>

def test_wavefront_CPP_2():
    img_dir = os.path.dirname(
        os.path.abspath(__file__)) + "/../PathPlanning/WavefrontCPP"
    img = plt.imread(os.path.join(img_dir, 'map', 'test_2.png'))
    img = 1 - img<N><N>
    start = (10, 0)
    goal = (10, 40)<N><N>
    wavefront_cpp(img, start, goal)<N><N>

def test_wavefront_CPP_3():
    img_dir = os.path.dirname(
        os.path.abspath(__file__)) + "/../PathPlanning/WavefrontCPP"
    img = plt.imread(os.path.join(img_dir, 'map', 'test_3.png'))
    img = 1 - img<N><N>
    start = (0, 0)
    goal = (30, 30)<N><N>
    wavefront_cpp(img, start, goal)<N><N>

if __name__ == '__main__':
    conftest.run_this_test(__file__)
<N><N>
from pyimagesearch.centroidtracker import CentroidTracker
from pyimagesearch.trackableobject import TrackableObject
from imutils.video import VideoStream
from imutils.video import FPS
from pyimagesearch.mailer import Mailer
from pyimagesearch import config, thread
import time, schedule, csv
import numpy as np
import argparse, imutils
import dlib, cv2, datetime
from itertools import zip_longest<N><N>
t0 = time.time()<N><N>
# import the necessary packages
from scipy.spatial import distance as dist
from collections import OrderedDict
import numpy as np<N><N>
class CentroidTracker:
	def __init__(self, maxDisappeared=50, maxDistance=50):
		# initialize the next unique object ID along with two ordered
		# dictionaries used to keep track of mapping a given object
		# ID to its centroid and number of consecutive frames it has
		# been marked as "disappeared", respectively
		self.nextObjectID = 0
		self.objects = OrderedDict()
		self.disappeared = OrderedDict()<N><N>
		# store the number of maximum consecutive frames a given
		# object is allowed to be marked as "disappeared" until we
		# need to deregister the object from tracking
		self.maxDisappeared = maxDisappeared<N><N>
		# store the maximum distance between centroids to associate
		# an object -- if the distance is larger than this maximum
		# distance we'll start to mark the object as "disappeared"
		self.maxDistance = maxDistance<N><N>
	def register(self, centroid):
		# when registering an object we use the next available object
		# ID to store the centroid
		self.objects[self.nextObjectID] = centroid
		self.disappeared[self.nextObjectID] = 0
		self.nextObjectID += 1<N><N>
	def deregister(self, objectID):
		# to deregister an object ID we delete the object ID from
		# both of our respective dictionaries
		del self.objects[objectID]
		del self.disappeared[objectID]<N><N>
	def update(self, rects):
		# check to see if the list of input bounding box rectangles
		# is empty
		if len(rects) == 0:
			# loop over any existing tracked objects and mark them
			# as disappeared
			for objectID in list(self.disappeared.keys()):
				self.disappeared[objectID] += 1<N><N>
				# if we have reached a maximum number of consecutive
				# frames where a given object has been marked as
				# missing, deregister it
				if self.disappeared[objectID] > self.maxDisappeared:
					self.deregister(objectID)<N><N>
			# return early as there are no centroids or tracking info
			# to update
			return self.objects<N><N>
		# initialize an array of input centroids for the current frame
		inputCentroids = np.zeros((len(rects), 2), dtype="int")<N><N>
		# loop over the bounding box rectangles
		for (i, (startX, startY, endX, endY)) in enumerate(rects):
			# use the bounding box coordinates to derive the centroid
			cX = int((startX + endX) / 2.0)
			cY = int((startY + endY) / 2.0)
			inputCentroids[i] = (cX, cY)<N><N>
		# if we are currently not tracking any objects take the input
		# centroids and register each of them
		if len(self.objects) == 0:
			for i in range(0, len(inputCentroids)):
				self.register(inputCentroids[i])<N><N>
		# otherwise, are are currently tracking objects so we need to
		# try to match the input centroids to existing object
		# centroids
		else:
			# grab the set of object IDs and corresponding centroids
			objectIDs = list(self.objects.keys())
			objectCentroids = list(self.objects.values())<N><N>
			# compute the distance between each pair of object
			# centroids and input centroids, respectively -- our
			# goal will be to match an input centroid to an existing
			# object centroid
			D = dist.cdist(np.array(objectCentroids), inputCentroids)<N><N>
			# in order to perform this matching we must (1) find the
			# smallest value in each row and then (2) sort the row
			# indexes based on their minimum values so that the row
			# with the smallest value as at the *front* of the index
			# list
			rows = D.min(axis=1).argsort()<N><N>
			# next, we perform a similar process on the columns by
			# finding the smallest value in each column and then
			# sorting using the previously computed row index list
			cols = D.argmin(axis=1)[rows]<N><N>
			# in order to determine if we need to update, register,
			# or deregister an object we need to keep track of which
			# of the rows and column indexes we have already examined
			usedRows = set()
			usedCols = set()<N><N>
			# loop over the combination of the (row, column) index
			# tuples
			for (row, col) in zip(rows, cols):
				# if we have already examined either the row or
				# column value before, ignore it
				if row in usedRows or col in usedCols:
					continue<N><N>
				# if the distance between centroids is greater than
				# the maximum distance, do not associate the two
				# centroids to the same object
				if D[row, col] > self.maxDistance:
					continue<N><N>
				# otherwise, grab the object ID for the current row,
				# set its new centroid, and reset the disappeared
				# counter
				objectID = objectIDs[row]
				self.objects[objectID] = inputCentroids[col]
				self.disappeared[objectID] = 0<N><N>
				# indicate that we have examined each of the row and
				# column indexes, respectively
				usedRows.add(row)
				usedCols.add(col)<N><N>
			# compute both the row and column index we have NOT yet
			# examined
			unusedRows = set(range(0, D.shape[0])).difference(usedRows)
			unusedCols = set(range(0, D.shape[1])).difference(usedCols)<N><N>
			# in the event that the number of object centroids is
			# equal or greater than the number of input centroids
			# we need to check and see if some of these objects have
			# potentially disappeared
			if D.shape[0] >= D.shape[1]:
				# loop over the unused row indexes
				for row in unusedRows:
					# grab the object ID for the corresponding row
					# index and increment the disappeared counter
					objectID = objectIDs[row]
					self.disappeared[objectID] += 1<N><N>
					# check to see if the number of consecutive
					# frames the object has been marked "disappeared"
					# for warrants deregistering the object
					if self.disappeared[objectID] > self.maxDisappeared:
						self.deregister(objectID)<N><N>
			# otherwise, if the number of input centroids is greater
			# than the number of existing object centroids we need to
			# register each new input centroid as a trackable object
			else:
				for col in unusedCols:
					self.register(inputCentroids[col])<N><N>
		# return the set of trackable objects
		return self.objects<N><N>
#===============================================================================
""" Optional features config. """
#===============================================================================
# Enter mail below to receive real-time email alerts
# e.g., 'email@gmail.com'
MAIL = ''
# Enter the ip camera url (e.g., url = 'http://191.138.0.100:8040/video')
url = ''<N><N>
import smtplib, ssl<N><N>
class Mailer:<N><N>
    """
    This script initiaties the email alert function.<N><N>
import cv2, threading, queue<N><N>
class ThreadingClass:
  # initiate threading class
  def __init__(self, name):
    self.cap = cv2.VideoCapture(name)
	# define an empty queue and thread
    self.q = queue.Queue()
    t = threading.Thread(target=self._reader)
    t.daemon = True
    t.start()<N><N>
  # read the frames as soon as they are available
  # this approach removes OpenCV's internal buffer and reduces the frame lag
  def _reader(self):
    while True:
      ret, frame = self.cap.read() # read the frames and ---
      if not ret:
        break
      if not self.q.empty():
        try:
          self.q.get_nowait()
        except queue.Empty:
          pass
      self.q.put(frame) # --- store them in a queue (instead of the buffer)<N><N>
  def read(self):
    return self.q.get() # fetch frames from the queue one by one<N><N>
  def release(self):
    return self.cap.release() # release the hw resource
<N><N>
class TrackableObject:
	def __init__(self, objectID, centroid):
		# store the object ID, then initialize a list of centroids
		# using the current centroid
		self.objectID = objectID
		self.centroids = [centroid]<N><N>		# initialize a boolean used to indicate if the object has
		# already been counted or not
		self.counted = False
import os
import numpy as np
import cv2<N><N>
if __name__ == '__main__':
    image_path = "eyes_template.png"<N><N>    image = cv2.imread(image_path)
    cv2.imshow("image", image)
    cv2.waitKey(0)
    pass
import os
import cv2
import numpy as np<N><N>

if __name__ == '__main__':
    use_image = False
    image_path = "eyes_template.png"<N><N>
    cap = cv2.VideoCapture(0)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')<N><N>
    while True:
        if use_image:
            frame = cv2.imread(image_path)
        else:
            ret, frame = cap.read()
            if not ret:
                print("error read video")
                continue
        ori_image = frame
        draw_image = ori_image.copy()
        result_draw_image = ori_image.copy()<N><N>
        faces = face_cascade.detectMultiScale(
            ori_image,
            scaleFactor=1.3,
            minNeighbors=5,
            minSize=(40, 40)
        )
        eyes = eye_cascade.detectMultiScale(ori_image, 1.3, 10)  # return list, per item format is [x, y, w, h]
        # print(eyes)<N><N>
        # draw face
        for (x, y, w, h) in faces:
            # 画出人脸框，蓝色，画笔宽度微
            img = cv2.rectangle(draw_image, (x, y), (x + w, y + h), (255, 0, 0), 2)<N><N>
        # draw eyes
        for (ex, ey, ew, eh) in eyes:
            cv2.rectangle(draw_image, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 1)<N><N>
        layers = cv2.split(ori_image)
        feature_layer = cv2.subtract(layers[-1], layers[0])
        _, feature_layer = cv2.threshold(feature_layer, 80, 255, cv2.THRESH_BINARY)<N><N>
import cv2<N><N>face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N>img = cv2.imread('test.png')<N><N>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>faces = face_cascade.detectMultiScale(gray,1.1, 4)<N><N>for(x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)<N><N>    cv2.imshow('img', img)
    cv2.waitKey()
import cv2<N><N>face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N>cap = cv2.VideoCapture(0)<N><N>while True:
    _, img = cap.read()
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    for (x, y, w, h) in faces:
        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
    cv2.imshow('img', img)
    k = cv2.waitKey(30) & 0xff
    if k==27:
        break
cap.release()
import cv2<N><N>
img = cv2.imread('lena.png')<N><N>
classNames = []
classFile = 'coco.names'
with open(classFile,'rt') as f:
    classNames = f.read().rstrip('\n').split('\n')<N><N>
configPath = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'
weightsPath = 'frozen_inference_graph.pb'<N><N>
net = cv2.dnn_DetectionModel(weightsPath,configPath)
net.setInputSize(320,320)
net.setInputScale(1.0/ 127.5)
net.setInputMean(127.5, 127.5, 127.5)
net.setInputSwapRB(True)<N><N>
clasIds, confs, bbox = net.detect(img,confThreshold=0.5)
print(clasIds,bbox)<N><N>

cv2.imshow("Output",img)
cv2.waitKey(0)<N><N>
# Changed Camera Calibration code from https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html
import numpy as np
import cv2 as cv
import glob
import os
from checkerboard import detect_checkerboard<N><N>
################ FIND CHESSBOARD CORNERS - OBJECT POINTS AND IMAGE POINTS #############################<N><N>
chessboardSize = (6,6)
frameSize = (1920,1080)<N><N>
# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
objp = np.zeros((chessboardSize[0] * chessboardSize[1], 3), np.float32)
objp[:,:2] = np.mgrid[0:chessboardSize[0],0:chessboardSize[1]].T.reshape(-1,2)<N><N>

# Arrays to store object points and image points from all the images.
objpoints = [] # 3d point in real world space
imgpoints = [] # 2d points in image plane.<N><N>
image_read_counter = 0
images = glob.glob('*.jpg')<N><N>
# Loop over all images in the directory of the code
for image in images:<N><N>
    # Increment and print counter for maintainance purpose
    image_read_counter = image_read_counter + 1
    print("Reading Image:", image_read_counter)<N><N>
    img = cv.imread(image)
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)<N><N>
    # Find the chess board corners -> original OpenCV function
    ret, corners = cv.findChessboardCorners(gray, chessboardSize, None)<N><N>
    # Function to determine the quality of the chessboard detection  ( https://pypi.org/project/checkerboard/ )
    cornerpoints, score = detect_checkerboard(gray, chessboardSize)
    # Important to convert float64 -> float32 when using this function
    # corners = np.float32(corners)<N><N>
<N><N>
    # If a chessboard is found and the detection has a good quality, add object points and image points after refining them
    if (ret == True) and (score < 0.5):<N><N>
        # Copy valid images to ValidImages folder... For organisational purpose
        #cv.imwrite('/ValidImages/' + str(
        #    image_read_counter) + '.jpg', img)<N><N>
        print("Processing Image:", image_read_counter)<N><N>
        #cv.waitKey(0)<N><N>
        objpoints.append(objp)<N><N>
        # termination criteria for refinement
        criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)
        # Refining pixel positions
        corners2 = cv.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)
        imgpoints.append(corners2) # Here the OpenCV code has a Error and appends unrefined points<N><N>
        # Draw and display the corners
        #cv.drawChessboardCorners(img, chessboardSize, corners2, ret)
        #cv.imshow('img', img)
        #cv.imwrite('Pattern' + str(image_read_counter) + '.png',img)
        #cv.waitKey(2)<N><N>

cv.destroyAllWindows()<N><N>

############## CALIBRATION #######################################################<N><N>
ret, cameraMatrix, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, frameSize, None, None)
print('RMS calibration error:', ret, '\n', '\ncameraMatrix:\n', cameraMatrix, '\n', '\nDistorsion:', dist, '\n\nrvecs:\n', rvecs, '\n\ntvecs:\n', tvecs)<N><N>
############## UNDISTORTION #####################################################
#Image array for images to be undistorted
img = []<N><N>
#Images of first video
img1 = cv.imread('/CalibVideo1/163.jpg')
img.append(img1)
img2 = cv.imread('/CalibVideo1/269.jpg')
img.append(img2)
img3 = cv.imread('/CalibVideo1/741.jpg')
img.append(img3)
img4 = cv.imread('/CalibVideo1/695.jpg')
img.append(img4)<N><N>

#Images of second video
img5 = cv.imread('/CalibVideo2/1.jpg')
img.append(img5)
img6 = cv.imread('/CalibVideo2/645.jpg')
img.append(img6)
img7 = cv.imread('/CalibVideo2/1682.jpg')
img.append(img7)
img8 = cv.imread('/CalibVideo2/1746.jpg')
img.append(img8)<N><N>
write_counter = 1<N><N>
cameraMatrix = np.zeros(shape=(3, 3), dtype=float)
newCameraMatrix = np.zeros(shape=(3, 3), dtype=float)
dist = np.zeros(shape=(0, 3), dtype=float)<N><N>
# Determined parameters and distortions
#cameraMatrix =  np.array([[1308.48240, 0.00000000, 943.249037],[0.00000000, 1312.24559, 462.244300],[0.00000000, 0.00000000, 1.00000000]])
#newCameraMatrix = np.array([[784.14855957, 0., 873.06276642], [0., 804.65039062, 404.46434741], [0., 0., 1.]])
#dist = np.array([[-3.66140985e-01,  1.88285247e-01,  7.40083619e-04, -1.42772163e-04, -5.97380717e-02]])<N><N>

# Loop to undistord images
for imag in img:
    h,  w = img1.shape[:2]
    newCameraMatrix, roi = cv.getOptimalNewCameraMatrix(cameraMatrix, dist, (w,h), 1, (w,h))
    print('\nnewCameraMatrix:\n', newCameraMatrix)<N><N>
    # Undistortion method 1
    dst = cv.undistort(imag, cameraMatrix, dist, None, newCameraMatrix)<N><N>
    # Save undistorted images with black spots
    #cv.imwrite('caliResultdist' + str(write_counter) + '.png', dst)<N><N>
    # crop the image
    x, y, w, h = roi
    dst = dst[y:y+h, x:x+w]<N><N>
    #store cropped image
    cv.imwrite('caliResult' + str(write_counter) + '.png', dst)<N><N>
    write_counter = write_counter + 1<N><N>
<N><N>############## CALCULATING THE REPROJECTION ERROR #####################################################
mean_error = 0<N><N>
for i in range(len(objpoints)):
    imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], cameraMatrix, dist)
    error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2)
    mean_error += error<N><N>
print( "\ntotal error: {}".format(mean_error/len(objpoints)) )
<N><N>
#Video to image conversion function from https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames
import cv2<N><N>vc = cv2.VideoCapture('checkerboard_000.h264') # or ('checkerboard_019.h264')
c=0<N><N>if vc.isOpened():
    rval , frame = vc.read()
else:
    rval = False<N><N>while rval:
    rval, frame = vc.read()
    cv2.imwrite(str(c) + '.jpg',frame)
    c = c + 1
    cv2.waitKey(1)
vc.release()

import os
import cv2<N><N>
# Insert a Name for the person so that he can be identified by the program in the terminal.<N><N>
Name = input("Please enter your full name : ")<N><N>
# The inserted name is going to be used to created a folder where the images are going to be stored.<N><N>
output_folder = './recog_Rsc/' + Name
if not os.path.exists(output_folder):
    os.makedirs(output_folder)<N><N>
# Creating a variables using the CascadeClassifier method from OpenCV, and using the haarcascade_frontalface xml, as well as
# the haarcascade eye xml files.<N><N>
face_cascade = cv2.CascadeClassifier('./xml/haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('./xml/haarcascade_eye.xml')<N><N>
# Initializing the camera variable that holds the index of our webcam, as well as two integers for counting purposes.<N><N>
camera = cv2.VideoCapture(0)
count = 0
i = 0<N><N>
# Creating a loop where we are going to use all the variable that we already declared in order to store the image frame that are being captured
# by the webcam.<N><N>
import os
import cv2
import numpy as np<N><N>
resultTrainImg, total = 0, 0<N><N>
import cv2 as cv
import numpy as np<N><N>
window_name = "Byoda"
img = cv.imread("./res/braintumor.jpg")
img2 = cv.imread('./res/braintumor.jpg')
img2 = cv.cvtColor(img2, cv.COLOR_BGR2GRAY)
cv.namedWindow(window_name)<N><N>
wid = img2.shape[1]
hei = img2.shape[0]<N><N>
drawing = False
ix = -1
iy = -1
x1 = 0
y1 = 0<N><N>
def draw_rect(event, x, y, flags, param):
    global ix, iy, drawing, img, x1, y1
    if event == cv.EVENT_LBUTTONDOWN:
        drawing = True
        ix = x
        iy = y<N><N>
    elif event == cv.EVENT_MOUSEMOVE:
        if drawing:
            img1 = cv.imread("./res/braintumor.jpg")
            cv.rectangle(img1, pt1=(ix, iy), pt2=(x, y), color=(0, 255, 255), thickness=1)
            print("ix = " + str(ix) + " and iy = " + str(iy) + "\n x = " + str(x) + " and y = " + str(y))
            img = img1
            x1 = x
            y1 = y<N><N>
    elif event == cv.EVENT_LBUTTONUP:
        drawing = False
        img1 = cv.imread("./res/braintumor.jpg")
        cv.rectangle(img1, pt1=(ix, iy), pt2=(x, y), color=(0, 255, 255), thickness=1)
        img = img1
        x1 = x
        y1 = y<N><N>
cv.setMouseCallback(window_name, draw_rect)<N><N>
while True:
    cv.imshow(window_name, img)
    if cv.waitKey(10) == 13:
        break<N><N>
for line in range(iy, y1):
    for col in range(ix, x1):
        if 125 <= img2[line, col] <= 180:
            img2[line, col] = img2[line, col]
        elif img2[line, col] < 125:
            img2[line, col] = 0
        elif img2[line, col] > 180:
            img2[line, col] = 0<N><N>
imgB = np.zeros_like(img2)<N><N>
for lineB in range(iy, y1):
    for colB in range(ix, x1):
        if (241 <= lineB <= 383) and (270 <= colB <= 384):
            imgB[lineB, colB] = 1
        elif (383 < lineB < 241) and (384 < colB < 270):
            imgB[lineB, colB] = 0<N><N>
tensFinalImage = img2 * imgB
cv.imshow(window_name, tensFinalImage)
cv.waitKey()
cv.destroyAllWindows()
<N><N>

import cv2 
import numpy as np <N><N>
cap = cv2.VideoCapture(0)
face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_alt.xml")<N><N>
skip = 0
face_data = [] 
dataset_path = "./face_dataset/"  <N><N>
file_name = input("Enter the name of person : ")<N><N>

while True:
	ret,frame = cap.read()<N><N>
	gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)<N><N>
	if ret == False:
		continue<N><N>
	faces = face_cascade.detectMultiScale(gray_frame,1.3,5) 
	if len(faces) == 0:
		continue<N><N>
	k = 1<N><N>
	faces = sorted(faces, key = lambda x : x[2]*x[3] , reverse = True) <N><N>
	skip += 1<N><N>
	for face in faces[:1]:
		x,y,w,h = face<N><N>
		offset = 5 
		face_offset = frame[y-offset:y+h+offset,x-offset:x+w+offset]
		face_selection = cv2.resize(face_offset,(100,100)) <N><N>
		if skip % 10 == 0:
			face_data.append(face_selection)
			print (len(face_data))
		<N><N>
		cv2.imshow(str(k), face_selection)
		k += 1
		
		cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)<N><N>
	cv2.imshow("faces",frame)<N><N>
	key_pressed = cv2.waitKey(1) & 0xFF
	if key_pressed == ord('q'):
		break<N><N>
face_data = np.array(face_data)  
face_data = face_data.reshape((face_data.shape[0], -1)) 
print (face_data.shape)<N><N>

np.save(dataset_path + file_name, face_data) 
print ("Dataset saved at : {}".format(dataset_path + file_name + '.npy'))<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
import numpy as np
import cv2
import os<N><N>

########## KNN CODE ############
def distance(v1, v2):
	return np.sqrt(((v1-v2)**2).sum())<N><N>
def knn(train, test, k=5):
	dist = []
	
	for i in range(train.shape[0]):
		ix = train[i, :-1]
		iy = train[i, -1]
		d = distance(test, ix)
		dist.append([d, iy])
			
	dk = sorted(dist, key=lambda x: x[0])[:k]
			
	labels = np.array(dk)[:, -1]
	
			
	output = np.unique(labels, return_counts=True)
			
	index = np.argmax(output[1])
	return output[0][index]
################################<N><N>
cap = cv2.VideoCapture(0)
face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_alt.xml")<N><N>
dataset_path = "./face_dataset/"<N><N>
face_data = []
labels = []
class_id = 0
names = {}<N><N>
<N><N>for fx in os.listdir(dataset_path): 
	if fx.endswith('.npy'): 
		names[class_id] = fx[:-4] 
		data_item = np.load(dataset_path + fx)
		face_data.append(data_item) <N><N>
		target = class_id * np.ones((data_item.shape[0],)) 
		class_id += 1 
		labels.append(target)
		<N><N>
face_dataset = np.concatenate(face_data, axis=0)
face_labels = np.concatenate(labels, axis=0).reshape((-1, 1))
print(face_labels.shape)
print(face_dataset.shape)<N><N>
trainset = np.concatenate((face_dataset, face_labels), axis=1)
print(trainset.shape)<N><N>
font = cv2.FONT_HERSHEY_SIMPLEX<N><N>
while True:
	ret, frame = cap.read()
	if ret == False:
		continue
		
	gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<N><N>
		
	faces = face_cascade.detectMultiScale(gray, 1.3, 5)<N><N>
	for face in faces:
		x, y, w, h = face<N><N>
			
		offset = 5
		face_section = frame[y-offset:y+h+offset, x-offset:x+w+offset]
		face_section = cv2.resize(face_section, (100, 100))<N><N>
		out = knn(trainset, face_section.flatten()) <N><N>
			
		cv2.putText(frame, names[int(out)],(x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),2,cv2.LINE_AA)
		cv2.rectangle(frame, (x,y), (x+w,y+h), (255,255,255), 2)<N><N>
	cv2.imshow("Faces", frame)<N><N>
	if cv2.waitKey(1) & 0xFF == ord('q'):
		break<N><N>
cap.release()
cv2.destroyAllWindows()<N><N>
# 30 May 2021 10:33:00
import cv2
from matplotlib import pyplot as plt<N><N>
debugFlag = 1<N><N>

def __dout(status, output):
    if debugFlag:
        print(' ' + status + ' ' + output)
    else:
        pass<N><N>

__dout('+', 'Packages imported successfully')<N><N>
inputImage = cv2.imread('../../samples/input.jpg')
__dout('>', 'Image read')<N><N>
plt.imshow(inputImage)
plt.title('Input Image')
plt.show()
__dout('+', 'Image printed')<N><N>
__dout('>', 'Printing the dimensions of input 2D image.')
print('Shape of the image : ', inputImage.shape)
print('Height of the image : ', inputImage.shape[0])
print('Width of the image : ', inputImage.shape[1])<N><N>
__dout('>', 'Saving files in $PWD')
cv2.imwrite('saved-using-cv2.jpg', inputImage)
__dout('+', 'Saved with jpg extension')
cv2.imwrite('saved-using-cv2.png', inputImage)
__dout('+', 'Saved with png extension')<N><N>
__dout('\nxoxo END oxox', '')
<N><N>
# 30 May 2021 10:33:00
import cv2
# import numpy as np
from matplotlib import pyplot as plt<N><N>
debugFlag = 1<N><N>

def __dout(status, output):
    if debugFlag:
        print(' ' + status + ' ' + output.title())
    else:
        pass<N><N>

__dout('+', 'packages imported successfully')<N><N>
inputImage = cv2.imread('../../samples/input.jpg')
__dout('+', 'image imported successfully')<N><N>
__dout('>', 'using cvtColor to convert it to grayscale')
grayImage = cv2.cvtColor(inputImage, cv2.COLOR_BGR2GRAY)
__dout('>', 'converted and stored in \'grayImage\'')<N><N>
row, col = 2, 1
fig, axs = plt.subplots(row, col, figsize=(15, 10))
fig.tight_layout()<N><N>
axs[0].imshow(cv2.cvtColor(inputImage, cv2.COLOR_BGR2RGB))
axs[0].set_title('Original')
__dout('+', 'prints out the original image')
axs[1].imshow(cv2.cvtColor(grayImage, cv2.COLOR_BGR2RGB))
axs[1].set_title('Grayscale')
__dout('+', 'prints out the grayscaled version')
plt.show()<N><N>
cv2.imwrite('grayscaling.png', grayImage)
cv2.imwrite('grayscaling.jpg', grayImage)
__dout('+', 'saved the files with png and jpg extensions')<N><N>
__dout('\nxoxo END oxox', '')
<N><N>
# Sun May 30 16:49:39
debugFlag = 1<N><N>

def __dout(status, output):
    if debugFlag:
        print(' ' + status + ' ' + output.title())
    else:
        pass<N><N>

__dout('+', 'Started')<N><N>
import cv2
from matplotlib import pyplot as plt<N><N>
inputImage = cv2.imread('../../samples/input.jpg')
__dout('+', 'image is imported')
hvsVersion = cv2.cvtColor(inputImage, cv2.COLOR_BGR2HSV)
__dout('+', 'converted image into hvs')<N><N>
rows, cols = 2, 2
fig, axs = plt.subplots(rows, cols, figsize=(15, 10))
fig.tight_layout()
__dout('+', 'created a 2x2 matrix to fit the plots')<N><N>
# importing the module
import argparse
import numpy as np
import pandas as pd
import cv2<N><N>
# read colors cvs
index=["color","color_name","hex","R","G","B"]
csv = pd.read_csv('colors.csv', names=index, header=None)<N><N>
# function to get colors name from cvs using RGB code
def getColorName(R,G,B):
    minimum = 10000
    for i in range(len(csv)):
        d = abs(R- int(csv.loc[i,"R"])) + abs(G- int(csv.loc[i,"G"]))+ abs(B- int(csv.loc[i,"B"]))
        if(d<=minimum):
            minimum = d
            cname = csv.loc[i,"color_name"]
    return cname<N><N>
from __future__ import print_function
import cv2 as cv
import argparse<N><N>
max_value = 255
max_value_H = 360//2
low_H = 0
low_S = 0
low_V = 0
high_H = max_value_H
high_S = max_value
high_V = max_value
window_capture_name = 'Video Capture'
window_detection_name = 'Object Detection'
low_H_name = 'Low H'
low_S_name = 'Low S'
low_V_name = 'Low V'
high_H_name = 'High H'
high_S_name = 'High S'
high_V_name = 'High V'<N><N>
# [low]<N><N>

def on_low_H_thresh_trackbar(val):
    global low_H
    global high_H
    low_H = val
    low_H = min(high_H-1, low_H)
    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)
# [low]<N><N>
# [high]<N><N>

def on_high_H_thresh_trackbar(val):
    global low_H
    global high_H
    high_H = val
    high_H = max(high_H, low_H+1)
    cv.setTrackbarPos(high_H_name, window_detection_name, high_H)
# [high]<N><N>

def on_low_S_thresh_trackbar(val):
    global low_S
    global high_S
    low_S = val
    low_S = min(high_S-1, low_S)
    cv.setTrackbarPos(low_S_name, window_detection_name, low_S)<N><N>

def on_high_S_thresh_trackbar(val):
    global low_S
    global high_S
    high_S = val
    high_S = max(high_S, low_S+1)
    cv.setTrackbarPos(high_S_name, window_detection_name, high_S)<N><N>

def on_low_V_thresh_trackbar(val):
    global low_V
    global high_V
    low_V = val
    low_V = min(high_V-1, low_V)
    cv.setTrackbarPos(low_V_name, window_detection_name, low_V)<N><N>
def on_high_V_thresh_trackbar(val):
    global low_V
    global high_V
    high_V = val
    high_V = max(high_V, low_V+1)
    cv.setTrackbarPos(high_V_name, window_detection_name, high_V)<N><N>

parser = argparse.ArgumentParser(description='Code for Thresholding Operations using inRange tutorial.')
parser.add_argument('--camera', help='Camera divide number.', default=0, type=int)
args = parser.parse_args()<N><N>
## [cap]
cap = cv.VideoCapture(args.camera)
## [cap]<N><N>
## [window]
cv.namedWindow(window_capture_name)
cv.namedWindow(window_detection_name)
## [window]<N><N>
import cv2 as cv
import numpy as np
import paho.mqtt.publish as publish
import drivers<N><N>
# Rangos de detección en HSV
green_range_low = np.array([20, 0, 108])
green_range_high = np.array([88, 51, 183])
pink_range_low = np.array([157, 50, 140])
pink_range_high = np.array([180, 110, 220])<N><N>
# contador de productos
count = {'Pink': 0, 'Green': 0}
display = drivers.Lcd()
display.lcd_display_string("*** Deteccion de", 1)
display.lcd_display_string("colores :3***", 2)<N><N>

def frame_and_publish(src):
    global x_prev, count
    src_hsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)
    mask = {'Pink': cv.inRange(src_hsv, pink_range_low, pink_range_high),
            'Green': cv.inRange(src_hsv, green_range_low, green_range_high)}
    poly_approx = {'Pink': [], 'Green': []}<N><N>
import cv2 as cv
import numpy as np
import paho.mqtt.publish as publish
import drivers<N><N>
# Rangos de detección en HSV
green_range_low = np.array([20, 0, 108])
green_range_high = np.array([88, 51, 183])
pink_range_low = np.array([157, 50, 140])
pink_range_high = np.array([180, 110, 220])<N><N>
# contador de productos
count = {'Pink': 0, 'Green': 0}
display = drivers.Lcd()
display.lcd_display_string("*** Deteccion de", 1)
display.lcd_display_string("colores :3***", 2)<N><N>

def frame_and_publish(src):
    global x_prev, count
    src_hsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)
    mask = {'Pink': cv.inRange(src_hsv, pink_range_low, pink_range_high),
            'Green': cv.inRange(src_hsv, green_range_low, green_range_high)}
    poly_approx = {'Pink': [], 'Green': []}<N><N>
from __future__ import print_function
import cv2 as cv
import argparse<N><N>
max_value = 255
max_value_H = 360//2
low_H = 0
low_S = 0
low_V = 0
high_H = max_value_H
high_S = max_value
high_V = max_value
window_capture_name = 'Video Capture'
window_detection_name = 'Object Detection'
low_H_name = 'Low H'
low_S_name = 'Low S'
low_V_name = 'Low V'
high_H_name = 'High H'
high_S_name = 'High S'
high_V_name = 'High V'<N><N>
# [low]<N><N>

def on_low_H_thresh_trackbar(val):
    global low_H
    global high_H
    low_H = val
    low_H = min(high_H-1, low_H)
    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)
# [low]<N><N>
# [high]<N><N>

def on_high_H_thresh_trackbar(val):
    global low_H
    global high_H
    high_H = val
    high_H = max(high_H, low_H+1)
    cv.setTrackbarPos(high_H_name, window_detection_name, high_H)
# [high]<N><N>

def on_low_S_thresh_trackbar(val):
    global low_S
    global high_S
    low_S = val
    low_S = min(high_S-1, low_S)
    cv.setTrackbarPos(low_S_name, window_detection_name, low_S)<N><N>

def on_high_S_thresh_trackbar(val):
    global low_S
    global high_S
    high_S = val
    high_S = max(high_S, low_S+1)
    cv.setTrackbarPos(high_S_name, window_detection_name, high_S)<N><N>

def on_low_V_thresh_trackbar(val):
    global low_V
    global high_V
    low_V = val
    low_V = min(high_V-1, low_V)
    cv.setTrackbarPos(low_V_name, window_detection_name, low_V)<N><N>
def on_high_V_thresh_trackbar(val):
    global low_V
    global high_V
    high_V = val
    high_V = max(high_V, low_V+1)
    cv.setTrackbarPos(high_V_name, window_detection_name, high_V)<N><N>

parser = argparse.ArgumentParser(description='Code for Thresholding Operations using inRange tutorial.')
parser.add_argument('--camera', help='Camera divide number.', default=0, type=int)
args = parser.parse_args()<N><N>
## [cap]
frame = cv.imread('prueba.jpg')
frame = cv.resize(frame, (500, 500))
## [cap]<N><N>
## [window]
cv.namedWindow(window_capture_name)
cv.namedWindow(window_detection_name)
## [window]<N><N>
from smbus import SMBus
from RPi.GPIO import RPI_REVISION
from time import sleep
from re import findall
from subprocess import check_output
from os.path import exists<N><N>
# old and new versions of the RPi have swapped the two i2c buses
# they can be identified by RPI_REVISION (or check sysfs)
BUS_NUMBER = 0 if RPI_REVISION == 1 else 1<N><N>
# other commands
LCD_CLEARDISPLAY = 0x01
LCD_RETURNHOME = 0x02
LCD_ENTRYMODESET = 0x04
LCD_DISPLAYCONTROL = 0x08
LCD_CURSORSHIFT = 0x10
LCD_FUNCTIONSET = 0x20
LCD_SETCGRAMADDR = 0x40
LCD_SETDDRAMADDR = 0x80<N><N>
# flags for display entry mode
LCD_ENTRYRIGHT = 0x00
LCD_ENTRYLEFT = 0x02
LCD_ENTRYSHIFTINCREMENT = 0x01
LCD_ENTRYSHIFTDECREMENT = 0x00<N><N>
# flags for display on/off control
LCD_DISPLAYON = 0x04
LCD_DISPLAYOFF = 0x00
LCD_CURSORON = 0x02
LCD_CURSOROFF = 0x00
LCD_BLINKON = 0x01
LCD_BLINKOFF = 0x00<N><N>
# flags for display/cursor shift
LCD_DISPLAYMOVE = 0x08
LCD_CURSORMOVE = 0x00
LCD_MOVERIGHT = 0x04
LCD_MOVELEFT = 0x00<N><N>
# flags for function set
LCD_8BITMODE = 0x10
LCD_4BITMODE = 0x00
LCD_2LINE = 0x08
LCD_1LINE = 0x00
LCD_5x10DOTS = 0x04
LCD_5x8DOTS = 0x00<N><N>
# flags for backlight control
LCD_BACKLIGHT = 0x08
LCD_NOBACKLIGHT = 0x00<N><N>
En = 0b00000100  # Enable bit
Rw = 0b00000010  # Read/Write bit
Rs = 0b00000001  # Register select bit<N><N>
from .i2c_dev import Lcd

import numpy as np
import cv2
from collections import deque<N><N>
def setValues(x):
   print("")<N><N>
import cv2
import os<N><N>
cam = cv2.VideoCapture(0)
cam.set(3, 640)
cam.set(4, 480)<N><N>
<N><N>face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N>
face_id = input('\n enter user id:- ')<N><N>
print("\n [INFO] Initializing face capture....")<N><N>
count = 0<N><N>
while(True):<N><N>
    ret, img = cam.read()
    
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_detector.detectMultiScale(gray, 1.3, 5)<N><N>
    for (x,y,w,h) in faces:<N><N>
        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)     
        count += 1<N><N>
        cv2.imwrite("dataset/User." + str(face_id) + '.' + str(count) + ".jpg", gray[y:y+h,x:x+w])<N><N>
        cv2.imshow('image', img)<N><N>
    k = cv2.waitKey(100) & 0xff 
    if k == 27:
        break
    elif count >= 30: 
         break<N><N>
print("\n [INFO] Exiting Program")
cam.release()
cv2.destroyAllWindows()
<N><N>
import cv2
import numpy as np
import os 
import mysql.connector<N><N>
def updateinDB(id):
    print(id)
    q = str(id)
    mydb = mysql.connector.connect(host="localhost",user="root",passwd="Akhil@123",database="project")
    mycursor = mydb.cursor()
    mycursor.execute("update student set status = 'p' where id ="+q)
    mydb.commit()
    
    
recognizer = cv2.face.LBPHFaceRecognizer_create()
recognizer.read('trainer/trainer.yml')
cascadePath = "haarcascade_frontalface_default.xml"
faceCascade = cv2.CascadeClassifier(cascadePath);<N><N>
font = cv2.FONT_HERSHEY_TRIPLEX
id = 0
names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35] <N><N>
cam = cv2.VideoCapture(0)
cam.set(3, 640)
cam.set(4, 480)<N><N>
minW = 0.1*cam.get(3)
minH = 0.1*cam.get(4)<N><N>
while True:<N><N>
    ret, img =cam.read()
   
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<N><N>
    faces = faceCascade.detectMultiScale( 
        gray,
        scaleFactor = 1.2,
        minNeighbors = 5,
        minSize = (int(minW), int(minH)),
       )<N><N>
    for(x,y,w,h) in faces:<N><N>
        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)<N><N>
        id, confidence = recognizer.predict(gray[y:y+h,x:x+w])<N><N>
        if (confidence < 100):
            id = names[id]
            confidence = "  {0}%".format(round(100 - confidence))
        else:
            id = "unknown"
            confidence = "  {0}%".format(round(100 - confidence))
        <N><N>
        cv2.putText(img, str(id), (x+5,y-5), font, 1, (0,255,0), 2)
        cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)  
        updateinDB(id)
    cv2.imshow('camera',img) <N><N>
    k = cv2.waitKey(10) & 0xff 
    if k == 27:
        break<N><N>
print("\n [INFO] Exiting Program")
cam.release()
cv2.destroyAllWindows()<N><N>
import cv2
import numpy as np
from PIL import Image
import os<N><N>
path = 'dataset'<N><N>
recognizer = cv2.face.LBPHFaceRecognizer_create()
detector = cv2.CascadeClassifier("haarcascade_frontalface_default.xml");<N><N>
def getImagesAndLabels(path):<N><N>
    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]
    faceSamples=[]
    ids = []<N><N>
    for imagePath in imagePaths:<N><N>
        PIL_img = Image.open(imagePath).convert('L')
        img_numpy = np.array(PIL_img,'uint8')<N><N>
        id = int(os.path.split(imagePath)[-1].split(".")[1])
        faces = detector.detectMultiScale(img_numpy)<N><N>
        for (x,y,w,h) in faces:
            faceSamples.append(img_numpy[y:y+h,x:x+w])
            ids.append(id)<N><N>
    return faceSamples,ids<N><N>
print ("\n [INFO] Training faces. It will take a few seconds. Wait ...")
faces,ids = getImagesAndLabels(path)
recognizer.train(faces, np.array(ids))<N><N>
recognizer.write('trainer/trainer.yml')<N><N>
print("\n [INFO] {0} faces trained.".format(len(np.unique(ids))))<N><N>
import numpy as np
import cv2
import time<N><N>
cap = cv2.VideoCapture(0)<N><N>
time.sleep(3)<N><N>
background = 0<N><N>
for i in range(30):<N><N>
    ret, background = cap.read()<N><N>
while(cap.isOpened()):<N><N>
    ret, img = cap.read()
    if not ret:
        break<N><N>
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)<N><N>
    lower_red = np.array({0, 120, 70})
    upper_red = np.array({10, 255, 255})
    mask1 = cv2.inRange(hsv, lower_red, upper_red)<N><N>
    lower_red = np.array([170, 120, 70])
    upper_red = np.array([180, 255, 255])
    mask2 = cv2.inRange(hsv, lower_red, upper_red)<N><N>
    mask1 = mask1 + mask2<N><N>
    mask1 = cv2.morphologyEx(mask1, cv2.MORPH_OPEN, np.ones((3,3), np.unit8))
    mask1 = cv2.morphologyEx(mask1, cv2.MORPH_DILATE, np.ones((3, 3), np.unit8))<N><N>
    mask2 = cv2.bitwise_not(mask1)<N><N>
    res1 = cv2.bitwise_and(background, background, mask=mask1)
    res2 = cv2.bitwise_and(img, img, mask=mask2)
    final_output = cv2.addWeighted(res1, res2, 1, 0)
    
    cv2.imshow('Eureka!', final_output)
    k = cv2.waitKey(10)
    if k == 27:
        break
        
    cap.release()
    cv2.destroyAllWindows()
<N><N>
import cv2
import numpy as np
img = cv2.imread("cards.jpg")<N><N>
width, height = 250, 350
pnts1 = np.float32([[111,219],[287,188], [154,482], [352, 440]])
print(pnts1)
#circel at one corner
cv2.circle(img, (pnts1[0][0], pnts1[0][1]), 3, (0, 0, 255), -1)<N><N>
#we need for four corner
for i in range(0, 4):
    cv2.circle(img, (pnts1[i][0], pnts1[i][1]), 6, (0, 0, 255), -1)<N><N>
#let's define 4 perspective points to transform this image <N><N>
pnts2 = np.float32([[0,0], [width,0], [0,height], [width, height]])
print(pnts2)<N><N>

#draw these points into matrix
matrix_pnts = cv2.getPerspectiveTransform(pnts1, pnts2)<N><N>
#output image 
output_image= cv2.warpPerspective(img, matrix_pnts, (width, height))<N><N>
cv2.imshow("originalimage", img)
cv2.imshow("transformimage", output_image)<N><N>
cv2.waitKey(0)<N><N>
import cv2
import  numpy as np<N><N>
def empty(x):
    pass<N><N>
cv2.namedWindow('images')
#tracebar for variying no of corners and quality
cv2.createTrackbar("quality", "images", 1, 100, empty)
cv2.createTrackbar("num_of_corners", "images", 10, 100, empty)<N><N>

filepath = "/home/kawaii/opencvminorprojects/squares.jpg"
img = cv2.imread(filepath)
#convert to gray scale
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>

#getting tracebar positions
numofcorner = cv2.getTrackbarPos("num_of_corners", "images")
quality = cv2.getTrackbarPos("quality", "images")
quality = quality/100<N><N>
#corner detection
corners = cv2.goodFeaturesToTrack(img_gray, numofcorner, quality, 10)<N><N>

#eliminate . from cordinates
corners = np.int0(corners)
print(corners)<N><N>
<N><N>#looping through every corners
for corner in corners:
    x, y = corner.ravel()
    cv2.circle(img, (x, y), 4, (0,255, 0), -1)<N><N>
cv2.imshow("images", img)
cv2.waitKey(0)
cv2.destroyAllWindows()<N><N>
from collections import deque
import cv2
import numpy as np
import imutils
from imutils.video import VideoStream
import time<N><N>
greenLower = (29, 86, 6)
greenUpper = (64, 255, 255)
pts = deque(maxlen=64)<N><N>
<N><N>vs = VideoStream(src=0).start()<N><N>
time.sleep(2.0)<N><N>
while True:<N><N>
    frame = vs.read()<N><N>
    frame = frame[1]<N><N>
    if frame is None:
        break<N><N>

    frame = imutils.resize(frame, width=600)
    blurred = cv2.GaussianBlur(frame, (11,11),0)
    hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)<N><N>
    mask = cv2.inRange(hsv, greenLower, greenUpper)
    mask = cv2.erode(mask, None, iterations=2)
    mask = cv2.dilate(mask, None, iterations=2)<N><N>
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)
    center = None<N><N>
    if len(cnts)>0:<N><N>
        c = max(cnts, key=cv2.contourArea)
        ((x,y), radius) = cv2.minEnclosingCircle(c)
        M = cv2.moments(c)
        center = (int(M("m10") / M["m00"]), int(M["m01"]/ M["m00"]))<N><N>
        if radius >10:
            cv2.circle(frame, (int(x), int(y)), int(radius), (0,255,255), 2)
            cv2.circle(frame, center, 5, (0,0,255), -1)<N><N>
    pts.appendleft(center)<N><N>

    for i in range(1,len(pts)):<N><N>
        if pts[i-1] is None or pts[i] is None:
            continue<N><N>
        thickness = int(np.sqrt(64 / float(i + 1)) * 2.5)
        cv2.line(frame, pts[i - 1], pts[i], (0, 0, 255), thickness)<N><N>
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1) & 0xFF<N><N>

vs.stop()
cv2.destroyAllWindows()<N><N>
import cv2
import datetime
import imutils
cam = cv2.VideoCapture('2.mkv')
print(cam.get(cv2.CAP_PROP_FRAME_WIDTH))
print(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))<N><N>
ret, Frame1 = cam.read()
ret, Frame2 = cam.read()
while(cam.isOpened()):<N><N>
    diff = cv2.absdiff(Frame1,Frame2)
    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 0)
    canny = cv2.Canny(blur, 30,150)
    cnts = cv2.findContours(canny.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)<N><N>
    for c in cnts:<N><N>
        (x,y,w,h) = cv2.boundingRect(c)<N><N>
        if cv2.contourArea(c) < 700:
            continue
        cv2.rectangle(Frame1, (x,y), (x+w, y+h), (0,255,0),2)<N><N>
<N><N>
    cv2.imshow('Frame1', Frame1)
    Frame1 = Frame2
    ret, Frame2 = cam.read()<N><N>

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>

cam.release()
cv2.destroyAllWindows()<N><N>
from sklearn.externals import joblib
from hog import HOG
import dataset
import cv2
import mahotas
import imutils
import numpy as np
from imutils.video import VideoStream
import time<N><N>
model = joblib.load('data/svm.cpickle')<N><N>
hog = HOG(orientations=18, pixelsPerCell= (10,10),
          cellsPerBlock=(1,1), transform=True)<N><N>
camera = cv2.VideoCapture(0)<N><N>
while True:
    (grabbed, frame) = camera.read()<N><N>
    if not grabbed:
        break<N><N>
#image = cv2.imread('1.png')
    frame = imutils.resize(frame, width=600)<N><N>
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<N><N>
    blurred = cv2.GaussianBlur(gray, (5,5), 0)
    edged = cv2.Canny(blurred, 30, 150)<N><N>
    cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)<N><N>
    #cnts = sorted([(c, cv2.boundingRect(c)[0]) for c in cnts], key = lambda x: x[1])
    print(len(cnts))<N><N>

    for c in cnts:
        (x, y, w, h) = cv2.boundingRect(c)<N><N>

        roi = gray[y:y+h , x:x+w]
        thresh = roi.copy()
        T = mahotas.thresholding.otsu(roi)
        thresh[thresh>T]=255
        thresh =cv2.bitwise_not(thresh)<N><N>
        thresh = dataset.deskew(thresh,20)
        thresh = dataset.center_extent(thresh,(20,20))<N><N>
        cv2.imshow('thresh', thresh)<N><N>
        hist = hog.describe(thresh)
        digit = model.predict([hist])[0]
        print('i think the number is {}',digit)<N><N>
        cv2.rectangle(frame, (x,y), (x+w,y+h),(0,255,0),1)
        cv2.putText(frame, str(digit), (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX,1.2, (0,255,0),2)<N><N>
    cv2.imshow('finalimage', frame)
    time.sleep(0.05)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>
camera.release()
cv2.destroyAllWindows()<N><N>
import imutils
import numpy as np
import cv2
import mahotas<N><N>
def load_digits(datasetPath):
    data = np.genfromtxt(datasetPath, delimiter=',', dtype='uint8')<N><N>
    target = data[:,0]
    data = data[:,1:].reshape(data.shape[0], 28,28)<N><N>
    return (data, target)<N><N>
def deskew(image, width):
    (h,w) = image.shape[:2]
    moments = cv2.moments(image)
    print(moments)
    skew = moments['mu11']/ moments['mu02']
    M  = np.float32([
        [1,skew, -0.5*w*skew],
        [0,1,0]
    ])
    image=cv2.warpAffine(image,M,(w,h),flags=cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)<N><N>
    image = imutils.resize(image, width=width)<N><N>
    return image<N><N>
def center_extent(image, size):
    (ew, eh) = size<N><N>
    if image.shape[1] > image.shape[0]:
        image = imutils.resize(image, width= ew)<N><N>
    else:
        image = imutils.resize(image, height=eh)<N><N>
    extent = np.zeros((eh,ew), dtype='uint8')<N><N>
    offsetX = (ew - image.shape[1])//2
    offsetY = (eh - image.shape[0])//2<N><N>
    extent[offsetY:offsetY + image.shape[0], offsetX:offsetX + image.shape[1] ] = image
    CM = mahotas.center_of_mass(extent)
    (CY, CX) = np.round(CM).astype('int32')
    (dX, dY) = ((size[0] // 2) - CX, (size[1] // 2) - CY)
    M = np.float32([[1, 0, dX], [0, 1, dY]])
    extent = cv2.warpAffine(extent, M, size)<N><N>
    return extent<N><N>
<N><N><N><N>
from skimage import feature<N><N>
class HOG:<N><N>
    def __init__(self, orientations=9, pixelsPerCell = (8,8),
                 cellsPerBlock= (3,3), transform= False):
        self.orientations = orientations
        self.pixelsPerCell = pixelsPerCell
        self.cellsPerBlock = cellsPerBlock
        self.transform = transform<N><N>
    def describe(self, image):
        hist = feature.hog(image,
                           orientations=self.orientations,
                           pixels_per_cell=self.pixelsPerCell,
                           cells_per_block=self.cellsPerBlock,
                           transform_sqrt=self.transform)
        return hist
<N><N>
from sklearn.externals import joblib
from sklearn.svm import LinearSVC
from hog import HOG
import dataset<N><N>
(digits, target) = dataset.load_digits('data/train.csv')
data = []<N><N>
hog = HOG(orientations=18, pixelsPerCell= (10,10),
          cellsPerBlock=(1,1), transform=True)<N><N>
for image in digits:
    image = dataset.deskew(image,20)
    image = dataset.center_extent(image, (20, 20))<N><N>
    hist = hog.describe(image)
    data.append(hist)<N><N>

model = LinearSVC(random_state=42)
model.fit(data, target)<N><N>
joblib.dump(model, 'data/svm.cpickle')<N><N>
import cv2<N><N>
WIN_NAME = "color_window"
TRACKBAR_NAME = "current_color"<N><N>
# 定义在modules/imgproc/include/opencv2/imgproc.hpp
color_spaces = [
  cv2.COLOR_BGR2BGRA, cv2.COLOR_BGR2RGBA,
  cv2.COLOR_BGR2GRAY, cv2.COLOR_BGR2HSV_FULL,
  cv2.COLOR_BGR2YUV,
]<N><N>
cv2.namedWindow(WIN_NAME, cv2.WINDOW_NORMAL)<N><N>
def callback():
  pass<N><N>

img = cv2.imread("./test.jpg")<N><N>
cv2.createTrackbar(TRACKBAR_NAME, WIN_NAME, 0, len(color_spaces) - 1, callback)<N><N>
while True:
  cur_color_space = color_spaces[cv2.getTrackbarPos(TRACKBAR_NAME, WIN_NAME)]<N><N>
  # 转换色彩空间
  cvt_img = cv2.cvtColor(img, cur_color_space)<N><N>
  cv2.imshow(WIN_NAME, cvt_img)<N><N>
  key = cv2.waitKey(10)
  if key & 0xff == ord('q'):
    break<N><N>
cv2.destroyAllWindows()
<N><N>
import cv2
import numpy as np<N><N># 浅拷贝
img1 = cv2.imread('./test.jpg')
img2 = img1<N><N>img2[10:100, 10:100] = [0,0,255]<N><N># 深拷贝
img3 = img2.copy()
img3[10:100, 10:100] = [0,255,0]<N><N>
cv2.imshow("img1", img1) # 有一块红色
cv2.imshow("img2", img2) # 有一块红色
cv2.imshow("img3", img3) # 有一块绿色<N><N>key = cv2.waitKey(0)
if key:
  cv2.destroyAllWindows()
import cv2
import numpy as np<N><N>
def mouse_callback(evt, x, y, flags, userdata):
  print(evt, x, y, flags, userdata)<N><N>
win_name = "mouse_windows"
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)
cv2.resizeWindow(win_name, 640, 360)<N><N>cv2.setMouseCallback(win_name, mouse_callback, "=======")<N><N># 640 * 360的黑图
img = np.zeros((360, 640, 3), np.uint8)
while True:
  cv2.imshow(win_name, img)
  key = cv2.waitKey(1)
  if key & 0xff == ord('q'):
    break
cv2.destroyAllWindows()

import cv2
import numpy as np<N><N>

# ==================创建矩阵=======================
# 一维
a = np.array([1, 2, 3])
# 二维矩阵，2 * 3，也可以更多维，但是大部分时候用二维
b = np.array([[1, 2, 3], [4, 5, 6]])<N><N>
# (480, 640, 3) 矩阵行个数，列个数，通道数/层数(每个通道代表一个矩阵)
# np.uint8: 矩阵数据类型
c = np.zeros((2, 2, 3), np.uint8)<N><N>
cc = np.ones((2, 2, 3), np.uint8)
d = np.identity(4)
dd = np.eye(3, 4, k=1)
# print('eye k', dd)<N><N>
# ==================索引矩阵[y, x] or [y, x, channel]=======================
# img = np.zeros((480, 640, 3), np.uint8)
# print(img[100, 100])
# count = 0
# while count < 200:
#   # y=100画一条200像素蓝色横线
#   # BGR B设为255
#   img[100, count, 0] = 255<N><N>
#   # y=200画一条200像素绿色横线
#   img[200, count] = [0, 255, 0]<N><N>
#   count += 1<N><N>
# cv2.imshow("img_auto_create_window", img)<N><N>
# key = cv2.waitKey(0)
# if key:
#   cv2.destroyAllWindows()<N><N>
# ==================提取子矩阵=======================<N><N>
img2 = np.zeros((480, 640, 3), np.uint8)<N><N>
# 提取y:100~200, x: 100~200的100*100区域，把它变成红色
roi = img2[100:200, 100:200]
roi[:,:] = [0, 0, 255]
# 把y:10～40,x:10~40区域变绿色
roi[10:40,10:40] = [0, 255, 0]<N><N>
cv2.imshow("img_roi", roi)<N><N>
key = cv2.waitKey(0)
if key:
  cv2.destroyAllWindows()<N><N>
import cv2<N><N>cv2.namedWindow("img", cv2.WINDOW_NORMAL)
img_mat = cv2.imread('./test.jpg')<N><N>cv2.imshow("img", img_mat)<N><N>key = cv2.waitKey(0)<N><N>if key & 0xff == ord('q'):
  exit()<N><N>cv2.destroyAllWindows()

import cv2
import numpy as np<N><N>
WIN_NAME = "trackbar_test_window"
TRACKBAR_NAME_R = "trackbar_name_R"
TRACKBAR_NAME_G = "trackbar_name_G"
TRACKBAR_NAME_B = "trackbar_name_B"<N><N>
cv2.namedWindow(WIN_NAME, cv2.WINDOW_NORMAL)<N><N>

def callback():
  pass<N><N>

cv2.createTrackbar(TRACKBAR_NAME_R, WIN_NAME, 0, 255, callback)
cv2.createTrackbar(TRACKBAR_NAME_G, WIN_NAME, 0, 255, callback)
cv2.createTrackbar(TRACKBAR_NAME_B, WIN_NAME, 0, 255, callback)<N><N>
img = np.zeros((480, 460, 3), np.uint8)<N><N>
while True:
  cv2.imshow(WIN_NAME, img)<N><N>
  r = cv2.getTrackbarPos(TRACKBAR_NAME_R, WIN_NAME)
  g = cv2.getTrackbarPos(TRACKBAR_NAME_G, WIN_NAME)
  b = cv2.getTrackbarPos(TRACKBAR_NAME_B, WIN_NAME)<N><N>
  img[:] = [b, g, r]<N><N>
  key = cv2.waitKey(10)
  if key & 0xff == ord('q'):
    break<N><N>
cv2.destroyAllWindows()
<N><N>
import cv2
cv2.namedWindow("videoread", cv2.WINDOW_NORMAL)<N><N># 从视频文件读取
cap = cv2.VideoCapture("./test_video.mp4")<N><N>while True:
  ret, frame = cap.read()
  cv2.imshow("videoread", frame)<N><N>  # 设置40ms读取一次, 25fps
  key = cv2.waitKey(40)
  if key & 0xff == ord('q'):
    break<N><N>cap.release()
cv2.destroyAllWindows()

import cv2<N><N>cv2.namedWindow("videoread", cv2.WINDOW_NORMAL)<N><N>cap = cv2.VideoCapture(0)<N><N>while True:
  # 从摄像头读取视频帧
  ret, frame = cap.read()<N><N>  cv2.imshow("videoread", frame)<N><N>  key = cv2.waitKey(5)
  if key & 0xff == ord('q'):
    break<N><N>cap.release()
cv2.destroyAllWindows()

import cv2<N><N>
fourcc = cv2.VideoWriter_fourcc(*'MJPG')
# 分辨率按摄像头真实分辨率设置，不然可能保存不成功
videoWriter = cv2.VideoWriter("./temp/test_video_writer.mp4", fourcc, 25, (1280, 720))<N><N>
cv2.namedWindow("videoread", cv2.WINDOW_NORMAL)
cap = cv2.VideoCapture(0)<N><N>
# 判断摄像头状态
while cap.isOpened():
  ret, frame = cap.read()<N><N>
  if ret == True:
    cv2.imshow("videoread", frame)
    # 窗口可能被内容撑大，这里resize一下
    cv2.resizeWindow("videoread", 640, 360)<N><N>
    # 写帧
    videoWriter.write(frame)<N><N>
    # 设置40ms读取一次, 25fps
    key = cv2.waitKey(40)
    if key & 0xff == ord('q'):
      break
  else:
    break<N><N>
cap.release()
videoWriter.release()
cv2.destroyAllWindows()
<N><N>
import cv2<N><N># 创建窗口
cv2.namedWindow("test", cv2.WINDOW_NORMAL)
# 显示窗口
cv2.imshow("test", 0)
# 设置窗口大小
cv2.resizeWindow("test", 300, 300)<N><N># 设置窗口展示时间,0为一直展示
key = cv2.waitKey(0)<N><N># 响应推出键盘事件
if key == 'q':
  exit()<N><N>cv2.destroyAllWindows()

import cv2<N><N>cv2.namedWindow("img", cv2.WINDOW_NORMAL)
img_mat = cv2.imread('./test.jpg')
cv2.imshow("img", img_mat)<N><N>key = cv2.waitKey(0)<N><N>while True:
  if key & 0xff == ord('q'):
    break
  elif key & 0xff == ord('s'):
    # bool imwrite( constString& filename, InputArrayimg, conststd::vector<int>& params = std::vector<int>());
    cv2.imwrite('./temp/test-write.png', img_mat)
    break<N><N>cv2.destroyAllWindows()<N><N>
# coding:utf-8<N><N>
import cv2 as cv
from matplotlib import pyplot as plt
import os<N><N>
# coding: utf-8
import os
import numpy as np
import cv2 as cv
from mypackage.multiplot import multiplot as mplt<N><N>

def main():
    # using dir() to look at inner functions
    # flags = [i for i in dir(cv) if i.startswith('COLOR_')]
    # print(flags)<N><N>
    # how to track HSV value
    green = np.uint8([[[0, 0, 255]]])
    hsv_green = cv.cvtColor(green, cv.COLOR_BGR2HSV)
    hsv_g = hsv_green.ravel()
    print(hsv_g)<N><N>
    filename = os.path.join('../mydata', 'animal.jpg')
    src_img = cv.imread(filename)
    hsv_img = cv.cvtColor(src_img, cv.COLOR_BGR2HSV)<N><N>
    # define range of green color in HSV
    lower_green = np.array([hsv_g[0] - 20, 50, 50])
    higher_green = np.array([hsv_g[0] + 20, 255, 255])<N><N>
    # threshold the HSV image to get only green colors
    mask = cv.inRange(hsv_img, lower_green, higher_green)<N><N>
    # Bitwise-AND mask and original image
    res = cv.bitwise_and(src_img, src_img, mask=mask)<N><N>
    imdict = dict()
    imdict['scr'] = src_img
    imdict['mask'] = mask
    imdict['res'] = res<N><N>
    mplt.show(imdict)
    # key = cv.waitKey() & 0xFF
    # cv.destroyAllWindows()<N><N>

if __name__ == '__main__':
    main()
<N><N>
# coding: utf-8<N><N>
import cv2 as cv
from matplotlib import pyplot as plt
from mypackage.multiplot import multiplot as mplt
import os
import numpy as np<N><N>

def main():
    filename = os.path.join('../mydata', 'noisy2.png')
    src_im = cv.imread(filename, cv.IMREAD_GRAYSCALE)<N><N>
    ret1, th1 = cv.threshold(src_im, 127, 255, cv.THRESH_BINARY)<N><N>
    ret2, th2 = cv.threshold(src_im, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)<N><N>
    # 高斯滤波后再采用Otsu阈值
    blur = cv.GaussianBlur(src_im, (5, 5), 0)
    ret3, th3 = cv.threshold(blur, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)<N><N>
# coding: utf-8<N><N>import cv2 as cv
from mypackage.multiplot import multiplot as mplt
import os<N><N>
def main():
    filename = os.path.join('')<N><N>
if __name__ == '__main__':
    main()
import os
import time
import zipfile
from concurrent import futures<N><N>
import tqdm<N><N>

def extract(filepath, filename, destpath, pwd):
    with open(filepath, 'rb') as f:
        zf = zipfile.ZipFile(f)
        zf.extract(filename, destpath, pwd)
    return filename<N><N>
# coding: utf-8
# coding:utf-8<N><N>
import math
import cv2 as cv
from matplotlib import pyplot as plt<N><N>

def show(imdict):
    """
    This is an adaptive drawing module
    :param imgsdict:
    :return:
    """
    if not isinstance(imdict, dict):
        raise TypeError('param must be type dict()')
    if not imdict:
        print('img dict is empty')<N><N>
    for index, (title, data) in enumerate(imdict.items()):
        img_num = len(imdict)
        if img_num < 3:
            row = 1
        elif img_num < 7:
            row = 2
        elif img_num < 10:
            row = 3
        else:
            row = 4<N><N>
        plt.subplot(row, math.ceil(img_num / row), index + 1)<N><N>
        if len(data.shape) == 1:
            plt.plot(data)
        else:
            img = cv.cvtColor(data, cv.COLOR_BGR2RGB)
            plt.imshow(img, 'gray')
        plt.title(title)
        plt.xticks([]), plt.yticks([])
    plt.show()
<N><N>
# coding:utf-8

# coding: utf-8<N><N>
import logging<N><N>

def init_logger(path='back.log'):
    logger = logging.getLogger('mylogger')
    logger.setLevel('DEBUG')<N><N>
    # create console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel('INFO')<N><N>
    SIMPLE_FORMAT = '%(process)d [L:%(lineno)d] %(levelname)s %(message)s'
    console_formatter = logging.Formatter(SIMPLE_FORMAT)
    console_handler.setFormatter(console_formatter)<N><N>
    # create file handler
    file_handler = logging.FileHandler(path)
    file_handler.setLevel('DEBUG')<N><N>
    BASIC_FORMAT = '%(asctime)s %(filename)s %(process)d:%(threadName)s-%(thread)d [L:%(lineno)d] %(levelname)s %(message)s'
    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
    file_formatter = logging.Formatter(BASIC_FORMAT)
    file_handler.setFormatter(file_formatter)<N><N>
    # add handler
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)<N><N>
    return logger<N><N>

logger = init_logger()<N><N>
if __name__ == '__main__':
    logger.info('this is msg')
    logger.debug('msg')
    logger.error('this is msg')
<N><N>
# coding: utf-8<N><N>from .log import logger

# installing required libraries<N><N>
import os
import cv2
import face_recognition
import face_recognition as fr
import numpy as np<N><N>

# encoding all the faces using get_encoded_faces () function
def get_encoded_faces():
    encoded = {}
    for dirpath, dnames, fnames in os.walk("./faces"):
        for f in fnames:
            if f.endswith(".jpg") or f.endswith(".png"):
                face = fr.load_image_file("faces/" + f)
                encoding = fr.face_encodings(face)[0]
                encoded[f.split(".")[0]] = encoding
    return encoded<N><N>

# encode a face given the file name
def unknown_image_encoded(img):
    face = fr.load_image_file("faces/" + img)
    encoding = fr.face_encodings(face)[0]
    return encoding<N><N>

# finding all the faces images and label that images
def classify_face(im):
    faces = get_encoded_faces()
    faces_encoded = list(faces.values())
    known_face_names = list(faces.keys())
    img = cv2.imread(im, 1)
    face_locations = face_recognition.face_locations(img)
    unknown_face_encodings = face_recognition.face_encodings(img, face_locations)<N><N>
# Import libraries
import cv2
from matplotlib import pyplot as plt
from datetime import datetime
import sys
import os<N><N>
# CONSTANTS
IMG_WIDTH, IMG_HEIGHT = 416, 416
if len(sys.argv) >= 2:
    DATA_FOLDER = os.path.join('./data', sys.argv[1])
    print('Any captured frames are saved in: ',DATA_FOLDER)
    os.makedirs(DATA_FOLDER, exist_ok=True)
SAVE_FORMAT = '.jpg'<N><N>
def import_yolo():
    ''' 
    Import the pre-trained model
    '''
    MODEL = './yolo/yolov3-face.cfg'
    WEIGHT = './yolo/yolov3-wider_16000.weights'<N><N>
    net = cv2.dnn.readNetFromDarknet(MODEL, WEIGHT)
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
    return net<N><N>
def yolo_forward_pass(frame):
    '''
    Pass each captured frame to the model
    '''
    # Making blob object from original image
    blob = cv2.dnn.blobFromImage(
        frame,
        1/255,
        (IMG_WIDTH, IMG_HEIGHT),
        [0,0,0],
        1,
        crop = False
    )<N><N>
    #  Set model input
    net.setInput(blob)<N><N>
    # Define the layers that we want to get the outputs from
    output_layers = net.getUnconnectedOutLayersNames()<N><N>
    # Run 'prediction
    outs = net.forward(output_layers)
    return outs<N><N>
def find_high_confidence_bounding_boxes(outs):
    '''
    Scan through all the bounding boxes output from the network and keep only
    the ones with high confidence scores. Assign the box's class label as the
    class with the highest score.
    '''
    confidences = []
    boxes = []<N><N>
import cv2
import time
import datetime
import imutils<N><N>
cam=cv2.VideoCapture(0)
time.sleep(1)<N><N>
import cv2<N><N>
img = cv2.imread('images/shapes.jpg')
imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # if you have a color image just convert to gray
imgBlur = cv2.GaussianBlur(imgGray, (7, 7), 1)
imgCanny = cv2.Canny(imgBlur, 75, 200)
copyOfImage = img.copy()<N><N>

def detectShapes(imgCanny):
    contours, hierachy = cv2.findContours(imgCanny, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    for contour in contours:
        area = cv2.contourArea(contour)
        if area > 500:
            cv2.drawContours(copyOfImage, contour, -1, (255, 0, 0), 2)
            perimeter = cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, 0.01 * perimeter, True)
            approxLen = len(approx)
            x, y, w, h = cv2.boundingRect(approx)<N><N>
# Import packages
import os
import argparse
import cv2
import numpy as np
import sys
import time
from threading import Thread
import importlib.util
import ffmpeg<N><N>
# import the necessary packages
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import load_model
from imutils.video import VideoStream
import numpy as np
import imutils
import time
import cv2
import os<N><N>
def detect_and_predict_mask(frame, faceNet, maskNet):
	# grab the dimensions of the frame and then construct a blob from it
	(h, w) = frame.shape[:2]
	blob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),
		(104.0, 177.0, 123.0))<N><N>
	# pass the blob through the network and obtain the face detections
	faceNet.setInput(blob)
	detections = faceNet.forward()
	print(detections.shape)<N><N>
	# initialize our list of faces, their corresponding locations,
	# and the list of predictions from our face mask network
	faces = []
	locs = []
	preds = []<N><N>
	# loop over the detections
	for i in range(0, detections.shape[2]):
		# extract the probability associated with the detection<N><N>
		confidence = detections[0, 0, i, 2]<N><N>
		# filter out weak detections 
		if confidence > 0.5:
			# compute the (x, y)-coordinates of the bounding box
			box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
			(startX, startY, endX, endY) = box.astype("int")<N><N>
			# bounding boxes fall within the dimensions of frame
			(startX, startY) = (max(0, startX), max(0, startY))
			(endX, endY) = (min(w - 1, endX), min(h - 1, endY))<N><N>
			# extract face ROI, convert from BGR to RGB channel
			# ordering, resize to 224x224, and preprocess
			face = frame[startY:endY, startX:endX]
			face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
			face = cv2.resize(face, (224, 224))
			face = img_to_array(face)
			face = preprocess_input(face)<N><N>
			# add face and bounding boxes to respective lists
			faces.append(face)
			locs.append((startX, startY, endX, endY))<N><N>
	# only make a predictions if at least one face was detected
	if len(faces) > 0:
		# for faster inference we'll make batch predictions on *all*
		# faces at the same time rather than one-by-one predictions
		# in the above `for` loop
		faces = np.array(faces, dtype="float32")
		preds = maskNet.predict(faces, batch_size=32)<N><N>
	# return a 2-tuple of the face locations and their corresponding
	# locations
	return (locs, preds)<N><N>
# load our serialized face detector model from disk
prototxtPath = r"face_detector\deploy.prototxt"
weightsPath = r"face_detector\res10_300x300_ssd_iter_140000.caffemodel"
faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)<N><N>
# load the face mask detector model from disk
maskNet = load_model("mask_detector.model")<N><N>
# initialize the video stream
print("[INFO] starting video stream...")
vs = VideoStream(src=0).start()<N><N>
# loop over the frames from the video stream
while True:
	# grab the frame from the threaded video stream and resize it
	# to have a maximum width of 400 pixels
	frame = vs.read()
	frame = imutils.resize(frame, width=400)<N><N>
	# detect faces in the frame and determine if they are wearing a
	# face mask or not
	(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)<N><N>
	# loop over the detected face locations and their corresponding
	# locations
	for (box, pred) in zip(locs, preds):
		# unpack the bounding box and predictions
		(startX, startY, endX, endY) = box
		(mask, withoutMask) = pred<N><N>
		# determine label and color to draw bounding box and text
		label = "Mask" if mask > withoutMask else "No Mask"
		color = (0, 255, 0) if label == "Mask" else (0, 0, 255)<N><N>
		# include the probability in the label
		label = "{}: {:.2f}%".format(label, max(mask, withoutMask) * 100)<N><N>
		# display label and bounding box on the output frame
		cv2.putText(frame, label, (startX, startY - 10),
			cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)
		cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)<N><N>
	# show the output frame
	cv2.imshow("Frame", frame)
	key = cv2.waitKey(1) & 0xFF<N><N>
	# `q` key was pressed, break
	if key == ord("q"):
		break<N><N>
# do cleanup
cv2.destroyAllWindows()
vs.stop()<N><N>
# requirments
# tensorflow>=1.15.2
# keras==2.3.1
# imutils==0.5.3
# numpy==1.18.2
# opencv-python==4.2.0.*
# matplotlib==3.2.1
# scipy==1.4.1<N><N>
import cv2
import mediapipe as mp
import time<N><N>
cap = cv2.VideoCapture(0)<N><N>
mpHands = mp.solutions.hands
hands = mpHands.Hands()
mpDraw = mp.solutions.drawing_utils<N><N>
pTime = 0
cTime = 0<N><N>
while True:
    success, img = cap.read()<N><N>
    imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    results = hands.process(imgRGB)<N><N>
    # print(results.multi_hand_landmarks)<N><N>
    if results.multi_hand_landmarks:
        for handLms in results.multi_hand_landmarks:
            for id, lm in enumerate(handLms.landmark):
                # print(id,lm)
                h,w,c = img.shape
                cx,cy = int(lm.x*w), int(lm.y*h)
                print(id,cx,cy)
                # if id==18:
                cv2.circle(img,(cx,cy),6,(255,0,255),cv2.FILLED)
            mpDraw.draw_landmarks(img,handLms,mpHands.HAND_CONNECTIONS)<N><N>
    cTime = time.time()
    fps = 1/(cTime-pTime)
    pTime = cTime<N><N>
    cv2.putText(img,str(int(fps)),(10,70),cv2.FONT_HERSHEY_PLAIN,3,(255,0,25),3)<N><N>
    cv2.imshow("Image :", img)
    cv2.waitKey(1)<N><N>
import cv2
import mediapipe as mp
import time<N><N>
class HandDectector():
    def __init__(self,mode=False,MaxHands=2,detectionCon=0.5,trackCon=0.5):
        self.mode = mode
        self.MaxHands = MaxHands
        self.detectionCon = detectionCon
        self.trackCon = trackCon<N><N>

        self.mpHands = mp.solutions.hands
        self.hands = self.mpHands.Hands(self.mode,self.MaxHands,self.detectionCon,self.trackCon)
        self.mpDraw = mp.solutions.drawing_utils<N><N>
    def findHands(self,img,draw=True):<N><N>
        imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        self.results = self.hands.process(imgRGB)<N><N>
        # print(results.multi_hand_landmarks)<N><N>
        if self.results.multi_hand_landmarks:
            for handLms in self.results.multi_hand_landmarks:
                if draw:
                    self.mpDraw.draw_landmarks(img,handLms,self.mpHands.HAND_CONNECTIONS)
        return img<N><N>
    def findPosition(self,img,handNo=0,draw=True):<N><N>
        lmList = []<N><N>

        if self.results.multi_hand_landmarks:<N><N>
            myHand = self.results.multi_hand_landmarks [handNo]<N><N>
            for id, lm in enumerate(myHand.landmark):
                # print(id,lm)
                h, w, c = img.shape
                cx, cy = int(lm.x * w), int(lm.y * h)
                # print(id, cx, cy)
                lmList.append([id,cx,cy])
                # if id==18:
                if draw:
                    cv2.circle(img, (cx, cy), 7, (255, 0, 255), cv2.FILLED)<N><N>
        return lmList<N><N>

def main():
    pTime = 0
    cTime = 0<N><N>
    cap = cv2.VideoCapture(0)
    dectector = HandDectector()
    while True:
        success, img = cap.read()
        img = dectector.findHands(img)<N><N>
        lmList = dectector.findPosition(img);
        if len(lmList) != 0:
            print(lmList)<N><N>
        cTime = time.time()
        fps = 1 / (cTime - pTime)
        pTime = cTime<N><N>
        cv2.putText(img, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)<N><N>
        cv2.imshow("Image :", img)
        cv2.waitKey(1)<N><N>
if __name__=='__main__':
    main()<N><N>
import cv2
import mediapipe
import time
import HandTrackingModule as htm<N><N>
pTime = 0
cTime = 0
cap = cv2.VideoCapture(0)
dectector = htm.HandDectector()
while True:
    success, img = cap.read()
    img = dectector.findHands(img,draw=False)<N><N>
    lmList = dectector.findPosition(img,draw=False);
    if len(lmList) != 0:
        print(lmList[0],lmList[4],lmList[8],lmList[12],lmList[16])<N><N>
    cTime = time.time()
    fps = 1 / (cTime - pTime)
    pTime = cTime<N><N>
    cv2.putText(img, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 25), 3)<N><N>
    cv2.imshow("Image :", img)
    cv2.waitKey(1)
<N><N>
import cv2
import mediapipe as mp
import time<N><N>
cap = cv2.VideoCapture(0)
pTime = 0<N><N>
mpPose = mp.solutions.pose
pose = mpPose.Pose()
mpDraw = mp.solutions.drawing_utils<N><N>
while True:<N><N>
    success , img = cap.read()<N><N>
    imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    results = pose.process(imgRGB)
    # print(results)
    # print(results.pose_landmarks)
    if results.pose_landmarks:
        mpDraw.draw_landmarks(img,results.pose_landmarks,mpPose.POSE_CONNECTIONS)<N><N>
        for id, lm in enumerate(results.pose_landmarks.landmark):
            h , w , c = img.shape
            print(id,lm)
            cx , cy = int(lm.x*w) , int(lm.y*h)
            cv2.circle(img,(cx,cy),4,(255,0,0,cv2.FILLED))<N><N>
    cTime = time.time()
    fps = 1/(cTime-pTime)
    pTime = cTime<N><N>
    cv2.putText(img,str(int(fps)),(70,50),cv2.FONT_HERSHEY_PLAIN,3,(255,0,0),3)
    cv2.imshow("Image :", img)<N><N>
    cv2.waitKey(1)
<N><N>
import cv2
import mediapipe as mp
import time<N><N>

class poseDetector():<N><N>
    def __init__(self,mode=False,upBody=False,smooth=True,detectionCon=0.5,trackCon=0.5):
        self.mode = mode
        self.upBody = upBody
        self.smooth = smooth
        self.detectionCon = detectionCon
        self.trackCon = trackCon<N><N>
        self.mpDraw = mp.solutions.drawing_utils
        self.mpPose = mp.solutions.pose
        self.pose = self.mpPose.Pose(self.mode,self.upBody,self.smooth,
                                     self.detectionCon,self.trackCon)<N><N>
    def findPose(self,img,draw=True):<N><N>
        imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        self.results = self.pose.process(imgRGB)
        # print(results)
        # print(results.pose_landmarks)<N><N>
        if self.results.pose_landmarks:
            if draw:
                self.mpDraw.draw_landmarks(img,self.results.pose_landmarks,
                                       self.mpPose.POSE_CONNECTIONS)
        return img<N><N>

    def getPosition(self,img,draw=True):
        lmList = []
        if self.results.pose_landmarks:<N><N>
            for id, lm in enumerate(self.results.pose_landmarks.landmark):
                h , w , c = img.shape
                # print(id,lm)
                cx , cy = int(lm.x*w) , int(lm.y*h)
                lmList.append([id,cx,cy])
                if draw:
                    cv2.circle(img,(cx,cy),4,(255,0,0,cv2.FILLED))
        return lmList<N><N>
def main():
    cap = cv2.VideoCapture(0)
    pTime = 0<N><N>
    detector = poseDetector()<N><N>
    while True:
        success, img = cap.read()
        detector.findPose(img)
        lmList = detector.getPosition(img)
        if len(lmList) !=0:
            print(lmList)
        cTime = time.time()
        fps = 1/(cTime-pTime)
        pTime = cTime<N><N>
        cv2.putText(img,str(int(fps)),(70,50),cv2.FONT_HERSHEY_PLAIN,3,(255,0,0),3)
        cv2.imshow("Image :", img)<N><N>
        cv2.waitKey(1)<N><N>

if __name__=='__main__':
    main()
<N><N>
import cv2
import mediapipe as mp
import time
import PoseModule as pm<N><N>
<N><N><N><N>
<N><N>
cap = cv2.VideoCapture(0)
pTime = 0
detector = pm.poseDetector()<N><N>
while True:
    success, img = cap.read()
    detector.findPose(img)
    lmList = detector.getPosition(img,draw=False)
    if len(lmList) !=0:
        print(lmList)
        cv2.circle(img,(lmList[0][1],lmList[0][2]),10,(0,0,255),cv2.FILLED)
    cTime = time.time()
    fps = 1/(cTime-pTime)
    pTime = cTime<N><N>
    cv2.putText(img,str(int(fps)),(70,50),cv2.FONT_HERSHEY_PLAIN,3,(255,0,0),3)
    cv2.imshow("Image :", img)<N><N>
    cv2.waitKey(1)
<N><N>
from imutils.video import VideoStream
import numpy as np<N><N>
import cv2
import imutils
import time<N><N>
# importing directkeys for using press key and release key functions
from directkeys import  W, A, S, D
from directkeys import PressKey, ReleaseKey <N><N>
# define the lower and upper boundaries of the "blue" object in the HSV color space
# https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv
blueLower = np.array([53, 187, 0])
blueUpper = np.array([180, 255, 255])<N><N>
# Staring the webcam
video = VideoStream(src=0).start()
 
# allow the camera or video file to warm up
time.sleep(2.0)
initial = True
flag = False
current_key_pressed = set()
circle_radius = 30<N><N>
# Defining window boundaries for each logically divided region
windowSize = 80
windowSize2 = 80<N><N>
lr_counter = 0<N><N>
# keep looping
while True:
    keyPressed = False
    keyPressed_lr = False
    
    # grab the current frame
    frame = video.read()
    
    # My video frame was flipped horizontally. If your video is not flipped by default you can ommit this
    frame = cv2.flip(frame,1);<N><N>
    # resize the frame, blur it, and convert it to the HSV color space
    frame = imutils.resize(frame, width=600)
    frame = imutils.resize(frame, height=300)<N><N>
#for Macbook models
import time
from Quartz.CoreGraphics import CGEventCreateKeyboardEvent
from Quartz.CoreGraphics import CGEventPost<N><N>
# Python releases things automatically, using CFRelease will result in a scary error
#from Quartz.CoreGraphics import CFRelease<N><N>
from Quartz.CoreGraphics import kCGHIDEventTap<N><N>
# From http://stackoverflow.com/questions/281133/controlling-the-mouse-from-python-in-os-x
# and from https://developer.apple.com/library/mac/documentation/Carbon/Reference/QuartzEventServicesRef/index.html#//apple_ref/c/func/CGEventCreateKeyboardEvent<N><N>
W = 0x0D
A = 0x00
S = 0x01
D = 0x02<N><N>
def KeyDown(k):
    keyCode, shiftKey = toKeyCode(k)<N><N>
    time.sleep(0.0001)<N><N>
    if shiftKey:
        CGEventPost(kCGHIDEventTap, CGEventCreateKeyboardEvent(None, 0x38, True))
        time.sleep(0.0001)<N><N>
    CGEventPost(kCGHIDEventTap, CGEventCreateKeyboardEvent(None, keyCode, True))
    time.sleep(0.0001)<N><N>
    if shiftKey:
        CGEventPost(kCGHIDEventTap, CGEventCreateKeyboardEvent(None, 0x38, False))
        time.sleep(0.0001)<N><N>
def KeyUp(k):
    keyCode, shiftKey = toKeyCode(k)<N><N>
    time.sleep(0.0001)<N><N>
    CGEventPost(kCGHIDEventTap, CGEventCreateKeyboardEvent(None, keyCode, False))
    time.sleep(0.0001)<N><N>
def PressKey(keyCode):<N><N>
    time.sleep(0.0001)<N><N>
    CGEventPost(kCGHIDEventTap, CGEventCreateKeyboardEvent(None, keyCode, True))
    time.sleep(0.0001)<N><N>
def ReleaseKey(keyCode):<N><N>
    time.sleep(0.0001)<N><N>
    CGEventPost(kCGHIDEventTap, CGEventCreateKeyboardEvent(None, keyCode, False))
    time.sleep(0.0001)<N><N>

# From http://stackoverflow.com/questions/3202629/where-can-i-find-a-list-of-mac-virtual-key-codes<N><N>
def toKeyCode(c):
    shiftKey = False
    # Letter
    if c.isalpha():
        if not c.islower():
            shiftKey = True
            c = c.lower()<N><N>
    if c in shiftChars:
        shiftKey = True
        c = shiftChars[c]
    if c in keyCodeMap:
        keyCode = keyCodeMap[c]
    else:
        keyCode = ord(c)
    return keyCode, shiftKey<N><N>
shiftChars = {
    '~': '`',
    '!': '1',
    '@': '2',
    '#': '3',
    '$': '4',
    '%': '5',
    '^': '6',
    '&': '7',
    '*': '8',
    '(': '9',
    ')': '0',
    '_': '-',
    '+': '=',
    '{': '[',
    '}': ']',
    '|': '\\',
    ':': ';',
    '"': '\'',
    '<': ',',
    '>': '.',
    '?': '/'
}<N><N>
#for Windows
import ctypes
import time<N><N>
SendInput = ctypes.windll.user32.SendInput<N><N>

W = 0x11
A = 0x1E
S = 0x1F
D = 0x20<N><N>
# C struct redefinitions
PUL = ctypes.POINTER(ctypes.c_ulong)
class KeyBdInput(ctypes.Structure):
    _fields_ = [("wVk", ctypes.c_ushort),
                ("wScan", ctypes.c_ushort),
                ("dwFlags", ctypes.c_ulong),
                ("time", ctypes.c_ulong),
                ("dwExtraInfo", PUL)]<N><N>
class HardwareInput(ctypes.Structure):
    _fields_ = [("uMsg", ctypes.c_ulong),
                ("wParamL", ctypes.c_short),
                ("wParamH", ctypes.c_ushort)]<N><N>
class MouseInput(ctypes.Structure):
    _fields_ = [("dx", ctypes.c_long),
                ("dy", ctypes.c_long),
                ("mouseData", ctypes.c_ulong),
                ("dwFlags", ctypes.c_ulong),
                ("time",ctypes.c_ulong),
                ("dwExtraInfo", PUL)]<N><N>
class Input_I(ctypes.Union):
    _fields_ = [("ki", KeyBdInput),
                 ("mi", MouseInput),
                 ("hi", HardwareInput)]<N><N>
class Input(ctypes.Structure):
    _fields_ = [("type", ctypes.c_ulong),
                ("ii", Input_I)]<N><N>
# Actuals Functions<N><N>
def PressKey(hexKeyCode):
    extra = ctypes.c_ulong(0)
    ii_ = Input_I()
    ii_.ki = KeyBdInput( 0, hexKeyCode, 0x0008, 0, ctypes.pointer(extra) )
    x = Input( ctypes.c_ulong(1), ii_ )
    ctypes.windll.user32.SendInput(1, ctypes.pointer(x), ctypes.sizeof(x))<N><N>
def ReleaseKey(hexKeyCode):
    extra = ctypes.c_ulong(0)
    ii_ = Input_I()
    ii_.ki = KeyBdInput( 0, hexKeyCode, 0x0008 | 0x0002, 0, ctypes.pointer(extra) )
    x = Input( ctypes.c_ulong(1), ii_ )
    ctypes.windll.user32.SendInput(1, ctypes.pointer(x), ctypes.sizeof(x))<N><N>
if __name__ == '__main__':
    PressKey(0x11)
    time.sleep(1)
    ReleaseKey(0x11)
    time.sleep(1)
<N><N>
import cv2
import numpy as np
import streamlit as st<N><N>
st.title("Cartoonify App")<N><N>
st.write(
    """This app creates a filtered version of the image you provide, start by uploading an image 👇
    """
)<N><N>
uploaded_file = st.file_uploader("Upload Files", type=["jpeg", "jpg", "png", "jiff"])<N><N>
if uploaded_file is not None:
    # Convert the file to an opencv image.
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    opencv_image = cv2.imdecode(file_bytes, 1)<N><N>
<N><N>def color_quantization(img, k):
  data = np.float32(img).reshape((-1, 3))<N><N>
  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 0.001)
  ret, label, center = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
  center = np.uint8(center)
  result = center[label.flatten()]
  result = result.reshape(img.shape)
  return result<N><N>
try:<N><N>
    total_color = 12<N><N>
    results = color_quantization(opencv_image, total_color)
    col1, col2 = st.beta_columns(2)<N><N>
    col1.image(opencv_image, use_column_width=True, channels="BGR", caption="Original")<N><N>
    col2.image(results, use_column_width=True, channels = "BGR",caption="Filtered")<N><N>
except:
    pass<N><N>
  <N><N><N><N>
<N><N><N><N>
import cv2
import numpy as np
def nothing(x):
    pass
# Create a black image, a window
img = np.zeros((300,512,3), np.uint8)
cv2.namedWindow('RGB-VALUES')<N><N>
# create trackbars for color change
cv2.createTrackbar('R','RGB-VALUES',0,255,nothing)
cv2.createTrackbar('G','RGB-VALUES',0,255,nothing)
cv2.createTrackbar('B','RGB-VALUES',0,255,nothing)<N><N>
#!/usr/bin/env python3<N><N>
from __future__ import print_function<N><N>
from core.colors import end, red, white, bad, info<N><N>
# Just a fancy ass banner
print('''%s
\tXSStrike %sv3.1.4
%s''' % (red, white, end))<N><N>
import concurrent.futures
import re<N><N>
from core.colors import green, end
from core.config import blindParams, xsschecker, threadCount
from core.requester import requester
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>

def checky(param, paraNames, url, headers, GET, delay, timeout):
    if param not in paraNames:
        logger.debug('Checking param: {}'.format(param))
        response = requester(url, {param: xsschecker},
                             headers, GET, delay, timeout).text
        if '\'%s\'' % xsschecker in response or '"%s"' % xsschecker in response or ' %s ' % xsschecker in response:
            paraNames[param] = ''
            logger.good('Valid parameter found: %s%s', green, param)<N><N>
import copy
from fuzzywuzzy import fuzz
import re
from urllib.parse import unquote<N><N>
from core.config import xsschecker
from core.requester import requester
from core.utils import replaceValue, fillHoles<N><N>
import sys
import os
import platform<N><N>
changes = '''Negligible DOM XSS false positives;x10 faster crawling'''
globalVariables = {}  # it holds variables during runtime for collaboration across modules<N><N>
defaultEditor = 'nano'
blindPayload = ''  # your blind XSS payload
xsschecker = 'v3dm0s'  # A non malicious string to check for reflections and stuff<N><N>
#  More information on adding proxies: http://docs.python-requests.org/en/master/user/advanced/#proxies
proxies = {'http': 'http://0.0.0.0:8080', 'https': 'http://0.0.0.0:8080'}<N><N>
minEfficiency = 90  # payloads below this efficiency will not be displayed<N><N>
delay = 0  # default delay between http requests
threadCount = 10  # default number of threads
timeout = 10  # default number of http request timeout<N><N>
# attributes that have special properties
specialAttributes = ['srcdoc', 'src']<N><N>
badTags = ('iframe', 'title', 'textarea', 'noembed',
           'style', 'template', 'noscript')<N><N>
tags = ('html', 'd3v', 'a', 'details')  # HTML Tags<N><N>
# "Things" that can be used between js functions and breakers e.g. '};alert()//
jFillings = (';')
# "Things" that can be used before > e.g. <tag attr=value%0dx>
lFillings = ('', '%0dx')
# "Things" to use between event handler and = or between function and =
eFillings = ('%09', '%0a', '%0d',  '+')
fillings = ('%09', '%0a', '%0d', '/+/')  # "Things" to use instead of space<N><N>
eventHandlers = {  # Event handlers and the tags compatible with them
    'ontoggle': ['details'],
    'onpointerenter': ['d3v', 'details', 'html', 'a'],
    'onmouseover': ['a', 'html', 'd3v']
}<N><N>
functions = (  # JavaScript functions to get a popup
    '[8].find(confirm)', 'confirm()',
    '(confirm)()', 'co\u006efir\u006d()',
    '(prompt)``', 'a=prompt,a()')<N><N>
import re<N><N>
from core.colors import red, end, yellow<N><N>
import base64 as b64
import re<N><N>
def base64(string):
    if re.match(r'^[A-Za-z0-9+\/=]+$', string) and (len(string) % 4) == 0:
        return b64.b64decode(string.encode('utf-8')).decode('utf-8')
    else:
        return b64.b64encode(string.encode('utf-8')).decode('utf-8')

from core.checker import checker<N><N>
import copy
from random import randint
from time import sleep
from urllib.parse import unquote<N><N>
from core.colors import end, red, green, yellow
from core.config import fuzzes, xsschecker
from core.requester import requester
from core.utils import replaceValue, counter
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
from core.config import xsschecker, badTags, fillings, eFillings, lFillings, jFillings, eventHandlers, tags, functions
from core.jsContexter import jsContexter
from core.utils import randomUpper as r, genGen, extractScripts<N><N>
import re<N><N>
from core.config import badTags, xsschecker
from core.utils import isBadContext, equalize, escaped, extractScripts<N><N>
import re<N><N>
from core.config import xsschecker
from core.utils import stripper<N><N>
import logging
from .colors import *<N><N>
__all__ = ['setup_logger', 'console_log_level', 'file_log_level', 'log_file']<N><N>
console_log_level = 'INFO'
file_log_level = None
log_file = 'xsstrike.log'<N><N>
"""
Default Logging Levels
CRITICAL = 50
ERROR = 40
WARNING = 30
INFO = 20
DEBUG = 10
"""<N><N>
VULN_LEVEL_NUM = 60
RUN_LEVEL_NUM = 22
GOOD_LEVEL_NUM = 25<N><N>

logging.addLevelName(VULN_LEVEL_NUM, 'VULN')
logging.addLevelName(RUN_LEVEL_NUM, 'RUN')
logging.addLevelName(GOOD_LEVEL_NUM, 'GOOD')<N><N>

def _vuln(self, msg, *args, **kwargs):
    if self.isEnabledFor(VULN_LEVEL_NUM):
        self._log(VULN_LEVEL_NUM, msg, args, **kwargs)<N><N>

def _run(self, msg, *args, **kwargs):
    if self.isEnabledFor(RUN_LEVEL_NUM):
        self._log(RUN_LEVEL_NUM, msg, args, **kwargs)<N><N>

def _good(self, msg, *args, **kwargs):
    if self.isEnabledFor(GOOD_LEVEL_NUM):
        self._log(GOOD_LEVEL_NUM, msg, args, **kwargs)<N><N>

logging.Logger.vuln = _vuln
logging.Logger.run = _run
logging.Logger.good = _good<N><N>
import re
import concurrent.futures
from urllib.parse import urlparse<N><N>
from core.dom import dom
from core.log import setup_logger
from core.utils import getUrl, getParams
from core.requester import requester
from core.zetanize import zetanize
from plugins.retireJs import retireJs<N><N>
logger = setup_logger(__name__)<N><N>
import os
import tempfile<N><N>
from core.config import defaultEditor
from core.colors import white, yellow
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
import random
import requests
import time
from urllib3.exceptions import ProtocolError
import warnings<N><N>
import core.config
from core.utils import converter, getVar
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
warnings.filterwarnings('ignore')  # Disable SSL related warnings<N><N>
import os
import re
from requests import get<N><N>
from core.config import changes
from core.colors import run, que, good, info, end, green
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>

def updater():
    logger.run('Checking for updates')
    latestCommit = get(
        'https://raw.githubusercontent.com/s0md3v/XSStrike/master/core/config.py').text<N><N>
import json
import random
import re
from urllib.parse import urlparse<N><N>
import core.config
from core.config import xsschecker<N><N>
import json
import re
import sys<N><N>
from core.requester import requester
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
import re<N><N>

def zetanize(response):
    def e(string):
        return string.encode('utf-8')<N><N>
    def d(string):
        return string.decode('utf-8')<N><N>
import copy
from urllib.parse import urlparse, unquote<N><N>
from core.colors import good, green, end
from core.requester import requester
from core.utils import getUrl, getParams
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
import copy
import re<N><N>
import core.config
from core.colors import red, good, green, end
from core.config import xsschecker
from core.filterChecker import filterChecker
from core.generator import generator
from core.htmlParser import htmlParser
from core.requester import requester
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
import copy
import re
from urllib.parse import urlparse, quote, unquote<N><N>
from core.arjun import arjun
from core.checker import checker
from core.colors import good, bad, end, info, green, red, que
import core.config
from core.config import xsschecker, minEfficiency
from core.dom import dom
from core.filterChecker import filterChecker
from core.generator import generator
from core.htmlParser import htmlParser
from core.requester import requester
from core.utils import getUrl, getParams, getVar
from core.wafDetector import wafDetector
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
import copy
from urllib.parse import urlparse<N><N>
from core.colors import green, end
from core.config import xsschecker
from core.fuzzer import fuzzer
from core.requester import requester
from core.utils import getUrl, getParams
from core.wafDetector import wafDetector
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>
import re
import json
import hashlib
from urllib.parse import urlparse<N><N>
from core.colors import green, end
from core.requester import requester
from core.utils import deJSON, js_extractor, handle_anchor, getVar, updateVar
from core.log import setup_logger<N><N>
logger = setup_logger(__name__)<N><N>

def is_defined(o):
    return o is not None<N><N>
#here we import all our necessary libaries
import cv2<N><N>
#here I am defining something known as the cascade classifier, where I am inputing a haarcascade dataset of faces
face_detection = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')<N><N>
#this is to capture my live video, and the 0 is telling the computer that I want to use my computer's cam
cap = cv2.VideoCapture(0)<N><N>
#sets width and height for the video capture screen
cap.set(3,640)
cap.set(4,480)<N><N>
#we use a while loop to diplay the vid, because the at the end of the day a vid is just a bunch of images repeating
while True:
    #here the computer is understanding the live video, and defining the video feed as image. Also the success variable is a boolean value here letting us know if the video was read
    success, img = cap.read()<N><N>
    #here I am converting each of the frame of the bideo to grayscale, because it is easy for the computer to read
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<N><N>
    #this is a line to detect multiscale images, meaning images of different sizes(different sizes of face).
    #the gray image here is telling it to detect multiscale iamges in the live feed
    #the 1.3 variable here is scale factor, which is to extend and decrease the size of a image so the computer can detect any scale of image
    #6 is the k neighbour, whcih is important in identifying the correct face, and reducing false detection
    faces = face_detection.detectMultiScale(img_gray, 1.3, 6)<N><N>
    #for loop to use coordinates in faces and draw rectnagle around it with putting text
    for (x,y,w,h) in faces:
        rect = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),4)
        cv2.putText(rect, 'Face', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)
    
        
    
    #dispaying the video
    cv2.imshow("Video", img)<N><N>
    #if statement to break loop if q clicked
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break<N><N>
    <N><N>
import os
from pprint import pprint
import openpyxl<N><N>
cwd = os.getcwd()<N><N>
dirs = [
    os.path.join(cwd, 'output-txt'), 
    os.path.join(cwd, 'img_heic'), 
    os.path.join(cwd, 'img_cropped'),
    os.path.join(cwd, 'img_boxed') 
        ]<N><N>
for dir in dirs:
    print('> Scanning the folder: {dir_name}...'.format(dir_name=dir))
    files = [os.path.join(dir, item) for item in os.listdir(dir)]
    
    if len(files) >= 1:
        print('> Deleting the files from the folder ...')
        for file in files:
            os.remove(file)
        print('> Files in the folder removed')
    else: 
        print('> The folder is empty.')
        <N><N>
# cleaning the data base file 
excel_file_path = os.path.join(cwd, 'output_data', 'customer_list-0.xlsx') <N><N>
print('=====================================')
wb = openpyxl.load_workbook(excel_file_path)
ws = wb['English']<N><N>
rows = ws.max_row
print('=> deleting the data...')
ws.delete_rows(2, rows)
print('=> data deleted:', rows, 'rows')<N><N>
wb.save(excel_file_path)<N><N>
from pprint import pprint
import os 
import cv2
import pytesseract
from pytesseract import Output
from PIL import Image
import pyheif
import openpyxl<N><N>
<N><N># opening heic files 
def heic_to_pil(input_abs):
    heic_file = pyheif.read(input_abs)
    img = Image.frombytes(heic_file.mode, 
                          heic_file.size, 
                          heic_file.data, 
                          'raw', 
                          heic_file.mode, 
                          heic_file.stride,)
    
    return img <N><N>
from pprint import pprint
import csv
import os 
import cv2
import pytesseract
from pytesseract import Output
from PIL import Image
import pyheif
import openpyxl
import re<N><N>
# ==========================================================
# --- --- FUNCTIONS --- ---
# ==========================================================<N><N>
# opening heic files 
def heic_to_pil(input_abs):
    heic_file = pyheif.read(input_abs)
    img = Image.frombytes(heic_file.mode, heic_file.size, heic_file.data, 'raw', heic_file.mode, heic_file.stride,)
    
    return img <N><N>
def img_to_text(input_dir, output_dir):
    img_names = os.listdir(input_dir)
    img_names.sort()<N><N>
import os
from pprint import pprint
import re <N><N>
cwd = os.getcwd()
in_dir = os.path.join(cwd, 'output-txt')<N><N>
files = os.listdir(in_dir)
files.sort()<N><N>
file_1 = files[2]
file_1_path = os.path.join(in_dir, file_1)
file_1 = open(file_1_path, 'r')<N><N>
def squiz_list(lst):
    new_lst = list(filter(None, lst))
    new_lst = new_lst[0:new_lst.index('CONTACT DETAILS')]
    return new_lst<N><N>
def extract_company_name(lst):
    info_unit, data = 'Company Name', ''
    if info_unit in lst:
        i = lst.index(info_unit)
        if i == len(lst)-1: # this means the key word is last element in the list
            data = lst[-2]
        elif i == len(lst) - 2: # this means the key word is second to last el in the list 
            data = lst[-1]
        lst.pop()
        lst.pop()
    else:
        print('=> There is no info :', info_unit)
    return data, lst<N><N>
# Importing modules
import cv2
import pytesseract
from pytesseract import Output
from pprint import pprint<N><N>
# STEP-1: reading the image using OpenCV
img_obj = cv2.imread('pytesseract_test/IMG_6823-cropped.jpeg')
# cv2.imshow('Original Image', img_obj)
# cv2.waitKey(0)<N><N>
# STEP-2: converting the image into the gray scale
img_obj_gray = cv2.cvtColor(img_obj, cv2.COLOR_BGR2GRAY)
# cv2.imshow('Gray Scale Image', img_obj)
# cv2.waitKey(0)<N><N>
# STEP-3: converting the image into binary image by thresholding
img_obj_threshold = cv2.threshold(img_obj_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]
# cv2.imshow('Thresholded Image', img_obj)
# cv2.waitKey(0)<N><N>
# STEP-4: configuring the parameters for tesseract
custom_config = r'--oem 3 --psm 6'<N><N>
# STEP-5: feeding image to pytesseract
details = pytesseract.image_to_data(img_obj_threshold, output_type=Output.DICT, config=custom_config, lang='eng')<N><N>
# STEP-6: Drawign the bounding boxes for the image 
total_boxes = len(details['text'])<N><N>
for seq_num in range(total_boxes):
    if int(details['conf'][seq_num]) > 20:
        (x, y, w, h) = (details['left'][seq_num], details['top'][seq_num], details['width'][seq_num], details['height'][seq_num])
        img_obj_threshold = cv2.rectangle(img_obj_threshold, (x, y), (x + w, y + h), (0, 252, 0), 1)
    
#displaying the image 
cv2.imshow('Captured text', img_obj_threshold)
cv2.waitKey(0)<N><N>
# STEP-7: Parsing the text 
parse_txt, word_list, last_word = [], [], ''
for word in details['text']:
    if word != '':
        word_list.append(word)
        last_word = word
    
    if (last_word != '' and word == '') or (word == details['text'][-1]):
        parse_txt.append(word_list)
        word_list = []<N><N>
pprint(parse_txt)
pprint(word_list)<N><N>
print('done')<N><N>
import cv2
import numpy as np<N><N>

def canny(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    blur_image = cv2.GaussianBlur(gray_image, (5,5),0)
    canny_image = cv2.Canny(blur_image, 50, 150)
    return canny_image<N><N>

def region_of_interest(image):
    height = image.shape[0]
    polygons = np.array([[(200, height), (1100, height), (550, 250)]])
    mask = np.zeros_like(image)
    cv2.fillPoly(mask, polygons, 255)
    masked_image = cv2.bitwise_and(image, mask)
    return masked_image<N><N>
def display_lines(image,lines):
    line_image = np.zeros_like(image)
    if lines is not None:
        for x1,y1,x2,y2 in lines:
            cv2.line(line_image, (x1,y1), (x2,y2), (255,0,0), 10)
    return(line_image)<N><N>

def make_coordinates(image, line_parameters):
    slope, intercept = line_parameters
    y1 = image.shape[0]
    y2 = int(y1*(3/5))
    x1 = int((y1-intercept)/slope)
    x2 = int((y2-intercept)/slope)
    return np.array([x1,y1,x2,y2])<N><N>
import cv2
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1280)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))<N><N>

cam = cv2.VideoCapture(0)
cam.set(cv2.CAP_PROP_FRAME_HEIGHT,720)
cam.set(cv2.CAP_PROP_FRAME_WIDTH,720)<N><N>

fincas = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<N><N>
while True:
    _, frame = cap.read()
    
    faces = fincas.detectMultiScale(frame, 1.1, 4)
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)
    cv2.imshow("test1", frame)
    
    k = cv2.waitKey(1)
    if k == 27:
        break
cap.release()
cv2.destroyAllWindows()
<N><N>
import rondom<N><N>
import cv2
import numpy<N><N>
